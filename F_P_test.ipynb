{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alitoast/Exam_IR/blob/fetcher_parser-dev/F_P_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Librerie#"
      ],
      "metadata": {
        "id": "EFS9PhzpB5wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#url di prova\n",
        "# ha sitemap-index ma non priority etc.\n",
        "start_url_uno = \"https://www.dragopublisher.com/it/\"\n",
        "# non ha sitemap\n",
        "start_url_due = \"https://www.wildraccoon.it/shop/\"\n",
        "# sito giocattolo ha tutto\n",
        "start_url = \"https://nonciclopedia.org/wiki/Wikipedia\" #--> no è in italiano\n",
        "start_url= \"https://www.vice.com/en/contributor/vice-uk/\""
      ],
      "metadata": {
        "id": "wsDOXSrnc3Ti"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rfc3986"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sRo7YHUxAKc",
        "outputId": "f9b6f703-bc9b-44e7-83b7-58df28b0ae16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rfc3986\n",
            "  Downloading rfc3986-2.0.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Downloading rfc3986-2.0.0-py2.py3-none-any.whl (31 kB)\n",
            "Installing collected packages: rfc3986\n",
            "Successfully installed rfc3986-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "import urllib.robotparser #per gestire il file robot.txt\n",
        "import xml.etree.ElementTree as ET  #per gestire i file xml\n",
        "import requests #per http\n",
        "import time #per gestire il tempo\n",
        "from bs4 import BeautifulSoup #per gestire il parsing del html (si può usare anche per xml)\n",
        "import rfc3986 # per la normalizzazione degli urls\n"
      ],
      "metadata": {
        "id": "X-hlOT7K3Wzn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fetcher#\n",
        "gestione richieste e download pagine web"
      ],
      "metadata": {
        "id": "o64w-kJVAOHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UserAgentPolicy:\n",
        "  '''\n",
        "     Class to identify user agents and store relevant information to manage their requests appropriately over time\n",
        "  '''\n",
        "  def __init__(self, name, base_url, path_disallow, crawl_delay, request_rate,last_access, header):\n",
        "        self.base_url = base_url\n",
        "        self.name = name\n",
        "        self.path_disallow = path_disallow\n",
        "        self.crawl_delay = crawl_delay\n",
        "        self.request_rate = request_rate\n",
        "        self.last_access = last_access\n",
        "        self.header = header"
      ],
      "metadata": {
        "id": "JS4QwNGoXol-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Useragents\n",
        "useragent_dict = {\"Googlebot\": UserAgentPolicy(\"Googlebot\",None, None, None, None, 1e9,'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'),\n",
        "                  \"Bingbot\": UserAgentPolicy(\"Bingbot\",None, None, None, None, 1e9,'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)'),\n",
        "                  \"Slurp\": UserAgentPolicy(\"Slurp\",None, None, None, None, 1e9, 'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)'),\n",
        "                  \"DuckDuckbot\": UserAgentPolicy(\"DuckDuckbot\",None, None, None, None, 1e9,'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)' ),\n",
        "                  \"Yandex\": UserAgentPolicy(\"Yandex\",None, None, None, None, 1e9,'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)' ),\n",
        "                  \"*\": UserAgentPolicy(\"*\",None, None, None, None, 1e9,'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)' )\n",
        "                  }\n",
        "default_agent = useragent_dict[\"Googlebot\"]\n",
        "\n",
        "# header generated by ChatGPT\n",
        "useragent_dict_header = {'Googlebot':'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',\n",
        "                    'Bingbot':'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)',\n",
        "                    'Slurp':'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)',\n",
        "                    'DuckDuckbot':'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)',\n",
        "                    'Yandex':'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)',\n",
        "                    '*': 'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)'}"
      ],
      "metadata": {
        "id": "2NPNZPZCMBDm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global variable to keep track of the visited site\n",
        "visulized_url = set()"
      ],
      "metadata": {
        "id": "GXWR8DTa7RKC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_robots(url,useragent=default_agent):\n",
        "\n",
        "  '''\n",
        "    Inputs:\n",
        "        url (str): The base URL of the website to analyze.\n",
        "        useragent (class): The user agent to use when checking access permissions. Default is '*'.\n",
        "\n",
        "    Output:\n",
        "        sitemap (list or None): List of sitemap URLs found in robots.txt, if any.\n",
        "\n",
        "    Description:\n",
        "        This function checks the robots.txt file of a given website to determine:\n",
        "        - Whether the given user agent is allowed to fetch content.\n",
        "        - Which paths are disallowed for the user agent.\n",
        "        - The crawl delay and request rate for the user agent.\n",
        "        - Any sitemap URLs provided in the robots.txt.\n",
        "\n",
        "        It updates the given useragent object with these details.\n",
        "    '''\n",
        "\n",
        "  useragent.base_url = url\n",
        "\n",
        "  # Initialize the robot parser\n",
        "  rfl = urllib.robotparser.RobotFileParser()\n",
        "  # Construct the full URL to the robots.txt file\n",
        "  new_url = url + \"/robots.txt\"\n",
        "\n",
        "  rfl.set_url(new_url)\n",
        "  # Read and parse the robots.txt file\n",
        "  rfl.read()\n",
        "  check = rfl.can_fetch(useragent.name,new_url)\n",
        "  if check:\n",
        "    print(\"{} lo user può effettuare il fetch dei dati\".format(check))\n",
        "  else:\n",
        "    print(\"lo user non può effettuare il fetch dei dati\")\n",
        "\n",
        "  # Retrieve the list of the path not to follow\n",
        "  path_disallow = rfl.parse(\"Disallow\")\n",
        "  useragent.path_disallow = path_disallow\n",
        "\n",
        "  # Retrieve the crawl delay (minimum interval between requests)\n",
        "  crawl_delay = rfl.crawl_delay(useragent)\n",
        "  useragent.crawl_delay = crawl_delay\n",
        "\n",
        "  # Retrieve the request rate (tuple of requests per time interval)\n",
        "  request_rate = rfl.request_rate(useragent)\n",
        "  useragent.request_rate = request_rate\n",
        "\n",
        "  # Retrieve any sitemap URLs declared in the robots.txt\n",
        "  sitemap = rfl.site_maps()\n",
        "  print(\"sitemap: {}\".format(sitemap))\n",
        "\n",
        "  return sitemap"
      ],
      "metadata": {
        "id": "IAoElvcYAQsv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_time(useragent=default_agent):\n",
        "\n",
        "  '''\n",
        "    Inputs:\n",
        "        useragent (UserAgentPolicy): The user agent object containing rate-limiting information.\n",
        "\n",
        "    Output:\n",
        "      None\n",
        "\n",
        "    Description:\n",
        "        Ensures that the user agent respects the specified crawl delay and request rate policies:\n",
        "        - Calculates the appropriate delay based on the more restrictive value between `crawl_delay` and `request_rate`.\n",
        "        - If neither is defined, defaults to a delay of 1.5 seconds.\n",
        "        - Ensures the delay is respected by pausing execution as needed.\n",
        "        - Updates `last_access` to the current time after waiting.\n",
        "  '''\n",
        "\n",
        "\n",
        "  # If both crawl_delay and request_rate are defined, use the more restrictive one (converted to nanoseconds)\n",
        "\n",
        "  if useragent.request_rate and useragent.crawl_delay:\n",
        "     # Calculate delay from request rate: total interval (in ns) divided by number of requests\n",
        "     request_delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]\n",
        "     # Convert crawl delay from seconds to nanoseconds\n",
        "     crawl_delay = useragent.crawl_delay * 1e9\n",
        "     # Use the larger (i.e., more restrictive) delay\n",
        "     delay = max(request_delay, crawl_delay)\n",
        "\n",
        "  elif useragent.crawl_delay:\n",
        "     delay = useragent.crawl_delay * 1e9\n",
        "\n",
        "  elif useragent.request_rate:\n",
        "     delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]\n",
        "\n",
        "  # If neither crawl_delay and request rate are defined, use a default delay of 1.5 seconds\n",
        "  else:\n",
        "    delay = 1.5 * 1e9  # 1.5 secondi in nanosecondi\n",
        "\n",
        "  # Get the current time in nanoseconds\n",
        "  now = time.monotonic_ns()\n",
        "  wait = max(0, (useragent.last_access + delay) - now)\n",
        "\n",
        "  if wait > 0:\n",
        "     time.sleep(wait / 1e9)\n",
        "\n",
        "  # Update last_access to the current time to enforce timing on next request\n",
        "  useragent.last_access = time.monotonic_ns()\n"
      ],
      "metadata": {
        "id": "bY-25bn6Fkxx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch(url,useragent=default_agent):\n",
        "  '''\n",
        "    Inputs:\n",
        "        url (str): The target URL to fetch.\n",
        "        useragent (UserAgentPolicy): The user agent object containing headers and rate-limiting rules.\n",
        "\n",
        "    Outputs:\n",
        "        str or None: The HTML content of the page if the request is successful (HTTP 200), otherwise None.\n",
        "\n",
        "    Description:\n",
        "        Sends an HTTP GET request to the specified URL, respecting the request policies defined for the given user agent.\n",
        "\n",
        "    '''\n",
        "\n",
        "  # Check if the page is been visited alredy\n",
        "  if url not in visulized_url:\n",
        "\n",
        "    # Enforce crawl delay and request rate restrictions for the user agent\n",
        "    check_time(useragent)\n",
        "\n",
        "    # Build the request headers using the specified user agent (optional, useful for testing)\n",
        "    headers = {\n",
        "          'User-Agent': useragent.header\n",
        "     }\n",
        "\n",
        "    # Send the HTTP GET request\n",
        "    try:\n",
        "       response = requests.get(url, headers=headers, timeout=10)\n",
        "       if response.status_code == 200:\n",
        "         print(\"Pagina recuperata con successo:{}\".format(url))\n",
        "         html = response.text\n",
        "         visulized_url.add(url)\n",
        "         return html\n",
        "       else:\n",
        "        print(\"Errore nella ricezione del sito:{}\".format(url))\n",
        "        return None\n",
        "\n",
        "    # Handle any request-related exceptions (connection errors, timeouts, etc.)\n",
        "    except requests.RequestException as e:\n",
        "          print(\"Eccezione durante la richiesta:\", e)\n",
        "          return None\n",
        "  else:\n",
        "    print(\"Pagina già visulizzata:{}\".format(url))\n",
        "    return None"
      ],
      "metadata": {
        "id": "JxhLLKQIC9Nd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parser#\n",
        "analisi contenuto della pagina, estrazione testo e link e normalizzazione link trovati"
      ],
      "metadata": {
        "id": "Sv_y5bWRARWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_url(url):\n",
        "\n",
        "  '''\n",
        "    Input:\n",
        "        url (str): The URL to be normalized.\n",
        "\n",
        "    Output:\n",
        "        str: The normalized URL as a string. If normalization fails, the original URL is returned.\n",
        "\n",
        "    Description:\n",
        "        Normalizes the input URL according to the RFC 3986 standard and returns a string representation.\n",
        "        This function uses the `rfc3986` library to parse and normalize the URL according to the\n",
        "        URI standard defined in RFC 3986. This includes handling issues such as case normalization,\n",
        "        removing default ports, sorting query parameters (if applicable), and more.\n",
        "\n",
        "        If an error occurs during normalization (e.g., invalid input), it catches the exception\n",
        "        and returns the original URL as a fallback.\n",
        "  '''\n",
        "\n",
        "  try:\n",
        "      uri = rfc3986.uri_reference(url).normalize()\n",
        "      return uri.unsplit()\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"Errore nella normalizzazione URL: {url} – {e}\")\n",
        "      return url\n"
      ],
      "metadata": {
        "id": "9xyD_K9RT7AI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_spider_traps(url):\n",
        "\n",
        "  '''\n",
        "\n",
        "    Input:\n",
        "     url(str):The URL to be analyzed.\n",
        "\n",
        "    Output:\n",
        "      - Returns False if the URL is suspicious or considered a \"spider trap.\"\n",
        "      - Returns True if the URL seems safe to crawl.\n",
        "\n",
        "    Description:\n",
        "      Checks whether a given URL is potentially harmful or could trap a web crawler\n",
        "      in infinite loops or unnecessary crawling paths.\n",
        "\n",
        "  '''\n",
        "\n",
        "  MAX_URL_LENGTH = 200   # Arbitrary maximum allowed URL length\n",
        "  MAX_PATH_DEPTH = 6     # Maximum allowed number of slashes in path\n",
        "  trap_pattern = re.compile(r\"(calendar|sessionid|track|ref|sort|date=|page=\\d{3,})\", re.IGNORECASE)   # Pattern matching common signs of spider traps:\n",
        "                                                                                                       # calendars, session IDs, tracking params, endless pagination, etc.\n",
        "\n",
        "  link = urlparse(url)\n",
        "\n",
        "  # Check URL length\n",
        "  if len(link) > MAX_URL_LENGTH:\n",
        "     return False\n",
        "\n",
        "  # Check path depth (number of '/' in path)\n",
        "  if link.path.count('/') > MAX_PATH_DEPTH:\n",
        "     return False\n",
        "\n",
        "  # Check for suspicious patterns in the URL\n",
        "  if trap_pattern.search(url):\n",
        "     return False\n",
        "\n",
        "  return True\n"
      ],
      "metadata": {
        "id": "qwi0JZuNG566"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_sitemap(sitemap_list):\n",
        "\n",
        "    '''\n",
        "      Input:\n",
        "        sitemap_list (list): A list of sitemap URLs to parse.\n",
        "\n",
        "      Output:\n",
        "          Returns a DataFrame containing:\n",
        "         - URL\n",
        "         - Priority\n",
        "         - Update frequency\n",
        "\n",
        "      Description:\n",
        "         Supports both XML and HTML sitemaps. Handles nested sitemap indexes.\n",
        "         Returns a DataFrame with the parsed sitemap entries.\n",
        "\n",
        "    '''\n",
        "    namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "    all_entries = []\n",
        "\n",
        "    if not sitemap_list:\n",
        "        print(\"Nessuna sitemap fornita.\")\n",
        "        return pd.DataFrame(columns=['url', 'priority', 'update'])\n",
        "\n",
        "    def parse_single_sitemap(sitemap_url):\n",
        "        try:\n",
        "            response = requests.get(sitemap_url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Errore durante il recupero di {sitemap_url}: {e}\")\n",
        "            return []\n",
        "\n",
        "        content_type = response.headers.get(\"Content-Type\", \"\").lower()\n",
        "\n",
        "        # Sitemap XML\n",
        "        if \"xml\" in content_type:\n",
        "            try:\n",
        "                root = ET.fromstring(response.content)\n",
        "            except ET.ParseError as e:\n",
        "                print(f\"Errore nel parsing XML: {e}\")\n",
        "                return []\n",
        "\n",
        "            # If it's a sitemap index, recursively parse child sitemaps\n",
        "            if root.tag.endswith('index'):\n",
        "                sitemap_urls = [loc.text for loc in root.findall('ns:sitemap/ns:loc', namespace)]\n",
        "                entries = []\n",
        "                for sub_url in sitemap_urls:\n",
        "                    entries.extend(parse_single_sitemap(sub_url))\n",
        "                return entries\n",
        "\n",
        "            #  # If it's a standard URL set\n",
        "            elif root.tag.endswith('urlset'):\n",
        "                return [{\n",
        "                    'url': url.findtext('ns:loc', default='', namespaces=namespace),\n",
        "                    'priority': url.findtext('ns:priority', default=None, namespaces=namespace),\n",
        "                    'update': url.findtext('ns:changefreq', default=None, namespaces=namespace)\n",
        "                } for url in root.findall('ns:url', namespace)]\n",
        "\n",
        "        # Handle HTML sitemaps (fallback)\n",
        "        elif \"html\" in content_type:\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "            return [{\n",
        "                'url': normalize_url(a['href']),\n",
        "                'priority': None,\n",
        "                'update': None\n",
        "            } for a in soup.find_all('a', href=True)]\n",
        "\n",
        "        else:\n",
        "            print(f\"Formato non riconosciuto per {sitemap_url} ({content_type})\")\n",
        "            return []\n",
        "\n",
        "    #  Process each sitemap URL in the list\n",
        "    for sitemap_url in sitemap_list:\n",
        "        if sitemap_url:\n",
        "            entries = parse_single_sitemap(sitemap_url)\n",
        "            for entry in entries:\n",
        "                url = normalize_url(entry['url']) #normalizzo l'url\n",
        "                if check_spider_traps(url): # provo a limitare le spider-trap\n",
        "                    entry['url'] = url\n",
        "\n",
        "            all_entries.extend(entries)\n",
        "\n",
        "    # Create a DataFrame and drop duplicates\n",
        "    df = pd.DataFrame(all_entries, columns=['url', 'priority', 'update']).drop_duplicates()\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "-b4iywtuKnz5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_page_url(html,sitemaps_urls,useragent=default_agent):\n",
        "\n",
        "  '''\n",
        "     Inputs:\n",
        "     - html(str): The HTML content of the page.\n",
        "     - sitemaps_urls(str): A list of URLs already known from sitemaps.\n",
        "     - useragent(class): A user agent object containing disallowed paths (robots.txt rules).\n",
        "\n",
        "     Output:\n",
        "     - A list of new, allowed URLs extracted from the page.\n",
        "\n",
        "     Description:\n",
        "       Given an HTML page, this function extracts all URLs and filters them based on:\n",
        "       - Exclusion of private or disallowed directories (from robots.txt rules)\n",
        "       - Deduplication with URLs already found in the sitemap\n",
        "       - Optional normalization and trap filtering\n",
        "\n",
        "  '''\n",
        "\n",
        "  # Parse HTML content\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "  urls = []\n",
        "  # Extract URLs from href attributes (e.g., <a>, <link>, etc.)\n",
        "  for tag in soup.find_all(href=True):\n",
        "      urls.append(tag['href'])\n",
        "\n",
        "  # Extract URLs from src attributes (e.g., <img>, <script>, etc.)\n",
        "  for tag in soup.find_all(src=True):\n",
        "      urls.append(tag['src'])\n",
        "\n",
        "  # Filter out disallowed URLs based on robots.txt rules\n",
        "  if useragent.path_disallow != None:\n",
        "    for url in urls:\n",
        "      for path in useragent.path_disallow:\n",
        "        if path in url:\n",
        "         urls.remove(url)\n",
        "\n",
        "  # Normalize URLs (e.g., remove fragments, resolve relative paths, etc.)\n",
        "    urls = [normalize_url(url) for url in urls]\n",
        "\n",
        "  # Remove duplicate URLs\n",
        "    urls = list(set(urls))\n",
        "\n",
        "  # Filter out potential spider traps\n",
        "    urls = [url for url in urls if check_spider_traps(url)]\n",
        "\n",
        "  # Filter out URLs that are already in the sitemap\n",
        "  if sitemaps_urls != None:\n",
        "    def_urls = sitemaps_urls\n",
        "    for url in urls:\n",
        "      if url not in def_urls:\n",
        "         def_urls.append(url)\n",
        "  else:\n",
        "    def_urls = urls\n",
        "\n",
        "  return def_urls\n",
        "\n"
      ],
      "metadata": {
        "id": "NbmdtzHa4GKp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_page_tags_all(html,tags_type = None):\n",
        "\n",
        "  '''\n",
        "    Inputs:\n",
        "    - html: The HTML content as a string.\n",
        "    - tags_type: A list of tag names to search for (default includes common content tags).\n",
        "\n",
        "    Output:\n",
        "    - A list of text strings extracted from the specified tags, preserving DOM order\n",
        "\n",
        "    Description:\n",
        "      Parses the given HTML content and extracts text from specified HTML tags.\n",
        "\n",
        "  '''\n",
        "  if tags_type == None:\n",
        "     tags_type = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span', 'a']\n",
        "\n",
        "  # Initialize the HTML parser\n",
        "  soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "  # Find tag to remove\n",
        "  for tag in soup(['script','style','footer','nav','noscript','header','form','aside']):\n",
        "      tag.decompose()\n",
        "\n",
        "  # Find all comments and removes them\n",
        "  for comment in soup.find_all(string=lambda text: isinstance(text,comment)):\n",
        "      comment.extract()\n",
        "\n",
        "  # Find all tags of the specified types (respects DOM order)\n",
        "  tags = soup.find_all(tags_type)\n",
        "\n",
        "  # Extract clean text from each tag (removing whitespace and combining with spaces)\n",
        "  texts = [tag.get_text(separator=' ', strip=True) for tag in tags]\n",
        "\n",
        "  return texts\n",
        "\n"
      ],
      "metadata": {
        "id": "AsZuPyQ_s5No"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html = fetch(start_url_due, default_agent)\n",
        "words = parse_page_tags_all(html)\n",
        "print(words)\n",
        "print(default_agent.last_access)"
      ],
      "metadata": {
        "id": "zbdkGLyhwrre",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a0cc6db-1686-4ee5-c274-d9843e14fe2a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pagina recuperata con successo:https://www.wildraccoon.it/shop/\n",
            "['Home', 'Le birre', 'Shop Lattine Merchandising', 'Lattine', 'Merchandising', 'Il Metodo', 'Chi siamo', 'Bolli spina', 'Contatti', 'Home', 'Le birre', 'Shop Lattine Merchandising', 'Lattine', 'Merchandising', 'Il Metodo', 'Chi siamo', 'Bolli spina', 'Contatti', 'SHOP', 'Shop', 'Kopernik was a woman 24,00 € – 66,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Kopernik was a woman', 'Beyond Infinity 19,00 € – 54,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Beyond Infinity', 'Straight to harvest 22,00 € – 62,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Straight to harvest', 'Each others paranoia 20,00 € – 58,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Each others paranoia', 'Vfx 19,00 € – 54,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Vfx', 'Dolci on fire – Mango, banana & lassi 25,00 € – 68,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Dolci on fire – Mango, banana & lassi', 'End of the road 22,00 € – 62,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'End of the road', 'Niceness doesn’t last 19,00 € – 52,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Niceness doesn’t last', 'Don’t be gloomy 20,00 € – 58,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Don’t be gloomy', 'All the layers peanut 32,00 € – 96,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'All the layers peanut', 'All the layers tonka coffee 32,00 € – 96,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'All the layers tonka coffee', 'Good bye winter 18,00 € – 50,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Good bye winter', 'A little money 22,00 € – 62,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'A little money', 'Twenty shells 22,00 € – 62,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Twenty shells', 'Dolci on fire – PB&J 25,00 € – 68,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Dolci on fire – PB&J', 'Time and Space 24,00 € – 66,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Time and Space', 'Tears in the rain 20,00 € – 58,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Tears in the rain', 'APA – Blockbuster 18,00 € – 50,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'APA – Blockbuster', 'Ragioniere, batti? 17,00 € – 48,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Ragioniere, batti?', 'Cross your heart 22,00 € – 62,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Cross your heart', 'Arctic Terror 20,00 € – 58,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Arctic Terror', 'Gentle Crime 17,00 € – 48,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Gentle Crime', 'Maglietta beige Wild Raccoon 20,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Maglietta beige Wild Raccoon', 'Maglietta lilla Wild Raccoon 20,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Maglietta lilla Wild Raccoon', 'Bicchiere Wild Raccoon Oslo 400ml 5,00 € Aggiungi al carrello', 'Bicchiere Wild Raccoon Oslo 400ml', 'Crush on you 20,00 € – 58,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Crush on you', 'One of a kind 17,00 € – 48,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'One of a kind', '5minute window 20,00 € – 58,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', '5minute window', 'Cartone degustazione 60,00 € Aggiungi al carrello', 'Cartone degustazione', 'Wild Raccoon SRL Sede legale Via Giosue Carducci 22 Tavagnacco 33010 (UD) Sede operativa Via Decani Di Cussignacco Udine 33100 (UD) P.IVA 03067900302 email: amministrazione@wildraccoon.it', 'PRIVACY POLICY', 'COOKIE POLICY', 'FOLLOW US']\n",
            "179810043148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xc9dW6BfwEUn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test#"
      ],
      "metadata": {
        "id": "StwStCmAwE0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sito privo di sitemap, provo con il parse a trovare i link"
      ],
      "metadata": {
        "id": "mXe3WuG7wVPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#default_agent = 'Googlebot'\n",
        "sitemap = check_robots(start_url_due,default_agent)\n",
        "html = fetch(start_url_due, default_agent)\n",
        "if sitemap == None:\n",
        "  urls = parse_page_url(html,None,default_agent)\n",
        "print(urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "mbgoW6JN97Ip",
        "outputId": "de19dbe4-bf87-42ec-d011-c52e2bd0d137"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True il sito puo effettuare il fetch dei dati\n",
            "sitemap: None\n",
            "Pagina già visulizzata:https://www.wildraccoon.it/shop/\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Incoming markup is of an invalid type: None. Markup must be a string, a bytestring, or an open filehandle.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-2572899830>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_url_due\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msitemap\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_page_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdefault_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-222052929>\u001b[0m in \u001b[0;36mparse_page_url\u001b[0;34m(html, sitemaps_urls, useragent)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;31m# Parse HTML content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mmarkup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__len__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    443\u001b[0m                 \u001b[0;34mf\"Incoming markup is of an invalid type: {markup!r}. Markup must be a string, a bytestring, or an open filehandle.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             )\n",
            "\u001b[0;31mTypeError\u001b[0m: Incoming markup is of an invalid type: None. Markup must be a string, a bytestring, or an open filehandle."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#useragent = '*'\n",
        "sitemap = check_robots(start_url_due,useragent=useragent_dict['*'])\n",
        "html = fetch(start_url_due, default_agent)\n",
        "if sitemap == None:\n",
        "  urls = parse_page_url(html,None,default_agent)\n",
        "print(urls)"
      ],
      "metadata": {
        "id": "esFHxTOrwiyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sito con sitemap provo ad anlizzarla"
      ],
      "metadata": {
        "id": "p8GUnCy4w1dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#default_agent = 'Googlebot'\n",
        "sitemap = check_robots(start_url,default_agent)\n",
        "html = fetch(start_url_due, default_agent)\n",
        "if sitemap != None:\n",
        "  df = parse_sitemap(sitemap)\n",
        "  print(df)\n",
        "\n",
        "urls = parse_page_url(html,df['url'].tolist(),default_agent)\n",
        "print(urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-7LiZndw5eg",
        "outputId": "033d5709-061a-4e9a-c06f-9356c1348ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True lo user può effettuare il fetch dei dati\n",
            "sitemap: ['https://www.vice.com/sitemap.xml']\n",
            "Pagina già visulizzata:https://www.wildraccoon.it/shop/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#default_agent = '*'\n",
        "sitemap = check_robots(start_url_uno,useragent=useragent_dict['*'])\n",
        "html = fetch(start_url_due, default_agent)\n",
        "if sitemap != None:\n",
        "  df = parse_sitemap(sitemap)\n",
        "  print(df)\n",
        "\n",
        "urls = parse_page_url(html,df['url'].tolist(),default_agent)\n",
        "print(urls)"
      ],
      "metadata": {
        "id": "hAcieh8jxB5_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}