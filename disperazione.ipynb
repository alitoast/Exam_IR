{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alitoast/Exam_IR/blob/fetcher_parser-dev/disperazione.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBDLQqyN5JH7"
      },
      "source": [
        "#Librerie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v48r7wuM5a4e",
        "outputId": "0e11def1-ad10-4708-96e2-d919667c7ca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\n",
            "Requirement already satisfied: asyncio in /usr/local/lib/python3.11/dist-packages (3.4.3)\n",
            "Requirement already satisfied: rfc3986 in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Collecting simhash\n",
            "  Downloading simhash-2.1.2-py3-none-any.whl.metadata (382 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from simhash) (2.0.2)\n",
            "Downloading simhash-2.1.2-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: simhash\n",
            "Successfully installed simhash-2.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install aiohttp\n",
        "!pip install asyncio\n",
        "!pip install rfc3986\n",
        "!pip install simhash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NBcOHHSJ5IQ_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "import urllib.robotparser #per gestire il file robot.txt\n",
        "import xml.etree.ElementTree as ET  #per gestire i file xml\n",
        "import requests #per http\n",
        "import time #per gestire il tempo\n",
        "from bs4 import BeautifulSoup #per gestire il parsing del html (si puÃ² usare anche per xml)\n",
        "import rfc3986 # per la normalizzazione degli urls\n",
        "import logging\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import aiofiles\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import nltk\n",
        "import simhash\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ, ADV\n",
        "from collections import defaultdict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6Jl6cEZCSy0",
        "outputId": "8adfb3c7-8d4a-47fb-a035-dc5af00074fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iktFyOp5Eic"
      },
      "source": [
        "#Fetcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YKNYw4qFTU0"
      },
      "outputs": [],
      "source": [
        "#url di prova\n",
        "# ha sitemap-index ma non priority etc.\n",
        "start_url_uno = \"https://www.dragopublisher.com/it/\"\n",
        "# non ha sitemap\n",
        "start_url_due = \"https://www.wildraccoon.it/shop/\"\n",
        "# sito con sitemap\n",
        "start_url = \"https://www.bbc.com/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IaYkcwAd_pIj"
      },
      "outputs": [],
      "source": [
        "# questo funziona\n",
        "class UserAgentPolicy:\n",
        "    def __init__(self, name, header):\n",
        "        self.name = name\n",
        "        self.header = header\n",
        "        self.base_url = None\n",
        "        self.path_disallow = None\n",
        "        self.crawl_delay = None\n",
        "        self.request_rate = None\n",
        "        self.last_access = 1e9\n",
        "        self.visited = set()\n",
        "        self.lock = asyncio.Lock()\n",
        "\n",
        "\n",
        "class Fetcher:\n",
        "    def __init__(self, session, useragent_name=\"Googlebot\"):\n",
        "        self.session = session\n",
        "        self.useragents = {\n",
        "            \"Googlebot\": UserAgentPolicy(\"Googlebot\", 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'),\n",
        "            \"Bingbot\": UserAgentPolicy(\"Bingbot\", 'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)'),\n",
        "            \"*\": UserAgentPolicy(\"*\", 'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)'),\n",
        "        }\n",
        "        self.default_agent = self.useragents.get(useragent_name, self.useragents[\"Googlebot\"])\n",
        "\n",
        "\n",
        "    async def check_robots(self, url, useragent=None):\n",
        "        if not useragent:\n",
        "            useragent = self.default_agent\n",
        "        useragent.base_url = url\n",
        "        new_url = url.rstrip(\"/\") + \"/robots.txt\"\n",
        "\n",
        "        rfl = urllib.robotparser.RobotFileParser()\n",
        "        try:\n",
        "            async with self.session.get(new_url, timeout=10) as response:\n",
        "                if response.status == 200:\n",
        "                    robots_txt = await response.text()\n",
        "                    rfl.parse(robots_txt.splitlines())\n",
        "                    logging.info(f\"Fetched robots.txt: {new_url}\")\n",
        "                else:\n",
        "                    logging.warning(f\"Failed to fetch robots.txt: HTTP {response.status}\")\n",
        "                    return None\n",
        "        except aiohttp.ClientError as e:\n",
        "            logging.error(f\"Error fetching robots.txt: {e}\")\n",
        "            return None\n",
        "\n",
        "        useragent.path_disallow = rfl.parse(\"Disallow\")\n",
        "        useragent.crawl_delay = rfl.crawl_delay(useragent.name)\n",
        "        useragent.request_rate = rfl.request_rate(useragent.name)\n",
        "\n",
        "        return rfl.site_maps()\n",
        "\n",
        "    async def check_time(self, useragent):\n",
        "        async with useragent.lock:\n",
        "            if useragent.request_rate and useragent.crawl_delay:\n",
        "                request_delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]\n",
        "                crawl_delay = useragent.crawl_delay * 1e9\n",
        "                delay = max(request_delay, crawl_delay)\n",
        "            elif useragent.crawl_delay:\n",
        "                delay = useragent.crawl_delay * 1e9\n",
        "            elif useragent.request_rate:\n",
        "                delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]\n",
        "            else:\n",
        "                delay = 1.5 * 1e9  # default delay\n",
        "            logging.info(f\"Delay: {delay / 1e9} seconds\")\n",
        "            now = time.monotonic_ns()\n",
        "            wait = max(0, (useragent.last_access + delay) - now)\n",
        "            if wait > 0:\n",
        "                await asyncio.sleep(wait / 1e9)\n",
        "            useragent.last_access = time.monotonic_ns()\n",
        "            logging.info(f\"{useragent.name} Last access: {useragent.last_access / 1e9} seconds\")\n",
        "\n",
        "    async def fetch(self, url, useragent=None):\n",
        "      if not useragent:\n",
        "          useragent = self.default_agent\n",
        "\n",
        "      if url in useragent.visited:\n",
        "          logging.warning(f\"[{useragent.name}] Page already visited: {url}\")\n",
        "          return None\n",
        "\n",
        "      await self.check_time(useragent)\n",
        "\n",
        "      headers = {'User-Agent': useragent.header}\n",
        "\n",
        "      start_time = time.perf_counter()\n",
        "      try:\n",
        "          async with self.session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=20)) as response:\n",
        "              duration = time.perf_counter() - start_time\n",
        "\n",
        "              if response.status == 200:\n",
        "                  #content_type = response.headers.get('Content-Type', '').lower()\n",
        "                  #if 'text/html' in content_type:\n",
        "                      html = await response.text()\n",
        "                      logging.info(f\"[{useragent.name}] Fetched HTML page: {url} in {duration:.2f}s\")\n",
        "                      print(f\"[{useragent.name}] Fetched HTML page: {url} in {duration:.2f}s\")\n",
        "                      useragent.visited.add(url)\n",
        "                      return (html, str(response.url), response.status)\n",
        "                  #else:\n",
        "                      #logging.warning(f\"[{useragent.name}] Skipping non-HTML content: {url} (Content-Type: {content_type})\")\n",
        "                      #return None\n",
        "              else:\n",
        "                  logging.warning(f\"[{useragent.name}] Failed to fetch {url}: HTTP {response.status} in {duration:.2f}s\")\n",
        "                  print(f\"[{useragent.name}] Failed to fetch {url}: HTTP {response.status} in {duration:.2f}s\")\n",
        "                  return None\n",
        "\n",
        "      except asyncio.TimeoutError:\n",
        "          duration = time.perf_counter() - start_time\n",
        "          logging.error(f\"[{useragent.name}] Timeout after {duration:.2f}s fetching {url}\")\n",
        "          return None\n",
        "      except aiohttp.ClientError as e:\n",
        "          duration = time.perf_counter() - start_time\n",
        "          logging.error(f\"[{useragent.name}] Client error fetching {url} after {duration:.2f}s: {e}\")\n",
        "          return None\n",
        "      except Exception as e:\n",
        "          duration = time.perf_counter() - start_time\n",
        "          logging.error(f\"[{useragent.name}] Unexpected error fetching {url} after {duration:.2f}s: {e}\")\n",
        "          return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEDuthcggqB4"
      },
      "outputs": [],
      "source": [
        "# Useragents\n",
        "useragent_dict = {\n",
        "    \"Googlebot\": UserAgentPolicy(\"Googlebot\",'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'),\n",
        "\n",
        "    \"Bingbot\": UserAgentPolicy(\"Bingbot\",'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)'),\n",
        "\n",
        "    \"Slurp\": UserAgentPolicy(\"Slurp\",'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)'),\n",
        "\n",
        "    \"DuckDuckbot\": UserAgentPolicy(\"DuckDuckbot\",'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)'),\n",
        "\n",
        "    \"Yandex\": UserAgentPolicy(\"Yandex\", 'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)'),\n",
        "\n",
        "    \"*\": UserAgentPolicy(\"*\",'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)')\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRd2irs-5LkN"
      },
      "source": [
        "#Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLoPyHWWAz6j"
      },
      "outputs": [],
      "source": [
        "# questo funziona\n",
        "class Parser:\n",
        "    def __init__(self):\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def normalize_url(self, url):\n",
        "        try:\n",
        "            uri = rfc3986.uri_reference(url).normalize()\n",
        "            return uri.unsplit()\n",
        "        except Exception as e:\n",
        "            self.logger.error(\"Error normalizing %s: %s\", url, e)\n",
        "            return url\n",
        "\n",
        "    def check_spider_traps(self, url):\n",
        "        MAX_URL_LENGTH = 200\n",
        "        MAX_PATH_DEPTH = 6\n",
        "        trap_pattern = re.compile(r\"(calendar|sessionid|track|ref|sort|date=|page=\\d{3,})\", re.IGNORECASE)\n",
        "\n",
        "        link = urlparse(url)\n",
        "\n",
        "        if len(url) > MAX_URL_LENGTH:\n",
        "            return False\n",
        "        if link.path.count('/') > MAX_PATH_DEPTH:\n",
        "            return False\n",
        "        if trap_pattern.search(url):\n",
        "            return False\n",
        "        logging.info(f\"{url} Ã¨ sicuro\")\n",
        "        return True\n",
        "\n",
        "    async def fetch_sitemap(self, session, sitemap_url):\n",
        "        try:\n",
        "            async with session.get(sitemap_url, timeout=10) as response:\n",
        "                if response.status != 200:\n",
        "                    self.logger.warning(f\"Failed to fetch {sitemap_url} (Status {response.status})\")\n",
        "                    return []\n",
        "\n",
        "                content_type = response.headers.get(\"Content-Type\", \"\").lower()\n",
        "                content = await response.text()\n",
        "\n",
        "                # XML Sitemap\n",
        "                if \"xml\" in content_type:\n",
        "                    try:\n",
        "                        root = ET.fromstring(content.encode())\n",
        "                    except ET.ParseError as e:\n",
        "                        self.logger.error(f\"XML parse error in {sitemap_url}: {e}\")\n",
        "                        return []\n",
        "\n",
        "                    namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "\n",
        "                    if root.tag.endswith('index'):\n",
        "                        sub_sitemaps = [loc.text for loc in root.findall('ns:sitemap/ns:loc', namespace)]\n",
        "                        results = []\n",
        "                        for sub_url in sub_sitemaps:\n",
        "                            results.extend(await self.fetch_sitemap(session, sub_url))\n",
        "                        return results\n",
        "\n",
        "                    elif root.tag.endswith('urlset'):\n",
        "                        return [{\n",
        "                            'url': url.findtext('ns:loc', default='', namespaces=namespace),\n",
        "                            'priority': url.findtext('ns:priority', default=None, namespaces=namespace),\n",
        "                            'update': url.findtext('ns:changefreq', default=None, namespaces=namespace)\n",
        "                        } for url in root.findall('ns:url', namespace)]\n",
        "\n",
        "                # HTML Sitemap\n",
        "                elif \"html\" in content_type:\n",
        "                    soup = BeautifulSoup(content, \"html.parser\")\n",
        "                    return [{\n",
        "                        'url': self.normalize_url(a['href']),\n",
        "                        'priority': None,\n",
        "                        'update': None\n",
        "                    } for a in soup.find_all('a', href=True)]\n",
        "\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unsupported format: {sitemap_url} ({content_type})\")\n",
        "                    return []\n",
        "\n",
        "        except aiohttp.ClientError as e:\n",
        "            self.logger.error(f\"Error fetching {sitemap_url}: {e}\")\n",
        "            return []\n",
        "\n",
        "    async def parse_sitemap(self, sitemap_list):\n",
        "        if not sitemap_list:\n",
        "            self.logger.warning(\"No sitemap available.\")\n",
        "            return pd.DataFrame(columns=['url', 'priority', 'update'])\n",
        "\n",
        "        all_entries = []\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            tasks = [self.fetch_sitemap(session, url) for url in sitemap_list if url]\n",
        "            results = await asyncio.gather(*tasks)\n",
        "\n",
        "            for entries in results:\n",
        "                for entry in entries:\n",
        "                    url = self.normalize_url(entry['url'])\n",
        "                    if self.check_spider_traps(url):\n",
        "                        entry['url'] = url\n",
        "                        all_entries.append(entry)\n",
        "\n",
        "        print(f\"sitemap urls{all_entries}\")\n",
        "        return pd.DataFrame(all_entries, columns=['url', 'priority', 'update']).drop_duplicates()\n",
        "\n",
        "    def extract_links(self, html, sitemaps_urls=None, useragent=None):\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        urls = []\n",
        "\n",
        "        for tag in soup.find_all(href=True):\n",
        "            urls.append(tag['href'])\n",
        "        for tag in soup.find_all(src=True):\n",
        "            urls.append(tag['src'])\n",
        "\n",
        "        if useragent and useragent.path_disallow:\n",
        "            urls = [url for url in urls if all(path not in url for path in useragent.path_disallow)]\n",
        "\n",
        "        urls = [self.normalize_url(url) for url in urls]\n",
        "        urls = list(set(urls))\n",
        "        urls = [url for url in urls if self.check_spider_traps(url)]\n",
        "\n",
        "        if sitemaps_urls is not None:\n",
        "            def_urls = set(sitemaps_urls)\n",
        "            def_urls.update(url for url in urls if url not in def_urls)\n",
        "            return list(def_urls)\n",
        "        else:\n",
        "            return urls\n",
        "\n",
        "    def parse_page_tags_all(self, html, tags_type=None):\n",
        "        if tags_type is None:\n",
        "            tags_type = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span', 'a']\n",
        "\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "        for tag in soup(['script', 'style', 'footer', 'nav', 'noscript', 'header', 'form', 'aside']):\n",
        "            tag.decompose()\n",
        "\n",
        "        for comment in soup.find_all(string=lambda text: isinstance(text, comment)):\n",
        "            comment.extract()\n",
        "\n",
        "        tags = soup.find_all(tags_type)\n",
        "        print(f\"sono state trovate {len(tags)} parole\")\n",
        "        return [tag.get_text(separator=' ', strip=True) for tag in tags]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6PV1Da05N3S"
      },
      "source": [
        "#Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jZnKn6Q6zAP"
      },
      "outputs": [],
      "source": [
        "\n",
        "#   Check if the necessary NLTK resources have been downloaded. Otherwise, download them.\n",
        "for resource in [\"stopwords\", \"wordnet\", \"punkt\", \"averaged_perceptron_tagger\", \"omw-1.4\"]:\n",
        "    try:\n",
        "        nltk.data.find(f\"corpora/{resource}\")\n",
        "    except LookupError:\n",
        "        nltk.download(resource, quiet=True)\n",
        "\n",
        "\n",
        "#   Words to remove during preprocessing\n",
        "NUMBER_WORDS = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten']\n",
        "\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    \"\"\"\n",
        "    Maps POS tag to WordNet format.\n",
        "    Input:\n",
        "        tag (str): POS tag in Penn Treebank format.\n",
        "    Output:\n",
        "        WordNet POS tag constant (e.g., NOUN, VERB, ADJ, ADV).\n",
        "    Description:\n",
        "        Converts common Penn Treebank POS tags to the corresponding\n",
        "        WordNet POS constants used for lemmatization.\n",
        "    \"\"\"\n",
        "    if tag.startswith('J'): return ADJ\n",
        "    if tag.startswith('V'): return VERB\n",
        "    if tag.startswith('N'): return NOUN\n",
        "    if tag.startswith('R'): return ADV\n",
        "    return NOUN\n",
        "\n",
        "\n",
        "def preprocess_sync(text):\n",
        "    \"\"\"\n",
        "    Synchronously preprocesses text by tokenizing, filtering, POS tagging,\n",
        "    and lemmatizing.\n",
        "    Input:\n",
        "        text (str): Raw input text.\n",
        "    Output:\n",
        "        list of str: Preprocessed and lemmatized tokens.\n",
        "    Description:\n",
        "        Performs tokenization, removes stopwords and numeric words, tags POS,\n",
        "        and lemmatizes tokens to their base form.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = word_tokenize(text.lower())\n",
        "    words = [w for w in words if w.isalpha() and w not in stop_words and w not in NUMBER_WORDS]\n",
        "    tagged_words = pos_tag(words)\n",
        "    return [lemmatizer.lemmatize(w, get_wordnet_pos(t)) for w, t in tagged_words]\n",
        "\n",
        "\n",
        "async def preprocess(text):\n",
        "    \"\"\"\n",
        "    Asynchronously preprocesses text by running synchronous preprocessing\n",
        "    in a separate thread.\n",
        "    Input:\n",
        "        text (str): Raw input text.\n",
        "    Output:\n",
        "        list of str: Preprocessed and lemmatized tokens.\n",
        "    Description:\n",
        "        Wraps synchronous preprocessing to avoid blocking the async event loop.\n",
        "    \"\"\"\n",
        "    return await asyncio.to_thread(preprocess_sync, text)\n",
        "\n",
        "\n",
        "async def content_page(url):\n",
        "    \"\"\"\n",
        "    Fetches HTML content from a URL asynchronously and parses text content.\n",
        "    Input:\n",
        "        url (str): URL of the webpage.\n",
        "    Output:\n",
        "        str: Concatenated textual content extracted from the page.\n",
        "    Description:\n",
        "        Uses asynchronous fetch and synchronous parser to retrieve and process\n",
        "        page content for indexing or analysis.\n",
        "    \"\"\"\n",
        "    html = await fetcher.fetch(url)\n",
        "    text_from_page = parser.parse_page_tags_all(html)\n",
        "    return ' '.join(text_from_page)\n",
        "\n",
        "\n",
        "def compute_fingerprint(text):\n",
        "    \"\"\"\n",
        "        Computes a 64-bit Simhash fingerprint for a given text.\n",
        "    Input:\n",
        "        text (str): Text to fingerprint.\n",
        "    Output:\n",
        "        int: Simhash fingerprint value.\n",
        "    Description:\n",
        "        Uses synchronous preprocessing to tokenize and lemmatize the text,\n",
        "        then computes a Simhash to represent the text compactly for near-duplicate detection.\n",
        "    \"\"\"\n",
        "    words = preprocess_sync(text)\n",
        "    return simhash(words).value\n",
        "\n",
        "\n",
        "def hamming_distance(fp1, fp2):\n",
        "    \"\"\"\n",
        "    Computes the Hamming distance between two 64-bit integer fingerprints.\n",
        "    Input:\n",
        "        fp1 (int): First fingerprint.\n",
        "        fp2 (int): Second fingerprint.\n",
        "    Output:\n",
        "        int: Number of differing bits.\n",
        "    Description:\n",
        "        Counts differing bits between two fingerprints to measure similarity.\n",
        "    \"\"\"\n",
        "    x = (fp1 ^ fp2) & ((1 << 64) - 1)\n",
        "    distance = 0\n",
        "    while x:\n",
        "        distance += 1\n",
        "        x &= x - 1\n",
        "    return distance\n",
        "\n",
        "\n",
        "def compute_age(lambda_, t):\n",
        "    \"\"\"\n",
        "    Computes the age of content using an exponential decay model.\n",
        "    Input:\n",
        "        lambda_ (float): Decay rate parameter.\n",
        "        t (float): Time since last fetch (e.g., in days).\n",
        "    Output:\n",
        "        float: Computed age score.\n",
        "    Description:\n",
        "        Models content freshness with an aging function based on decay lambda.\n",
        "    \"\"\"\n",
        "    if lambda_ == 0:\n",
        "        return t\n",
        "    return (t + lambda_ * np.exp(-lambda_ * t) - 1) / lambda_\n",
        "\n",
        "\n",
        "def calculate_page_type(content, url=\"\"):\n",
        "    \"\"\"\n",
        "    Classifies a page type based on URL and content heuristics.\n",
        "    Input:\n",
        "        content (str): Text content of the page.\n",
        "        url (str): URL of the page (optional).\n",
        "    Output:\n",
        "        str: One of \"frequent\", \"occasional\", \"static\", or \"default\".\n",
        "    Description:\n",
        "        Uses keywords and URL patterns to assign a frequency category to the page.\n",
        "    \"\"\"\n",
        "    content = content.lower()\n",
        "    url = url.lower()\n",
        "    if \"guardian\" in url or \"cnn.com\" in url or \"bbc.com\" in url:\n",
        "        if \"live\" in content and \"update\" in content:\n",
        "            return \"frequent\"\n",
        "    if \"wikipedia.org/wiki/\" in url:\n",
        "        return \"static\"\n",
        "    if \"live\" in url or \"breaking\" in url:\n",
        "        return \"frequent\"\n",
        "    if any(k in url for k in [\"calendar\", \"event\", \"workshop\", \"conference\"]):\n",
        "        return \"occasional\"\n",
        "    if any(k in url for k in [\"about\", \"privacy\", \"contact\", \"terms\"]):\n",
        "        return \"static\"\n",
        "    frequent_keywords = [\"breaking news\", \"live updates\", \"as it happens\", \"developing story\"]\n",
        "    occasional_keywords = [\"calendar\", \"workshop\", \"conference\", \"event\", \"seminar\"]\n",
        "    static_keywords = [\"contact us\", \"about us\", \"privacy policy\", \"company info\", \"terms of service\"]\n",
        "    freq_count = sum(kw in content for kw in frequent_keywords)\n",
        "    occas_count = sum(kw in content for kw in occasional_keywords)\n",
        "    static_count = sum(kw in content for kw in static_keywords)\n",
        "    if freq_count >= 2:\n",
        "        return \"frequent\"\n",
        "    if occas_count >= 1:\n",
        "        return \"occasional\"\n",
        "    if static_count >= 1:\n",
        "        return \"static\"\n",
        "    return \"default\"\n",
        "\n",
        "\n",
        "def from_gap_encoding(gaps):\n",
        "    \"\"\"\n",
        "    Decodes a list of gap-encoded positions back into absolute positions.\n",
        "    Input:\n",
        "        gaps (list of int): Gap-encoded positions.\n",
        "    Output:\n",
        "        list of int: Absolute positions.\n",
        "    Description:\n",
        "        Reconstructs absolute token positions from gap-encoded format.\n",
        "    \"\"\"\n",
        "    if not gaps:\n",
        "        return []\n",
        "    positions = [gaps[0]]\n",
        "    for gap in gaps[1:]:\n",
        "        positions.append(positions[-1] + gap)\n",
        "    return positions\n",
        "\n",
        "\n",
        "def to_gap_encoding(positions):\n",
        "    \"\"\"\n",
        "    Encodes a list of absolute positions into gap-encoded format.\n",
        "    Input:\n",
        "        positions (list of int): Absolute token positions.\n",
        "    Output:\n",
        "        list of int: Gap-encoded positions.\n",
        "    Description:\n",
        "        Compresses positions by storing gaps between successive positions.\n",
        "    \"\"\"\n",
        "    if not positions:\n",
        "        return []\n",
        "    return [positions[0]] + [positions[i] - positions[i-1] for i in range(1, len(positions))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bMIuHVH5PXu"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "#   Lambda mapping for different page types\n",
        "LAMBDA_BY_TYPE = {\n",
        "    \"frequent\": 2.0,\n",
        "    \"occasional\": 1.0,\n",
        "    \"static\": 0.05,\n",
        "    \"default\": 0.5\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "class Storage:\n",
        "\n",
        "    \"\"\"\n",
        "    Asynchronous storage manager for pages metadata and inverted index.\n",
        "\n",
        "    Attributes:\n",
        "        pages_file (str): Path to the JSON file storing pages metadata.\n",
        "        index_file (str): Path to the JSON file storing inverted index.\n",
        "        pages (dict): In-memory dictionary of pages metadata.\n",
        "        inverted_index (dict): In-memory inverted index mapping terms to postings.\n",
        "        _lock (asyncio.Lock): Async lock to protect concurrent file access.\n",
        "\n",
        "    Methods:\n",
        "        __init__(pages_file, index_file):\n",
        "            Synchronous constructor that sets file paths and initializes empty dictionaries for\n",
        "            `pages` and `inverted_index`.\n",
        "            It creates an asynchronous lock to manage concurrent write operations.\n",
        "\n",
        "        async_init():\n",
        "            Asynchronously loads data from JSON files.\n",
        "\n",
        "        _load_json_async(filename):\n",
        "            Loads JSON file asynchronously, returns dict or None.\n",
        "\n",
        "        _save_json_async(filename, data):\n",
        "            Saves data as JSON asynchronously under lock.\n",
        "\n",
        "        save_page(url, content):\n",
        "            Saves a page's metadata and indexes its terms asynchronously.\n",
        "\n",
        "        index_terms(url, content, lock_acquired=False):\n",
        "            Indexes terms of a page asynchronously, optionally assuming lock is held.\n",
        "\n",
        "        _index_terms_internal(url, content):\n",
        "            Internal method to perform term indexing and update inverted index.\n",
        "\n",
        "        get_page(url):\n",
        "            Retrieves metadata dictionary for a given URL.\n",
        "\n",
        "        get_page_type(url):\n",
        "            Retrieves the type classification of a stored page.\n",
        "\n",
        "        get_last_fetch(url):\n",
        "            Retrieves the last fetch timestamp of a stored page.\n",
        "\n",
        "        get_tf(term):\n",
        "            Retrieves term frequency postings for a term.\n",
        "\n",
        "        needs_refresh(url):\n",
        "            Determines if a page should be re-fetched based on its age and type.\n",
        "\n",
        "        is_near_duplicate(content, threshold=5):\n",
        "            Checks if the content is near-duplicate of any stored page.\n",
        "\n",
        "        commit():\n",
        "            Saves pages metadata and inverted index to disk asynchronously.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pages_file=\"data/pages.json\", index_file=\"data/inverted_index.json\"):\n",
        "        self.pages_file = pages_file\n",
        "        self.index_file = index_file\n",
        "        self.pages = {}\n",
        "        self.inverted_index = {}\n",
        "        self._lock = asyncio.Lock()\n",
        "\n",
        "    async def async_init(self):\n",
        "        self.pages = await self._load_json_async(self.pages_file) or {}\n",
        "        self.inverted_index = await self._load_json_async(self.index_file) or {}\n",
        "        logger.info(f\"Storage initialized. Loaded {len(self.pages)} pages and {len(self.inverted_index)} terms.\")\n",
        "\n",
        "    async def _load_json_async(self, filename):\n",
        "        if not os.path.exists(filename):\n",
        "            return None\n",
        "        try:\n",
        "            async with aiofiles.open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "                content = await f.read()\n",
        "            # Parse json in thread pool per sicurezza\n",
        "            return await asyncio.to_thread(json.loads, content)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load JSON file {filename}: {e}\")\n",
        "            return None\n",
        "\n",
        "    async def _save_json_async(self, filename, data):\n",
        "        if not data:\n",
        "            logger.info(f\"[DEBUG] {filename} vuoto. Scrittura saltata.\")\n",
        "            return\n",
        "        async with self._lock:\n",
        "            try:\n",
        "                async with aiofiles.open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                    await f.write(json.dumps(data, indent=2))\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to save JSON file {filename}: {e}\")\n",
        "\n",
        "    def _load_json_sync(self, filename):\n",
        "        if not os.path.exists(filename):\n",
        "            return None\n",
        "        try:\n",
        "            with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore nel caricamento di {filename}: {e}\")\n",
        "            return None\n",
        "\n",
        "    async def save_page(self, url, content):\n",
        "        async with self._lock:\n",
        "            now = time.time()\n",
        "            page_type = calculate_page_type(content, url)\n",
        "            fingerprint = compute_fingerprint(content)\n",
        "            logger.info(f\"Saving page: {url}\")\n",
        "\n",
        "            self.pages[url] = {\n",
        "                \"fingerprint\": fingerprint,\n",
        "                \"page_type\": page_type,\n",
        "                \"last_fetch\": now\n",
        "            }\n",
        "\n",
        "            await self.index_terms(url, content, lock_acquired=True)\n",
        "\n",
        "    async def index_terms(self, url, content, lock_acquired=False):\n",
        "        if not lock_acquired:\n",
        "            async with self._lock:\n",
        "                await self._index_terms_internal(url, content)\n",
        "        else:\n",
        "            await self._index_terms_internal(url, content)\n",
        "\n",
        "\n",
        "    async def _index_terms_internal(self, url, content):\n",
        "        if url not in self.pages:\n",
        "            raise ValueError(f\"URL {url} not found in pages\")\n",
        "\n",
        "        words = await preprocess(content)\n",
        "        tf = Counter()\n",
        "        positions = {}\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            tf[word] += 1\n",
        "            positions.setdefault(word, []).append(i)\n",
        "\n",
        "        for word in positions:\n",
        "            positions[word] = to_gap_encoding(positions[word])\n",
        "\n",
        "        # Remove old entries\n",
        "        for term in list(self.inverted_index.keys()):\n",
        "            if url in self.inverted_index[term]:\n",
        "                del self.inverted_index[term][url]\n",
        "                if not self.inverted_index[term]:\n",
        "                    del self.inverted_index[term]\n",
        "\n",
        "        for term in tf:\n",
        "            if term not in self.inverted_index:\n",
        "                self.inverted_index[term] = {}\n",
        "            self.inverted_index[term][url] = {\n",
        "                \"tf\": tf[term],\n",
        "                \"positions\": positions[term]\n",
        "            }\n",
        "\n",
        "        # await self._save_json_async(self.index_file, self.inverted_index)\n",
        "\n",
        "    def get_page(self, url):\n",
        "        return self.pages.get(url)\n",
        "\n",
        "    def get_page_type(self, url):\n",
        "        page = self.pages.get(url)\n",
        "        if page:\n",
        "            return page.get(\"page_type\")\n",
        "        return None\n",
        "\n",
        "    def get_last_fetch(self, url):\n",
        "        page = self.pages.get(url)\n",
        "        if page:\n",
        "            return page.get(\"last_fetch\")\n",
        "        return None\n",
        "\n",
        "    def get_tf(self, term):\n",
        "        return self.inverted_index.get(term, {})\n",
        "\n",
        "    def needs_refresh(self, url):\n",
        "        page = self.pages.get(url)\n",
        "        if not page:\n",
        "            return True\n",
        "\n",
        "        last_fetch = page.get(\"last_fetch\")\n",
        "        if last_fetch is None:\n",
        "            return True\n",
        "\n",
        "        page_type = page.get(\"page_type\", \"default\")\n",
        "\n",
        "        now = time.time()\n",
        "        time_in_days = (now - last_fetch) / 86400\n",
        "        lambda_ = LAMBDA_BY_TYPE.get(page_type, 0.5)\n",
        "        threshold = 1 / lambda_\n",
        "\n",
        "        age = compute_age(lambda_, time_in_days)\n",
        "        return age > threshold\n",
        "\n",
        "    def is_near_duplicate(self, content, threshold=5):\n",
        "        new_fp = compute_fingerprint(content)\n",
        "        for url, page_data in self.pages.items():\n",
        "            fp = page_data.get(\"fingerprint\")\n",
        "            if fp is None:\n",
        "                continue\n",
        "            try:\n",
        "                d = hamming_distance(new_fp, fp)\n",
        "                if d <= threshold:\n",
        "                    return True, url\n",
        "            except Exception:\n",
        "                continue\n",
        "        return False, None\n",
        "\n",
        "    async def commit(self):\n",
        "        logger.info(f\"[DEBUG] Tentativo di commit. pages: {len(self.pages)}, index: {len(self.inverted_index)}\")\n",
        "        if not self.pages and not self.inverted_index:\n",
        "            logger.info(\"Nessun dato da salvare. Salvataggio saltato.\")\n",
        "            return\n",
        "\n",
        "        async with self._lock:\n",
        "            await asyncio.gather(\n",
        "                self._save_json_async(self.pages_file, self.pages),\n",
        "                self._save_json_async(self.index_file, self.inverted_index)\n",
        "            )\n",
        "        logger.info(\"Files pages.json and inverted_index.json saved!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATdRG-gZ5Pvc"
      },
      "source": [
        "#Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Scheduler:\n",
        "    def __init__(self, max_concurrency, num_spiders, fetcher):\n",
        "        self.running = True\n",
        "        self.frontier = asyncio.Queue()\n",
        "        self.seen = set()\n",
        "        self.visited = set()\n",
        "        self.semaphore = asyncio.Semaphore(max_concurrency)\n",
        "        self.host_locks = defaultdict(lambda: asyncio.Lock())\n",
        "        self.retries = defaultdict(int)\n",
        "\n",
        "        self.max_concurrency = max_concurrency\n",
        "        self.num_spiders = num_spiders\n",
        "\n",
        "        self.fetcher = fetcher\n",
        "        self.parser = Parser()\n",
        "        self.storage = Storage()\n",
        "\n",
        "    async def add_url(self, url):\n",
        "        if url not in self.seen:\n",
        "            self.seen.add(url)\n",
        "            await self.frontier.put(url)\n",
        "\n",
        "    async def seed_urls(self, urls):\n",
        "        for url in urls:\n",
        "            await self.add_url(url)\n",
        "\n",
        "    def get_hostname(self, url):\n",
        "        return urlparse(url).netloc\n",
        "\n",
        "    async def get_url(self):\n",
        "        return await self.frontier.get()\n",
        "\n",
        "    def task_done(self):\n",
        "        self.frontier.task_done()\n",
        "\n",
        "    async def fetch_url(self, url):\n",
        "        hostname = self.get_hostname(url)\n",
        "\n",
        "        async with self.semaphore:\n",
        "            async with self.host_locks[hostname]:\n",
        "                response = await self.fetcher.fetch(url)\n",
        "                return response\n",
        "\n",
        "    async def process_response(self, url, response):\n",
        "        if not response:\n",
        "            logger.warning(f\"Empty response for {url}\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            content, final_url, status = response\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected response format from {url}: {e}\")\n",
        "            print(f\"Unexpected response format from {url}: {e}\")\n",
        "            return\n",
        "\n",
        "        if status != 200 or not content:\n",
        "            logger.info(f\"Skipping {url} due to status {status} or empty content.\")\n",
        "            print(f\"Skipping {url} due to status {status} or empty content.\")\n",
        "            return\n",
        "\n",
        "        await self.storage.save_page(final_url, content)\n",
        "        self.visited.add(final_url)\n",
        "\n",
        "        try:\n",
        "            links = self.parser.extract_links(content, final_url)\n",
        "            for link in links:\n",
        "                await self.add_url(link)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to parse links from {final_url}: {e}\")\n",
        "\n",
        "    async def spider(self):\n",
        "        while self.running:\n",
        "            try:\n",
        "                url = await asyncio.wait_for(self.get_url(), timeout=2)\n",
        "            except asyncio.TimeoutError:\n",
        "                break\n",
        "            response = await self.fetch_url(url)\n",
        "            await self.process_response(url, response)\n",
        "            self.task_done()\n",
        "\n",
        "    async def run(self, seeds=None):\n",
        "        if not seeds:\n",
        "            seeds = [\"https://example.com\"]\n",
        "\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            self.fetcher = Fetcher(session)\n",
        "            await self.seed_urls(seeds)\n",
        "\n",
        "\n",
        "            unique_hosts = {urlparse(url).netloc for url in seeds}\n",
        "            for host in unique_hosts:\n",
        "                site_url = f\"https://{host}\"\n",
        "                await self.fetcher.check_robots(site_url)\n",
        "\n",
        "            self.running = True  # Attiva il flag di esecuzione\n",
        "            spiders = [asyncio.create_task(self.spider()) for _ in range(self.num_spiders)]\n",
        "\n",
        "            await self.frontier.join()  # Aspetta che la coda sia vuota\n",
        "            self.running = False        # Ferma gli spider\n",
        "\n",
        "            # Attendi che gli spider escano dal loop e terminino\n",
        "            await asyncio.gather(*spiders, return_exceptions=True)\n",
        "\n",
        "            logger.info(\"Crawling finished.\")\n",
        "            print(\"Crawling finished.\")\n",
        "            logger.info(f\"Total seen URLs: {len(self.seen)}\")\n",
        "            print(f\"Total seen URLs: {len(self.seen)}\")\n",
        "            logger.info(f\"Total successfully visited URLs: {len(self.visited)}\")\n",
        "            print(f\"Total successfully visited URLs: {len(self.visited)}\")\n"
      ],
      "metadata": {
        "id": "7PYWercpRAe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzOXWyNq5SCj"
      },
      "source": [
        "#Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WICyroI1nDFK"
      },
      "source": [
        "#Test 1#\n",
        "questo funziona ma la session Ã¨ locale nello scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnNG6uzpAP_t"
      },
      "outputs": [],
      "source": [
        "#questo funziona\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class Parser:\n",
        "    def extract_links(self, content, base_url):\n",
        "        # Per semplicitÃ  nessun link estratto\n",
        "        return []\n",
        "\n",
        "class Storage:\n",
        "    async def save_page(self, url, content):\n",
        "        logger.info(f\"Saving page: {url} (content length {len(content)})\")\n",
        "\n",
        "class Scheduler:\n",
        "    def __init__(self, max_concurrency, num_spiders):\n",
        "        self.running = True\n",
        "        self.frontier = asyncio.Queue()\n",
        "        self.seen = set()\n",
        "        self.visited = set()\n",
        "        self.semaphore = asyncio.Semaphore(max_concurrency)\n",
        "        self.host_locks = defaultdict(asyncio.Lock)\n",
        "        self.retries = defaultdict(int)\n",
        "\n",
        "        self.max_concurrency = max_concurrency\n",
        "        self.num_spiders = num_spiders\n",
        "\n",
        "        self.fetcher = Fetcher(None)\n",
        "        self.parser = Parser()\n",
        "        self.storage = Storage()\n",
        "\n",
        "    async def add_url(self, url):\n",
        "        if url not in self.seen:\n",
        "            self.seen.add(url)\n",
        "            await self.frontier.put(url)\n",
        "\n",
        "    async def seed_urls(self, urls):\n",
        "        for url in urls:\n",
        "            await self.add_url(url)\n",
        "\n",
        "    def get_hostname(self, url):\n",
        "        return urlparse(url).netloc\n",
        "\n",
        "    async def get_url(self):\n",
        "        return await self.frontier.get()\n",
        "\n",
        "    def task_done(self):\n",
        "        self.frontier.task_done()\n",
        "\n",
        "    async def fetch_url(self, url):\n",
        "        hostname = self.get_hostname(url)\n",
        "\n",
        "        async with self.semaphore:\n",
        "            async with self.host_locks[hostname]:\n",
        "                response = await self.fetcher.fetch(url)\n",
        "                return response\n",
        "\n",
        "    async def process_response(self, url, response):\n",
        "        if not response:\n",
        "            logger.warning(f\"Empty response for {url}\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            content, final_url, status = response\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected response format from {url}: {e}\")\n",
        "            print(f\"Unexpected response format from {url}: {e}\")\n",
        "            return\n",
        "\n",
        "        if status != 200 or not content:\n",
        "            logger.info(f\"Skipping {url} due to status {status} or empty content.\")\n",
        "            print(f\"Skipping {url} due to status {status} or empty content.\")\n",
        "            return\n",
        "\n",
        "        await self.storage.save_page(final_url, content)\n",
        "        self.visited.add(final_url)\n",
        "\n",
        "        try:\n",
        "            links = self.parser.extract_links(content, final_url)\n",
        "            for link in links:\n",
        "                await self.add_url(link)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to parse links from {final_url}: {e}\")\n",
        "\n",
        "    async def spider(self):\n",
        "        while self.running:\n",
        "            try:\n",
        "                url = await asyncio.wait_for(self.get_url(), timeout=2)\n",
        "            except asyncio.TimeoutError:\n",
        "                break\n",
        "            response = await self.fetch_url(url)\n",
        "            await self.process_response(url, response)\n",
        "            self.task_done()\n",
        "\n",
        "    async def run(self, seeds=None):\n",
        "        if not seeds:\n",
        "            seeds = [\"https://example.com\"]\n",
        "\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            self.fetcher = Fetcher(session)\n",
        "            await self.seed_urls(seeds)\n",
        "\n",
        "\n",
        "            unique_hosts = {urlparse(url).netloc for url in seeds}\n",
        "            for host in unique_hosts:\n",
        "                site_url = f\"https://{host}\"\n",
        "                await self.fetcher.check_robots(site_url)\n",
        "\n",
        "            self.running = True  # Attiva il flag di esecuzione\n",
        "            spiders = [asyncio.create_task(self.spider()) for _ in range(self.num_spiders)]\n",
        "\n",
        "            await self.frontier.join()  # Aspetta che la coda sia vuota\n",
        "            self.running = False        # Ferma gli spider\n",
        "\n",
        "            # Attendi che gli spider escano dal loop e terminino\n",
        "            await asyncio.gather(*spiders, return_exceptions=True)\n",
        "\n",
        "            logger.info(\"Crawling finished.\")\n",
        "            print(\"Crawling finished.\")\n",
        "            logger.info(f\"Total seen URLs: {len(self.seen)}\")\n",
        "            print(f\"Total seen URLs: {len(self.seen)}\")\n",
        "            logger.info(f\"Total successfully visited URLs: {len(self.visited)}\")\n",
        "            print(f\"Total successfully visited URLs: {len(self.visited)}\")\n",
        "\n",
        "async def main():\n",
        "    seeds = [\n",
        "        \"https://www.bbc.com/\", \"https://www.wildraccoon.it/shop/\",\n",
        "\n",
        "    ]\n",
        "    scheduler = Scheduler(max_concurrency=3, num_spiders=2)\n",
        "    await scheduler.run(seeds=seeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySzLdiG_5ic2",
        "outputId": "c2860990-ac63-4041-998b-684ebf28ff9d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:[Googlebot] Unexpected error fetching /news/articles/cn840275p5yo after 0.00s: Session is closed\n",
            "WARNING:__main__:Empty response for /news/articles/cn840275p5yo\n",
            "ERROR:root:[Googlebot] Unexpected error fetching /news/scotland after 0.00s: Session is closed\n",
            "WARNING:__main__:Empty response for /news/scotland\n",
            "ERROR:root:[Googlebot] Unexpected error fetching /news/bbcverify after 0.00s: Session is closed\n",
            "WARNING:__main__:Empty response for /news/bbcverify\n",
            "ERROR:root:[Googlebot] Unexpected error fetching /news/articles/c98wyyk475no after 0.00s: Session is closed\n",
            "WARNING:__main__:Empty response for /news/articles/c98wyyk475no\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawling finished.\n",
            "Total seen URLs: 3\n",
            "Total successfully visited URLs: 3\n"
          ]
        }
      ],
      "source": [
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYGWRyQajdej"
      },
      "source": [
        "#Test 2\n",
        "\n",
        "questo funziona con la sessione esterna --> ho dvuto cambiare le impostanzioni del fetcher e dello scheduler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UuzpPGI7jfAS"
      },
      "outputs": [],
      "source": [
        "#questo funziona\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class Parser:\n",
        "    def extract_links(self, content, base_url):\n",
        "        # Per semplicitÃ  nessun link estratto\n",
        "        return []\n",
        "\n",
        "class Storage:\n",
        "    async def save_page(self, url, content):\n",
        "        logger.info(f\"Saving page: {url} (content length {len(content)})\")\n",
        "\n",
        "class Scheduler:\n",
        "    def __init__(self, max_concurrency, num_spiders, fetcher):\n",
        "        self.running = True\n",
        "        self.frontier = asyncio.Queue()\n",
        "        self.seen = set()\n",
        "        self.visited = set()\n",
        "        self.semaphore = asyncio.Semaphore(max_concurrency)\n",
        "        self.host_locks = defaultdict(lambda: asyncio.Lock())\n",
        "        self.retries = defaultdict(int)\n",
        "\n",
        "        self.max_concurrency = max_concurrency\n",
        "        self.num_spiders = num_spiders\n",
        "\n",
        "        self.fetcher = fetcher\n",
        "        self.parser = Parser()\n",
        "        self.storage = Storage()\n",
        "\n",
        "    async def add_url(self, url):\n",
        "        if url not in self.seen:\n",
        "            self.seen.add(url)\n",
        "            await self.frontier.put(url)\n",
        "\n",
        "    async def seed_urls(self, urls):\n",
        "        for url in urls:\n",
        "            await self.add_url(url)\n",
        "\n",
        "    def get_hostname(self, url):\n",
        "        return urlparse(url).netloc\n",
        "\n",
        "    async def get_url(self):\n",
        "        return await self.frontier.get()\n",
        "\n",
        "    def task_done(self):\n",
        "        self.frontier.task_done()\n",
        "\n",
        "    async def fetch_url(self, url):\n",
        "        hostname = self.get_hostname(url)\n",
        "\n",
        "        async with self.semaphore:\n",
        "            async with self.host_locks[hostname]:\n",
        "                response = await self.fetcher.fetch(url)\n",
        "                return response\n",
        "\n",
        "    async def process_response(self, url, response):\n",
        "        if not response:\n",
        "            logger.warning(f\"Empty response for {url}\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            content, final_url, status = response\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected response format from {url}: {e}\")\n",
        "            print(f\"Unexpected response format from {url}: {e}\")\n",
        "            return\n",
        "\n",
        "        if status != 200 or not content:\n",
        "            logger.info(f\"Skipping {url} due to status {status} or empty content.\")\n",
        "            print(f\"Skipping {url} due to status {status} or empty content.\")\n",
        "            return\n",
        "\n",
        "        await self.storage.save_page(final_url, content)\n",
        "        self.visited.add(final_url)\n",
        "\n",
        "        try:\n",
        "            links = self.parser.extract_links(content, final_url)\n",
        "            for link in links:\n",
        "                await self.add_url(link)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to parse links from {final_url}: {e}\")\n",
        "\n",
        "    async def spider(self):\n",
        "        while self.running:\n",
        "            try:\n",
        "                url = await asyncio.wait_for(self.get_url(), timeout=2)\n",
        "            except asyncio.TimeoutError:\n",
        "                break\n",
        "            response = await self.fetch_url(url)\n",
        "            await self.process_response(url, response)\n",
        "            self.task_done()\n",
        "\n",
        "    async def run(self, seeds=None):\n",
        "        if not seeds:\n",
        "            seeds = [\"https://example.com\"]\n",
        "\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            self.fetcher = Fetcher(session)\n",
        "            await self.seed_urls(seeds)\n",
        "\n",
        "\n",
        "            unique_hosts = {urlparse(url).netloc for url in seeds}\n",
        "            for host in unique_hosts:\n",
        "                site_url = f\"https://{host}\"\n",
        "                await self.fetcher.check_robots(site_url)\n",
        "\n",
        "            self.running = True  # Attiva il flag di esecuzione\n",
        "            spiders = [asyncio.create_task(self.spider()) for _ in range(self.num_spiders)]\n",
        "\n",
        "            await self.frontier.join()  # Aspetta che la coda sia vuota\n",
        "            self.running = False        # Ferma gli spider\n",
        "\n",
        "            # Attendi che gli spider escano dal loop e terminino\n",
        "            await asyncio.gather(*spiders, return_exceptions=True)\n",
        "\n",
        "            logger.info(\"Crawling finished.\")\n",
        "            print(\"Crawling finished.\")\n",
        "            logger.info(f\"Total seen URLs: {len(self.seen)}\")\n",
        "            print(f\"Total seen URLs: {len(self.seen)}\")\n",
        "            logger.info(f\"Total successfully visited URLs: {len(self.visited)}\")\n",
        "            print(f\"Total successfully visited URLs: {len(self.visited)}\")\n",
        "\n",
        "async def main():\n",
        "\n",
        "   async with aiohttp.ClientSession() as session:\n",
        "        fetcher = Fetcher(session)\n",
        "        scheduler = Scheduler(max_concurrency=3, num_spiders=2, fetcher=fetcher)\n",
        "        seeds = [\n",
        "        \"https://www.bbc.com/\", \"https://www.wildraccoon.it/shop/\",\n",
        "\n",
        "        ]\n",
        "        await scheduler.run(seeds=seeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVlRusU3kR3a",
        "outputId": "e8981f1b-bf59-4cb8-a1e2-8acbe9ce3ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Googlebot] Fetched HTML page: https://www.bbc.com/ in 0.01s\n",
            "[Googlebot] Fetched HTML page: https://www.wildraccoon.it/shop/ in 1.54s\n",
            "Crawling finished.\n",
            "Total seen URLs: 2\n",
            "Total successfully visited URLs: 2\n"
          ]
        }
      ],
      "source": [
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeXWpKRkLvGl"
      },
      "source": [
        "#Test 3\n",
        "ottengo tutti gli url trovati con lo scarping + parsing di una pagina"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubQKF2W5Lyja"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Storage:\n",
        "    async def save_page(self, url, content):\n",
        "        logger.info(f\"Saving page: {url} (content length {len(content)})\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgqXnvE9g6AA"
      },
      "outputs": [],
      "source": [
        "async def main():\n",
        "    seeds = [\n",
        "        \"https://www.bbc.com\",\n",
        "        \"https://hb.cran.dev/html\",\n",
        "        \"https://www.wildraccoon.it/shop/\"\n",
        "    ]\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        fetcher = Fetcher(session)\n",
        "        parser = Parser()\n",
        "        scheduler = Scheduler(max_concurrency=2, num_spiders=1, fetcher=fetcher)\n",
        "\n",
        "        await scheduler.run(seeds=seeds)\n",
        "\n",
        "        if scheduler.visited:\n",
        "            sample_url = next(iter(scheduler.visited))\n",
        "            print(f\"\\nParsing del contenuto di: {sample_url}\")\n",
        "            response = await fetcher.fetch(sample_url)  # <-- ora va bene\n",
        "            if response:\n",
        "                html, _, _ = response\n",
        "                parsed = parser.parse_page_tags_all(html)\n",
        "                for i, block in enumerate(parsed[:10]):\n",
        "                    print(f\"{i+1}. {block}\")\n",
        "            else:\n",
        "                print(\"Impossibile scaricare nuovamente la pagina.\")\n",
        "        else:\n",
        "            print(\"Nessuna pagina Ã¨ stata visitata.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh3BuA4sOEkz",
        "outputId": "8ce34c09-78c1-479b-8863-777c44916226"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Googlebot] Fetched HTML page: https://www.bbc.com in 0.02s\n"
          ]
        }
      ],
      "source": [
        "await main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8iktFyOp5Eic",
        "oRd2irs-5LkN",
        "u6PV1Da05N3S"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}