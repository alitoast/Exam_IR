{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alitoast/Exam_IR/blob/fetcher_parser-dev/disperazione.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Librerie"
      ],
      "metadata": {
        "id": "uBDLQqyN5JH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aiohttp\n",
        "!pip install asyncio\n",
        "!pip install rfc3986\n",
        "!pip install simhash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v48r7wuM5a4e",
        "outputId": "531cbe4f-59c1-4afd-9eaa-93e0e01c623f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\n",
            "Requirement already satisfied: asyncio in /usr/local/lib/python3.11/dist-packages (3.4.3)\n",
            "Requirement already satisfied: rfc3986 in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: simhash in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from simhash) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "import urllib.robotparser #per gestire il file robot.txt\n",
        "import xml.etree.ElementTree as ET  #per gestire i file xml\n",
        "import requests #per http\n",
        "import time #per gestire il tempo\n",
        "from bs4 import BeautifulSoup #per gestire il parsing del html (si puÃ² usare anche per xml)\n",
        "import rfc3986 # per la normalizzazione degli urls\n",
        "import logging\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import aiofiles\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import nltk\n",
        "import simhash\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ, ADV\n",
        "from collections import defaultdict\n"
      ],
      "metadata": {
        "id": "NBcOHHSJ5IQ_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6Jl6cEZCSy0",
        "outputId": "59e3d171-8f2b-428c-90ce-517c1f805a60"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fetcher"
      ],
      "metadata": {
        "id": "8iktFyOp5Eic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#url di prova\n",
        "# ha sitemap-index ma non priority etc.\n",
        "start_url_uno = \"https://www.dragopublisher.com/it/\"\n",
        "# non ha sitemap\n",
        "start_url_due = \"https://www.wildraccoon.it/shop/\"\n",
        "# sito con sitemap\n",
        "start_url = \"https://www.bbc.com/\""
      ],
      "metadata": {
        "id": "5YKNYw4qFTU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# questo funziona\n",
        "class UserAgentPolicy:\n",
        "    def __init__(self, name, header):\n",
        "        self.name = name\n",
        "        self.header = header\n",
        "        self.base_url = None\n",
        "        self.path_disallow = None\n",
        "        self.crawl_delay = None\n",
        "        self.request_rate = None\n",
        "        self.last_access = 1e9\n",
        "        self.visited = set()\n",
        "        self.lock = asyncio.Lock()\n",
        "\n",
        "\n",
        "class Fetcher:\n",
        "    def __init__(self, session, useragent_name=\"Googlebot\"):\n",
        "        self.session = session\n",
        "        self.useragents = {\n",
        "            \"Googlebot\": UserAgentPolicy(\"Googlebot\", 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'),\n",
        "            \"Bingbot\": UserAgentPolicy(\"Bingbot\", 'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)'),\n",
        "            \"*\": UserAgentPolicy(\"*\", 'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)'),\n",
        "        }\n",
        "        self.default_agent = self.useragents.get(useragent_name, self.useragents[\"Googlebot\"])\n",
        "\n",
        "\n",
        "    async def check_robots(self, url, useragent=None):\n",
        "        if not useragent:\n",
        "            useragent = self.default_agent\n",
        "        useragent.base_url = url\n",
        "        new_url = url.rstrip(\"/\") + \"/robots.txt\"\n",
        "\n",
        "        rfl = urllib.robotparser.RobotFileParser()\n",
        "        try:\n",
        "            async with self.session.get(new_url, timeout=10) as response:\n",
        "                if response.status == 200:\n",
        "                    robots_txt = await response.text()\n",
        "                    rfl.parse(robots_txt.splitlines())\n",
        "                    logging.info(f\"Fetched robots.txt: {new_url}\")\n",
        "                else:\n",
        "                    logging.warning(f\"Failed to fetch robots.txt: HTTP {response.status}\")\n",
        "                    return None\n",
        "        except aiohttp.ClientError as e:\n",
        "            logging.error(f\"Error fetching robots.txt: {e}\")\n",
        "            return None\n",
        "\n",
        "        useragent.path_disallow = rfl.parse(\"Disallow\")\n",
        "        useragent.crawl_delay = rfl.crawl_delay(useragent.name)\n",
        "        useragent.request_rate = rfl.request_rate(useragent.name)\n",
        "\n",
        "        return rfl.site_maps()\n",
        "\n",
        "    async def check_time(self, useragent):\n",
        "        async with useragent.lock:\n",
        "            if useragent.request_rate and useragent.crawl_delay:\n",
        "                request_delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]\n",
        "                crawl_delay = useragent.crawl_delay * 1e9\n",
        "                delay = max(request_delay, crawl_delay)\n",
        "            elif useragent.crawl_delay:\n",
        "                delay = useragent.crawl_delay * 1e9\n",
        "            elif useragent.request_rate:\n",
        "                delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]\n",
        "            else:\n",
        "                delay = 1.5 * 1e9  # default delay\n",
        "            logging.info(f\"Delay: {delay / 1e9} seconds\")\n",
        "            now = time.monotonic_ns()\n",
        "            wait = max(0, (useragent.last_access + delay) - now)\n",
        "            if wait > 0:\n",
        "                await asyncio.sleep(wait / 1e9)\n",
        "            useragent.last_access = time.monotonic_ns()\n",
        "            logging.info(f\"{useragent.name} Last access: {useragent.last_access / 1e9} seconds\")\n",
        "\n",
        "    async def fetch(self, url, useragent=None):\n",
        "      if not useragent:\n",
        "          useragent = self.default_agent\n",
        "\n",
        "      if url in useragent.visited:\n",
        "          logging.warning(f\"[{useragent.name}] Page already visited: {url}\")\n",
        "          return None\n",
        "\n",
        "      await self.check_time(useragent)\n",
        "\n",
        "      headers = {'User-Agent': useragent.header}\n",
        "\n",
        "      start_time = time.perf_counter()\n",
        "      try:\n",
        "          async with self.session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=20)) as response:\n",
        "              duration = time.perf_counter() - start_time\n",
        "\n",
        "              if response.status == 200:\n",
        "                  content_type = response.headers.get('Content-Type', '').lower()\n",
        "                  if 'text/html' in content_type:\n",
        "                      html = await response.text()\n",
        "                      logging.info(f\"[{useragent.name}] Fetched HTML page: {url} in {duration:.2f}s\")\n",
        "                      useragent.visited.add(url)\n",
        "                      return (html, str(response.url), response.status)\n",
        "                  else:\n",
        "                      logging.warning(f\"[{useragent.name}] Skipping non-HTML content: {url} (Content-Type: {content_type})\")\n",
        "                      return None\n",
        "              else:\n",
        "                  logging.warning(f\"[{useragent.name}] Failed to fetch {url}: HTTP {response.status} in {duration:.2f}s\")\n",
        "                  return None\n",
        "\n",
        "      except asyncio.TimeoutError:\n",
        "          duration = time.perf_counter() - start_time\n",
        "          logging.error(f\"[{useragent.name}] Timeout after {duration:.2f}s fetching {url}\")\n",
        "          return None\n",
        "      except aiohttp.ClientError as e:\n",
        "          duration = time.perf_counter() - start_time\n",
        "          logging.error(f\"[{useragent.name}] Client error fetching {url} after {duration:.2f}s: {e}\")\n",
        "          return None\n",
        "      except Exception as e:\n",
        "          duration = time.perf_counter() - start_time\n",
        "          logging.error(f\"[{useragent.name}] Unexpected error fetching {url} after {duration:.2f}s: {e}\")\n",
        "          return None\n"
      ],
      "metadata": {
        "id": "IaYkcwAd_pIj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Useragents\n",
        "useragent_dict = {\n",
        "    \"Googlebot\": UserAgentPolicy(\"Googlebot\",'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'),\n",
        "\n",
        "    \"Bingbot\": UserAgentPolicy(\"Bingbot\",'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)'),\n",
        "\n",
        "    \"Slurp\": UserAgentPolicy(\"Slurp\",'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)'),\n",
        "\n",
        "    \"DuckDuckbot\": UserAgentPolicy(\"DuckDuckbot\",'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)'),\n",
        "\n",
        "    \"Yandex\": UserAgentPolicy(\"Yandex\", 'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)'),\n",
        "\n",
        "    \"*\": UserAgentPolicy(\"*\",'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)')\n",
        "}\n"
      ],
      "metadata": {
        "id": "IEDuthcggqB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parser"
      ],
      "metadata": {
        "id": "oRd2irs-5LkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# questo funziona\n",
        "class Parser:\n",
        "    def __init__(self):\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def normalize_url(self, url):\n",
        "        try:\n",
        "            uri = rfc3986.uri_reference(url).normalize()\n",
        "            return uri.unsplit()\n",
        "        except Exception as e:\n",
        "            self.logger.error(\"Error normalizing %s: %s\", url, e)\n",
        "            return url\n",
        "\n",
        "    def check_spider_traps(self, url):\n",
        "        MAX_URL_LENGTH = 200\n",
        "        MAX_PATH_DEPTH = 6\n",
        "        trap_pattern = re.compile(r\"(calendar|sessionid|track|ref|sort|date=|page=\\d{3,})\", re.IGNORECASE)\n",
        "\n",
        "        link = urlparse(url)\n",
        "\n",
        "        if len(url) > MAX_URL_LENGTH:\n",
        "            return False\n",
        "        if link.path.count('/') > MAX_PATH_DEPTH:\n",
        "            return False\n",
        "        if trap_pattern.search(url):\n",
        "            return False\n",
        "        logging.info(f\"{url} Ã¨ sicuro\")\n",
        "        return True\n",
        "\n",
        "    async def fetch_sitemap(self, session, sitemap_url):\n",
        "        try:\n",
        "            async with session.get(sitemap_url, timeout=10) as response:\n",
        "                if response.status != 200:\n",
        "                    self.logger.warning(f\"Failed to fetch {sitemap_url} (Status {response.status})\")\n",
        "                    return []\n",
        "\n",
        "                content_type = response.headers.get(\"Content-Type\", \"\").lower()\n",
        "                content = await response.text()\n",
        "\n",
        "                # XML Sitemap\n",
        "                if \"xml\" in content_type:\n",
        "                    try:\n",
        "                        root = ET.fromstring(content.encode())\n",
        "                    except ET.ParseError as e:\n",
        "                        self.logger.error(f\"XML parse error in {sitemap_url}: {e}\")\n",
        "                        return []\n",
        "\n",
        "                    namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "\n",
        "                    if root.tag.endswith('index'):\n",
        "                        sub_sitemaps = [loc.text for loc in root.findall('ns:sitemap/ns:loc', namespace)]\n",
        "                        results = []\n",
        "                        for sub_url in sub_sitemaps:\n",
        "                            results.extend(await self.fetch_sitemap(session, sub_url))\n",
        "                        return results\n",
        "\n",
        "                    elif root.tag.endswith('urlset'):\n",
        "                        return [{\n",
        "                            'url': url.findtext('ns:loc', default='', namespaces=namespace),\n",
        "                            'priority': url.findtext('ns:priority', default=None, namespaces=namespace),\n",
        "                            'update': url.findtext('ns:changefreq', default=None, namespaces=namespace)\n",
        "                        } for url in root.findall('ns:url', namespace)]\n",
        "\n",
        "                # HTML Sitemap\n",
        "                elif \"html\" in content_type:\n",
        "                    soup = BeautifulSoup(content, \"html.parser\")\n",
        "                    return [{\n",
        "                        'url': self.normalize_url(a['href']),\n",
        "                        'priority': None,\n",
        "                        'update': None\n",
        "                    } for a in soup.find_all('a', href=True)]\n",
        "\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unsupported format: {sitemap_url} ({content_type})\")\n",
        "                    return []\n",
        "\n",
        "        except aiohttp.ClientError as e:\n",
        "            self.logger.error(f\"Error fetching {sitemap_url}: {e}\")\n",
        "            return []\n",
        "\n",
        "    async def parse_sitemap(self, sitemap_list):\n",
        "        if not sitemap_list:\n",
        "            self.logger.warning(\"No sitemap available.\")\n",
        "            return pd.DataFrame(columns=['url', 'priority', 'update'])\n",
        "\n",
        "        all_entries = []\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            tasks = [self.fetch_sitemap(session, url) for url in sitemap_list if url]\n",
        "            results = await asyncio.gather(*tasks)\n",
        "\n",
        "            for entries in results:\n",
        "                for entry in entries:\n",
        "                    url = self.normalize_url(entry['url'])\n",
        "                    if self.check_spider_traps(url):\n",
        "                        entry['url'] = url\n",
        "                        all_entries.append(entry)\n",
        "\n",
        "        print(f\"sitemap urls{all_entries}\")\n",
        "        return pd.DataFrame(all_entries, columns=['url', 'priority', 'update']).drop_duplicates()\n",
        "\n",
        "    def extract_links(self, html, sitemaps_urls=None, useragent=None):\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        urls = []\n",
        "\n",
        "        for tag in soup.find_all(href=True):\n",
        "            urls.append(tag['href'])\n",
        "        for tag in soup.find_all(src=True):\n",
        "            urls.append(tag['src'])\n",
        "\n",
        "        if useragent and useragent.path_disallow:\n",
        "            urls = [url for url in urls if all(path not in url for path in useragent.path_disallow)]\n",
        "\n",
        "        urls = [self.normalize_url(url) for url in urls]\n",
        "        urls = list(set(urls))\n",
        "        urls = [url for url in urls if self.check_spider_traps(url)]\n",
        "\n",
        "        if sitemaps_urls is not None:\n",
        "            def_urls = set(sitemaps_urls)\n",
        "            def_urls.update(url for url in urls if url not in def_urls)\n",
        "            return list(def_urls)\n",
        "        else:\n",
        "            return urls\n",
        "\n",
        "    def parse_page_tags_all(self, html, tags_type=None):\n",
        "        if tags_type is None:\n",
        "            tags_type = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span', 'a']\n",
        "\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "        for tag in soup(['script', 'style', 'footer', 'nav', 'noscript', 'header', 'form', 'aside']):\n",
        "            tag.decompose()\n",
        "\n",
        "        for comment in soup.find_all(string=lambda text: isinstance(text, comment)):\n",
        "            comment.extract()\n",
        "\n",
        "        tags = soup.find_all(tags_type)\n",
        "        print(f\"sono state trovate {len(tags)} parole\")\n",
        "        return [tag.get_text(separator=' ', strip=True) for tag in tags]\n"
      ],
      "metadata": {
        "id": "qLoPyHWWAz6j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Storage"
      ],
      "metadata": {
        "id": "u6PV1Da05N3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#   Check if the necessary NLTK resources have been downloaded. Otherwise, download them.\n",
        "for resource in [\"stopwords\", \"wordnet\", \"punkt\", \"averaged_perceptron_tagger\", \"omw-1.4\"]:\n",
        "    try:\n",
        "        nltk.data.find(f\"corpora/{resource}\")\n",
        "    except LookupError:\n",
        "        nltk.download(resource, quiet=True)\n",
        "\n",
        "\n",
        "#   Words to remove during preprocessing\n",
        "NUMBER_WORDS = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten']\n",
        "\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    \"\"\"\n",
        "    Maps POS tag to WordNet format.\n",
        "    Input:\n",
        "        tag (str): POS tag in Penn Treebank format.\n",
        "    Output:\n",
        "        WordNet POS tag constant (e.g., NOUN, VERB, ADJ, ADV).\n",
        "    Description:\n",
        "        Converts common Penn Treebank POS tags to the corresponding\n",
        "        WordNet POS constants used for lemmatization.\n",
        "    \"\"\"\n",
        "    if tag.startswith('J'): return ADJ\n",
        "    if tag.startswith('V'): return VERB\n",
        "    if tag.startswith('N'): return NOUN\n",
        "    if tag.startswith('R'): return ADV\n",
        "    return NOUN\n",
        "\n",
        "\n",
        "def preprocess_sync(text):\n",
        "    \"\"\"\n",
        "    Synchronously preprocesses text by tokenizing, filtering, POS tagging,\n",
        "    and lemmatizing.\n",
        "    Input:\n",
        "        text (str): Raw input text.\n",
        "    Output:\n",
        "        list of str: Preprocessed and lemmatized tokens.\n",
        "    Description:\n",
        "        Performs tokenization, removes stopwords and numeric words, tags POS,\n",
        "        and lemmatizes tokens to their base form.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = word_tokenize(text.lower())\n",
        "    words = [w for w in words if w.isalpha() and w not in stop_words and w not in NUMBER_WORDS]\n",
        "    tagged_words = pos_tag(words)\n",
        "    return [lemmatizer.lemmatize(w, get_wordnet_pos(t)) for w, t in tagged_words]\n",
        "\n",
        "\n",
        "async def preprocess(text):\n",
        "    \"\"\"\n",
        "    Asynchronously preprocesses text by running synchronous preprocessing\n",
        "    in a separate thread.\n",
        "    Input:\n",
        "        text (str): Raw input text.\n",
        "    Output:\n",
        "        list of str: Preprocessed and lemmatized tokens.\n",
        "    Description:\n",
        "        Wraps synchronous preprocessing to avoid blocking the async event loop.\n",
        "    \"\"\"\n",
        "    return await asyncio.to_thread(preprocess_sync, text)\n",
        "\n",
        "\n",
        "async def content_page(url):\n",
        "    \"\"\"\n",
        "    Fetches HTML content from a URL asynchronously and parses text content.\n",
        "    Input:\n",
        "        url (str): URL of the webpage.\n",
        "    Output:\n",
        "        str: Concatenated textual content extracted from the page.\n",
        "    Description:\n",
        "        Uses asynchronous fetch and synchronous parser to retrieve and process\n",
        "        page content for indexing or analysis.\n",
        "    \"\"\"\n",
        "    html = await fetcher.fetch(url)\n",
        "    text_from_page = parser.parse_page_tags_all(html)\n",
        "    return ' '.join(text_from_page)\n",
        "\n",
        "\n",
        "def compute_fingerprint(text):\n",
        "    \"\"\"\n",
        "        Computes a 64-bit Simhash fingerprint for a given text.\n",
        "    Input:\n",
        "        text (str): Text to fingerprint.\n",
        "    Output:\n",
        "        int: Simhash fingerprint value.\n",
        "    Description:\n",
        "        Uses synchronous preprocessing to tokenize and lemmatize the text,\n",
        "        then computes a Simhash to represent the text compactly for near-duplicate detection.\n",
        "    \"\"\"\n",
        "    words = preprocess_sync(text)\n",
        "    return simhash(words).value\n",
        "\n",
        "\n",
        "def hamming_distance(fp1, fp2):\n",
        "    \"\"\"\n",
        "    Computes the Hamming distance between two 64-bit integer fingerprints.\n",
        "    Input:\n",
        "        fp1 (int): First fingerprint.\n",
        "        fp2 (int): Second fingerprint.\n",
        "    Output:\n",
        "        int: Number of differing bits.\n",
        "    Description:\n",
        "        Counts differing bits between two fingerprints to measure similarity.\n",
        "    \"\"\"\n",
        "    x = (fp1 ^ fp2) & ((1 << 64) - 1)\n",
        "    distance = 0\n",
        "    while x:\n",
        "        distance += 1\n",
        "        x &= x - 1\n",
        "    return distance\n",
        "\n",
        "\n",
        "def compute_age(lambda_, t):\n",
        "    \"\"\"\n",
        "    Computes the age of content using an exponential decay model.\n",
        "    Input:\n",
        "        lambda_ (float): Decay rate parameter.\n",
        "        t (float): Time since last fetch (e.g., in days).\n",
        "    Output:\n",
        "        float: Computed age score.\n",
        "    Description:\n",
        "        Models content freshness with an aging function based on decay lambda.\n",
        "    \"\"\"\n",
        "    if lambda_ == 0:\n",
        "        return t\n",
        "    return (t + lambda_ * np.exp(-lambda_ * t) - 1) / lambda_\n",
        "\n",
        "\n",
        "def calculate_page_type(content, url=\"\"):\n",
        "    \"\"\"\n",
        "    Classifies a page type based on URL and content heuristics.\n",
        "    Input:\n",
        "        content (str): Text content of the page.\n",
        "        url (str): URL of the page (optional).\n",
        "    Output:\n",
        "        str: One of \"frequent\", \"occasional\", \"static\", or \"default\".\n",
        "    Description:\n",
        "        Uses keywords and URL patterns to assign a frequency category to the page.\n",
        "    \"\"\"\n",
        "    content = content.lower()\n",
        "    url = url.lower()\n",
        "    if \"guardian\" in url or \"cnn.com\" in url or \"bbc.com\" in url:\n",
        "        if \"live\" in content and \"update\" in content:\n",
        "            return \"frequent\"\n",
        "    if \"wikipedia.org/wiki/\" in url:\n",
        "        return \"static\"\n",
        "    if \"live\" in url or \"breaking\" in url:\n",
        "        return \"frequent\"\n",
        "    if any(k in url for k in [\"calendar\", \"event\", \"workshop\", \"conference\"]):\n",
        "        return \"occasional\"\n",
        "    if any(k in url for k in [\"about\", \"privacy\", \"contact\", \"terms\"]):\n",
        "        return \"static\"\n",
        "    frequent_keywords = [\"breaking news\", \"live updates\", \"as it happens\", \"developing story\"]\n",
        "    occasional_keywords = [\"calendar\", \"workshop\", \"conference\", \"event\", \"seminar\"]\n",
        "    static_keywords = [\"contact us\", \"about us\", \"privacy policy\", \"company info\", \"terms of service\"]\n",
        "    freq_count = sum(kw in content for kw in frequent_keywords)\n",
        "    occas_count = sum(kw in content for kw in occasional_keywords)\n",
        "    static_count = sum(kw in content for kw in static_keywords)\n",
        "    if freq_count >= 2:\n",
        "        return \"frequent\"\n",
        "    if occas_count >= 1:\n",
        "        return \"occasional\"\n",
        "    if static_count >= 1:\n",
        "        return \"static\"\n",
        "    return \"default\"\n",
        "\n",
        "\n",
        "def from_gap_encoding(gaps):\n",
        "    \"\"\"\n",
        "    Decodes a list of gap-encoded positions back into absolute positions.\n",
        "    Input:\n",
        "        gaps (list of int): Gap-encoded positions.\n",
        "    Output:\n",
        "        list of int: Absolute positions.\n",
        "    Description:\n",
        "        Reconstructs absolute token positions from gap-encoded format.\n",
        "    \"\"\"\n",
        "    if not gaps:\n",
        "        return []\n",
        "    positions = [gaps[0]]\n",
        "    for gap in gaps[1:]:\n",
        "        positions.append(positions[-1] + gap)\n",
        "    return positions\n",
        "\n",
        "\n",
        "def to_gap_encoding(positions):\n",
        "    \"\"\"\n",
        "    Encodes a list of absolute positions into gap-encoded format.\n",
        "    Input:\n",
        "        positions (list of int): Absolute token positions.\n",
        "    Output:\n",
        "        list of int: Gap-encoded positions.\n",
        "    Description:\n",
        "        Compresses positions by storing gaps between successive positions.\n",
        "    \"\"\"\n",
        "    if not positions:\n",
        "        return []\n",
        "    return [positions[0]] + [positions[i] - positions[i-1] for i in range(1, len(positions))]"
      ],
      "metadata": {
        "id": "5jZnKn6Q6zAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#   Lambda mapping for different page types\n",
        "LAMBDA_BY_TYPE = {\n",
        "    \"frequent\": 2.0,\n",
        "    \"occasional\": 1.0,\n",
        "    \"static\": 0.05,\n",
        "    \"default\": 0.5\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "class Storage:\n",
        "\n",
        "    \"\"\"\n",
        "    Asynchronous storage manager for pages metadata and inverted index.\n",
        "\n",
        "    Attributes:\n",
        "        pages_file (str): Path to the JSON file storing pages metadata.\n",
        "        index_file (str): Path to the JSON file storing inverted index.\n",
        "        pages (dict): In-memory dictionary of pages metadata.\n",
        "        inverted_index (dict): In-memory inverted index mapping terms to postings.\n",
        "        _lock (asyncio.Lock): Async lock to protect concurrent file access.\n",
        "\n",
        "    Methods:\n",
        "        __init__(pages_file, index_file):\n",
        "            Synchronous constructor that sets file paths and initializes empty dictionaries for\n",
        "            `pages` and `inverted_index`.\n",
        "            It creates an asynchronous lock to manage concurrent write operations.\n",
        "\n",
        "        async_init():\n",
        "            Asynchronously loads data from JSON files.\n",
        "\n",
        "        _load_json_async(filename):\n",
        "            Loads JSON file asynchronously, returns dict or None.\n",
        "\n",
        "        _save_json_async(filename, data):\n",
        "            Saves data as JSON asynchronously under lock.\n",
        "\n",
        "        save_page(url, content):\n",
        "            Saves a page's metadata and indexes its terms asynchronously.\n",
        "\n",
        "        index_terms(url, content, lock_acquired=False):\n",
        "            Indexes terms of a page asynchronously, optionally assuming lock is held.\n",
        "\n",
        "        _index_terms_internal(url, content):\n",
        "            Internal method to perform term indexing and update inverted index.\n",
        "\n",
        "        get_page(url):\n",
        "            Retrieves metadata dictionary for a given URL.\n",
        "\n",
        "        get_page_type(url):\n",
        "            Retrieves the type classification of a stored page.\n",
        "\n",
        "        get_last_fetch(url):\n",
        "            Retrieves the last fetch timestamp of a stored page.\n",
        "\n",
        "        get_tf(term):\n",
        "            Retrieves term frequency postings for a term.\n",
        "\n",
        "        needs_refresh(url):\n",
        "            Determines if a page should be re-fetched based on its age and type.\n",
        "\n",
        "        is_near_duplicate(content, threshold=5):\n",
        "            Checks if the content is near-duplicate of any stored page.\n",
        "\n",
        "        commit():\n",
        "            Saves pages metadata and inverted index to disk asynchronously.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pages_file=\"data/pages.json\", index_file=\"data/inverted_index.json\"):\n",
        "        self.pages_file = pages_file\n",
        "        self.index_file = index_file\n",
        "        self.pages = {}\n",
        "        self.inverted_index = {}\n",
        "        self._lock = asyncio.Lock()\n",
        "\n",
        "    async def async_init(self):\n",
        "        self.pages = await self._load_json_async(self.pages_file) or {}\n",
        "        self.inverted_index = await self._load_json_async(self.index_file) or {}\n",
        "        logger.info(f\"Storage initialized. Loaded {len(self.pages)} pages and {len(self.inverted_index)} terms.\")\n",
        "\n",
        "    async def _load_json_async(self, filename):\n",
        "        if not os.path.exists(filename):\n",
        "            return None\n",
        "        try:\n",
        "            async with aiofiles.open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "                content = await f.read()\n",
        "            # Parse json in thread pool per sicurezza\n",
        "            return await asyncio.to_thread(json.loads, content)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load JSON file {filename}: {e}\")\n",
        "            return None\n",
        "\n",
        "    async def _save_json_async(self, filename, data):\n",
        "        if not data:\n",
        "            logger.info(f\"[DEBUG] {filename} vuoto. Scrittura saltata.\")\n",
        "            return\n",
        "        async with self._lock:\n",
        "            try:\n",
        "                async with aiofiles.open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                    await f.write(json.dumps(data, indent=2))\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to save JSON file {filename}: {e}\")\n",
        "\n",
        "    def _load_json_sync(self, filename):\n",
        "        if not os.path.exists(filename):\n",
        "            return None\n",
        "        try:\n",
        "            with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore nel caricamento di {filename}: {e}\")\n",
        "            return None\n",
        "\n",
        "    async def save_page(self, url, content):\n",
        "        async with self._lock:\n",
        "            now = time.time()\n",
        "            page_type = calculate_page_type(content, url)\n",
        "            fingerprint = compute_fingerprint(content)\n",
        "            logger.info(f\"Saving page: {url}\")\n",
        "\n",
        "            self.pages[url] = {\n",
        "                \"fingerprint\": fingerprint,\n",
        "                \"page_type\": page_type,\n",
        "                \"last_fetch\": now\n",
        "            }\n",
        "\n",
        "            await self.index_terms(url, content, lock_acquired=True)\n",
        "\n",
        "    async def index_terms(self, url, content, lock_acquired=False):\n",
        "        if not lock_acquired:\n",
        "            async with self._lock:\n",
        "                await self._index_terms_internal(url, content)\n",
        "        else:\n",
        "            await self._index_terms_internal(url, content)\n",
        "\n",
        "\n",
        "    async def _index_terms_internal(self, url, content):\n",
        "        if url not in self.pages:\n",
        "            raise ValueError(f\"URL {url} not found in pages\")\n",
        "\n",
        "        words = await preprocess(content)\n",
        "        tf = Counter()\n",
        "        positions = {}\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            tf[word] += 1\n",
        "            positions.setdefault(word, []).append(i)\n",
        "\n",
        "        for word in positions:\n",
        "            positions[word] = to_gap_encoding(positions[word])\n",
        "\n",
        "        # Remove old entries\n",
        "        for term in list(self.inverted_index.keys()):\n",
        "            if url in self.inverted_index[term]:\n",
        "                del self.inverted_index[term][url]\n",
        "                if not self.inverted_index[term]:\n",
        "                    del self.inverted_index[term]\n",
        "\n",
        "        for term in tf:\n",
        "            if term not in self.inverted_index:\n",
        "                self.inverted_index[term] = {}\n",
        "            self.inverted_index[term][url] = {\n",
        "                \"tf\": tf[term],\n",
        "                \"positions\": positions[term]\n",
        "            }\n",
        "\n",
        "        # await self._save_json_async(self.index_file, self.inverted_index)\n",
        "\n",
        "    def get_page(self, url):\n",
        "        return self.pages.get(url)\n",
        "\n",
        "    def get_page_type(self, url):\n",
        "        page = self.pages.get(url)\n",
        "        if page:\n",
        "            return page.get(\"page_type\")\n",
        "        return None\n",
        "\n",
        "    def get_last_fetch(self, url):\n",
        "        page = self.pages.get(url)\n",
        "        if page:\n",
        "            return page.get(\"last_fetch\")\n",
        "        return None\n",
        "\n",
        "    def get_tf(self, term):\n",
        "        return self.inverted_index.get(term, {})\n",
        "\n",
        "    def needs_refresh(self, url):\n",
        "        page = self.pages.get(url)\n",
        "        if not page:\n",
        "            return True\n",
        "\n",
        "        last_fetch = page.get(\"last_fetch\")\n",
        "        if last_fetch is None:\n",
        "            return True\n",
        "\n",
        "        page_type = page.get(\"page_type\", \"default\")\n",
        "\n",
        "        now = time.time()\n",
        "        time_in_days = (now - last_fetch) / 86400\n",
        "        lambda_ = LAMBDA_BY_TYPE.get(page_type, 0.5)\n",
        "        threshold = 1 / lambda_\n",
        "\n",
        "        age = compute_age(lambda_, time_in_days)\n",
        "        return age > threshold\n",
        "\n",
        "    def is_near_duplicate(self, content, threshold=5):\n",
        "        new_fp = compute_fingerprint(content)\n",
        "        for url, page_data in self.pages.items():\n",
        "            fp = page_data.get(\"fingerprint\")\n",
        "            if fp is None:\n",
        "                continue\n",
        "            try:\n",
        "                d = hamming_distance(new_fp, fp)\n",
        "                if d <= threshold:\n",
        "                    return True, url\n",
        "            except Exception:\n",
        "                continue\n",
        "        return False, None\n",
        "\n",
        "    async def commit(self):\n",
        "        logger.info(f\"[DEBUG] Tentativo di commit. pages: {len(self.pages)}, index: {len(self.inverted_index)}\")\n",
        "        if not self.pages and not self.inverted_index:\n",
        "            logger.info(\"Nessun dato da salvare. Salvataggio saltato.\")\n",
        "            return\n",
        "\n",
        "        async with self._lock:\n",
        "            await asyncio.gather(\n",
        "                self._save_json_async(self.pages_file, self.pages),\n",
        "                self._save_json_async(self.index_file, self.inverted_index)\n",
        "            )\n",
        "        logger.info(\"Files pages.json and inverted_index.json saved!\")\n"
      ],
      "metadata": {
        "id": "8bMIuHVH5PXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scheduler"
      ],
      "metadata": {
        "id": "ATdRG-gZ5Pvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Scheduler:\n",
        "    def __init__(self, max_concurrency, num_spiders):\n",
        "        self.running = False\n",
        "        self.frontier = asyncio.Queue() # URLs to crawl\n",
        "        self.seen = set()   # tracks seen URLs\n",
        "        self.visited = set()    # tracks visited URLs\n",
        "        self.semaphore = asyncio.Semaphore(max_concurrency) # limits max parallel fetches\n",
        "        self.host_locks = defaultdict(asyncio.Lock) #{}  # ensures one fetch per host at a time WILL REUSE SAME LOCK?\n",
        "        self.retries = defaultdict(int) # dictionary keeps count of how many times retried each URL\n",
        "\n",
        "        self.max_concurrency = max_concurrency\n",
        "        self.num_spiders = num_spiders\n",
        "\n",
        "        self.fetcher = Fetcher(None)\n",
        "        self.parser = Parser()\n",
        "        self.storage = Storage ()\n",
        "\n",
        "    async def add_url(self, url):\n",
        "        \"\"\"\n",
        "        Add a new URL to the frontier if it hasn't been seen.\n",
        "        \"\"\"\n",
        "        if url not in self.seen:\n",
        "            self.seen.add(url)\n",
        "            await self.frontier.put(url)\n",
        "\n",
        "    async def seed_urls(self, urls):\n",
        "        \"\"\"\n",
        "        Seeds the initial URL, or list of URLs, into the queue.\n",
        "        \"\"\"\n",
        "        for url in urls:\n",
        "            await self.add_url(url)   # passing it to add_url for check\n",
        "\n",
        "    def get_hostname(self, url):\n",
        "        \"\"\"\n",
        "        Extract the hostname from a URL.\n",
        "        \"\"\"\n",
        "        return urlparse(url).netloc\n",
        "\n",
        "    async def get_url(self):\n",
        "        \"\"\"\n",
        "        Get the next URL from the frontier.\n",
        "        Wait asynchronously for a URL to be available in the queue.\n",
        "        \"\"\"\n",
        "        return await self.frontier.get()\n",
        "\n",
        "    def task_done(self):\n",
        "        \"\"\"\n",
        "        Mark the current task as completed in the frontier.\n",
        "        \"\"\"\n",
        "        self.frontier.task_done()\n",
        "\n",
        "    async def handle_fetch_failure(self, url, exception):\n",
        "        \"\"\"\n",
        "        Handles a failed fetch attempt: retries up to 2 times.\n",
        "        \"\"\"\n",
        "        logger.error(f\"Fetch failed for {url}: {exception}\")\n",
        "        self.retries[url] += 1\n",
        "\n",
        "        if self.retries[url] <= 2:\n",
        "            logger.info(f\"Re-queuing {url} (attempt {self.retries[url]})\")\n",
        "            await self.frontier.put(url)\n",
        "        else:\n",
        "            logger.info(f\"Giving up on {url} after {self.retries[url]} attempts.\")\n",
        "\n",
        "    async def fetch_url(self, url):\n",
        "        \"\"\"\n",
        "        Fetches a URL with concurrency and politeness constraints.\n",
        "        Handles retry on failure.\n",
        "        \"\"\"\n",
        "        hostname = self.get_hostname(url)\n",
        "\n",
        "        # enforce both global fetch concurrency and per-host politeness\n",
        "        async with self.semaphore:\n",
        "            async with self.host_locks[hostname]: ## get_host_lock(hostname)\n",
        "                logger.info(f\"Fetching {url}\")\n",
        "                start_time = time.perf_counter()    # want to measure time taken for each request\n",
        "                try:\n",
        "                    response = await self.fetcher.fetch(url)\n",
        "                    duration = time.perf_counter() - start_time\n",
        "                    logger.info(f\"Fetched {url} in {duration:.2f}s\")\n",
        "                    return response\n",
        "                except aiohttp.ClientError as e:\n",
        "                    duration = time.perf_counter() - start_time\n",
        "                    logger.error(f\"Fetch failed for {url} after {duration:.2f}s: {e}\")\n",
        "                    await self.handle_fetch_failure(url, e)\n",
        "                    return None\n",
        "                except Exception as e:\n",
        "                    duration = time.perf_counter() - start_time\n",
        "                    logger.error(f\"Fetch failed for {url} after {duration:.2f}s: {e}\")\n",
        "                    await self.handle_fetch_failure(url, e)\n",
        "                    return None\n",
        "\n",
        "    async def process_response(self, url, response):\n",
        "        \"\"\"\n",
        "        Handles a successful fetch: parses, stores, and queues new URLs.\n",
        "        \"\"\"\n",
        "        if not response:\n",
        "            logger.warning(f\"Empty response for {url}\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            content, final_url, status = response\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected response format from {url}: {e}\")\n",
        "            return\n",
        "\n",
        "        # Skip non-successful responses or empty content\n",
        "        if status != 200 or not content:\n",
        "            logger.info(f\"Skipping {url} due to status {status} or empty content.\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            await self.storage.save_page(final_url, content)\n",
        "            logger.info(f\"Saved {final_url}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save {final_url}: {e}\")\n",
        "            return\n",
        "\n",
        "        # mark as successfully visited only after processing\n",
        "        self.visited.add(final_url)\n",
        "\n",
        "        try:\n",
        "            # Extract and enqueue links found in the page\n",
        "            links = self.parser.extract_links(content, final_url)\n",
        "            for link in links:\n",
        "                await self.add_url(link)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to parse links from {final_url}: {e}\")\n",
        "\n",
        "    async def spider(self):\n",
        "        while self.running:\n",
        "            try:\n",
        "                url = await asyncio.wait_for(self.get_url(), timeout=5)\n",
        "            except asyncio.TimeoutError:\n",
        "                # Se nessuna URL arriva per 5 secondi, esci dal loop\n",
        "                break\n",
        "\n",
        "            response = await self.fetch_url(url)\n",
        "            await self.process_response(url, response)\n",
        "            self.task_done()\n",
        "\n",
        "    async def run(self, seeds=None):\n",
        "        if not seeds:\n",
        "            seeds = [\"https://example.com\"]\n",
        "\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            self.fetcher = Fetcher(session)\n",
        "            await self.seed_urls(seeds)\n",
        "\n",
        "            unique_hosts = {urlparse(url).netloc for url in seeds}\n",
        "            for host in unique_hosts:\n",
        "                site_url = f\"https://{host}\"\n",
        "                await self.fetcher.check_robots(site_url)\n",
        "\n",
        "            self.running = True  # Attiva il flag di esecuzione\n",
        "            spiders = [asyncio.create_task(self.spider()) for _ in range(self.num_spiders)]\n",
        "\n",
        "            await self.frontier.join()  # Aspetta che la coda sia vuota\n",
        "            self.running = False        # Ferma gli spider\n",
        "\n",
        "            # Attendi che gli spider escano dal loop e terminino\n",
        "            await asyncio.gather(*spiders, return_exceptions=True)\n",
        "\n",
        "            logger.info(\"Crawling finished.\")\n",
        "            logger.info(f\"Total seen URLs: {len(self.seen)}\")\n",
        "            logger.info(f\"Total successfully visited URLs: {len(self.visited)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KnXaaw315RYg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "0ietvUWQt2Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "vzOXWyNq5SCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test 1#\n",
        "questo funziona"
      ],
      "metadata": {
        "id": "WICyroI1nDFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#questo funziona\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class Parser:\n",
        "    def extract_links(self, content, base_url):\n",
        "        # Per semplicitÃ  nessun link estratto\n",
        "        return []\n",
        "\n",
        "class Storage:\n",
        "    async def save_page(self, url, content):\n",
        "        logger.info(f\"Saving page: {url} (content length {len(content)})\")\n",
        "\n",
        "class Scheduler:\n",
        "    def __init__(self, max_concurrency, num_spiders):\n",
        "        self.running = True\n",
        "        self.frontier = asyncio.Queue()\n",
        "        self.seen = set()\n",
        "        self.visited = set()\n",
        "        self.semaphore = asyncio.Semaphore(max_concurrency)\n",
        "        self.host_locks = defaultdict(asyncio.Lock)\n",
        "        self.retries = defaultdict(int)\n",
        "\n",
        "        self.max_concurrency = max_concurrency\n",
        "        self.num_spiders = num_spiders\n",
        "\n",
        "        self.fetcher = Fetcher(None)\n",
        "        self.parser = Parser()\n",
        "        self.storage = Storage()\n",
        "\n",
        "    async def add_url(self, url):\n",
        "        if url not in self.seen:\n",
        "            self.seen.add(url)\n",
        "            await self.frontier.put(url)\n",
        "\n",
        "    async def seed_urls(self, urls):\n",
        "        for url in urls:\n",
        "            await self.add_url(url)\n",
        "\n",
        "    def get_hostname(self, url):\n",
        "        return urlparse(url).netloc\n",
        "\n",
        "    async def get_url(self):\n",
        "        return await self.frontier.get()\n",
        "\n",
        "    def task_done(self):\n",
        "        self.frontier.task_done()\n",
        "\n",
        "    async def fetch_url(self, url):\n",
        "        hostname = self.get_hostname(url)\n",
        "\n",
        "        async with self.semaphore:\n",
        "            async with self.host_locks[hostname]:\n",
        "                response = await self.fetcher.fetch(url)\n",
        "                return response\n",
        "\n",
        "    async def process_response(self, url, response):\n",
        "        if not response:\n",
        "            logger.warning(f\"Empty response for {url}\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            content, final_url, status = response\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected response format from {url}: {e}\")\n",
        "            print(f\"Unexpected response format from {url}: {e}\")\n",
        "            return\n",
        "\n",
        "        if status != 200 or not content:\n",
        "            logger.info(f\"Skipping {url} due to status {status} or empty content.\")\n",
        "            print(f\"Skipping {url} due to status {status} or empty content.\")\n",
        "            return\n",
        "\n",
        "        await self.storage.save_page(final_url, content)\n",
        "        self.visited.add(final_url)\n",
        "\n",
        "        try:\n",
        "            links = self.parser.extract_links(content, final_url)\n",
        "            for link in links:\n",
        "                await self.add_url(link)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to parse links from {final_url}: {e}\")\n",
        "\n",
        "    async def spider(self):\n",
        "        while self.running:\n",
        "            try:\n",
        "                url = await asyncio.wait_for(self.get_url(), timeout=2)\n",
        "            except asyncio.TimeoutError:\n",
        "                break\n",
        "            response = await self.fetch_url(url)\n",
        "            await self.process_response(url, response)\n",
        "            self.task_done()\n",
        "\n",
        "    async def run(self, seeds=None):\n",
        "        if not seeds:\n",
        "            seeds = [\"https://example.com\"]\n",
        "\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            self.fetcher = Fetcher(session)\n",
        "            await self.seed_urls(seeds)\n",
        "\n",
        "            # â CHECK robots.txt UNA VOLTA per ogni dominio nei seed\n",
        "            unique_hosts = {urlparse(url).netloc for url in seeds}\n",
        "            for host in unique_hosts:\n",
        "                site_url = f\"https://{host}\"\n",
        "                await self.fetcher.check_robots(site_url)\n",
        "\n",
        "            self.running = True  # Attiva il flag di esecuzione\n",
        "            spiders = [asyncio.create_task(self.spider()) for _ in range(self.num_spiders)]\n",
        "\n",
        "            await self.frontier.join()  # Aspetta che la coda sia vuota\n",
        "            self.running = False        # Ferma gli spider\n",
        "\n",
        "            # Attendi che gli spider escano dal loop e terminino\n",
        "            await asyncio.gather(*spiders, return_exceptions=True)\n",
        "\n",
        "            logger.info(\"Crawling finished.\")\n",
        "            print(\"Crawling finished.\")\n",
        "            logger.info(f\"Total seen URLs: {len(self.seen)}\")\n",
        "            print(f\"Total seen URLs: {len(self.seen)}\")\n",
        "            logger.info(f\"Total successfully visited URLs: {len(self.visited)}\")\n",
        "            print(f\"Total successfully visited URLs: {len(self.visited)}\")\n",
        "\n",
        "async def main():\n",
        "    seeds = [\n",
        "        \"https://www.bbc.com/\",\n",
        "\n",
        "    ]\n",
        "    scheduler = Scheduler(max_concurrency=3, num_spiders=2)\n",
        "    await scheduler.run(seeds=seeds)"
      ],
      "metadata": {
        "id": "qnNG6uzpAP_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await main()"
      ],
      "metadata": {
        "id": "ySzLdiG_5ic2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test 2\n",
        "ottengo tutti gli url trovati con lo scarping + parsing di una pagina"
      ],
      "metadata": {
        "id": "xeXWpKRkLvGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "class CustomScheduler(Scheduler):\n",
        "    def __init__(self, max_concurrency, num_spiders):\n",
        "        super().__init__(max_concurrency, num_spiders)\n",
        "        self.extracted_urls = set()\n",
        "        self.page_contents = {}  # {url: html}\n",
        "\n",
        "    async def process_response(self, url, response):\n",
        "        if not response:\n",
        "            return\n",
        "\n",
        "        content, final_url, status = response\n",
        "        await self.storage.save_page(final_url, content)\n",
        "        self.visited.add(final_url)\n",
        "\n",
        "\n",
        "        self.page_contents[final_url] = content\n",
        "\n",
        "        links = self.parser.extract_links(content)\n",
        "        for link in links:\n",
        "            self.extracted_urls.add(link)\n",
        "            await self.add_url(link)\n",
        "\n",
        "\n",
        "\n",
        "class Storage:\n",
        "    async def save_page(self, url, content):\n",
        "        logger.info(f\"Saving page: {url} (content length {len(content)})\")\n",
        "\n",
        "async def main():\n",
        "    # Setup\n",
        "    seed_urls = [\"https://www.bbc.com/\",]  # Puoi cambiare\n",
        "    parser = Parser()\n",
        "    scheduler = CustomScheduler(max_concurrency=2, num_spiders=1)\n",
        "\n",
        "    scheduler.parser = parser\n",
        "    scheduler.fetcher = Fetcher(None)  # usa la tua fetcher senza sessione aiohttp qui\n",
        "    scheduler.storage = Storage()\n",
        "\n",
        "    # Avvia lo scraping\n",
        "    await scheduler.seed_urls(seed_urls)\n",
        "    await scheduler.run(seed_urls)\n",
        "\n",
        "    # â Stampa i risultati\n",
        "    print(\"\\nð URL trovati:\")\n",
        "    for url in sorted(scheduler.extracted_urls):\n",
        "        print(url)\n",
        "\n",
        "    # â Analizza una delle pagine trovate\n",
        "    if scheduler.page_contents:\n",
        "        sample_url, html = next(iter(scheduler.page_contents.items()))\n",
        "        parsed_text = parser.parse_page_tags_all(html)\n",
        "\n",
        "        print(f\"\\nð§  Contenuto analizzato da: {sample_url}\\n\")\n",
        "        for line in parsed_text[:10]:  # stampa solo i primi 10 pezzi\n",
        "            print(\"-\", line)\n",
        "\n",
        "    else:\n",
        "        print(\"Nessuna pagina disponibile per l'analisi.\")\n"
      ],
      "metadata": {
        "id": "ubQKF2W5Lyja"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await main()"
      ],
      "metadata": {
        "id": "Mh3BuA4sOEkz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}