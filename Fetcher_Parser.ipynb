{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7JUf8ZqEAeOG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Osservazioni codice#"
      ],
      "metadata": {
        "id": "RaGoZfCzKuOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **check_time**: devo usare anche il domain del sito per forza?\n",
        "*   **fetch**: devo usare un if se è la prima volta che visito l'url cosi richiamo il robots.txt? quando devo controllare la sitema etc.?\n"
      ],
      "metadata": {
        "id": "BjpFU79iKz4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Librerie#"
      ],
      "metadata": {
        "id": "EFS9PhzpB5wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#url di prova\n",
        "# ha sitemap-index ma non priority etc.\n",
        "start_url_uno = \"https://www.dragopublisher.com/it/\"\n",
        "# non ha sitemap\n",
        "start_url_due = \"https://www.wildraccoon.it/shop/\"\n",
        "# sito giocattolo ha tutto\n",
        "start_url = \"https://nonciclopedia.org/wiki/Wikipedia\""
      ],
      "metadata": {
        "id": "wsDOXSrnc3Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import urllib.robotparser #per gestire il file robot.txt\n",
        "import xml.etree.ElementTree as ET  #per gestire i file xml\n",
        "import requests #per http\n",
        "import time #per gestire il tempo\n",
        "from bs4 import BeautifulSoup #per gestire il parsing del html (si può usare anche per xml)\n",
        "import rfc3986 # per la normalizzazione degli urls\n"
      ],
      "metadata": {
        "id": "X-hlOT7K3Wzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fetcher#\n",
        "gestione richieste e download pagine web"
      ],
      "metadata": {
        "id": "o64w-kJVAOHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chech_roobots(url,useragent=\"*\"):\n",
        "  '''\n",
        "  input: useragent (default: *, generico), url sito da analizzare\n",
        "  output: check,path_disallow,crawl_delay,request_rate,sitemap\n",
        "\n",
        "  controlla il file robot.txt associato all'url\n",
        "  '''\n",
        "\n",
        "  rfl = urllib.robotparser.RobotFileParser()\n",
        "  # aggiungo all'url selezionato l'estensione corretta per leggere il file robots.txt\n",
        "  new_url = url + \"/robots.txt\"\n",
        "\n",
        "  rfl.set_url(new_url)\n",
        "  #legge il file robots.txt e controlla se lo useragent puo effettuare il fetch dei dati\n",
        "  rfl.read()\n",
        "  check = rfl.can_fetch(useragent,new_url)\n",
        "  if check:\n",
        "    print(\"{} il sito puo effettuare il fetch dei dati\".format(check))\n",
        "  else:\n",
        "    print(\"il sito non puo effettuare il fetch dei dati\")\n",
        "\n",
        "  # salva le path non accessibili (Disallow:...)\n",
        "  path_disallow = rfl.parse(\"Disallow\")\n",
        "  print(\"path non accessibili: {}\".format(path_disallow))\n",
        "\n",
        "  # salva il craw_delay: intervallo temporale tra richieste dello stesso useragent\n",
        "  crawl_delay = rfl.crawl_delay(useragent)\n",
        "  print(\"crawl delay: {}\".format(crawl_delay))\n",
        "\n",
        "  # salva il request_rate: numero di richieste ogni tot secondi (tuple (r,s))\n",
        "  request_rate = request_rate(useragent)\n",
        "  print(\"request rate: {}\".format(request_rate))\n",
        "\n",
        "  # salva la sitemap\n",
        "  sitemap = rfl.site_maps()\n",
        "  print(\"sitemap: {}\".format(sitemap))\n",
        "\n",
        "\n",
        "\n",
        "  return check,path_disallow,crawl_delay,request_rate,sitemap"
      ],
      "metadata": {
        "id": "IAoElvcYAQsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UserAgentPolicy\n",
        "  '''\n",
        "    classe per identificare gli useragent e salvare di volta in volta le informazioni necessarie per gestire le richieste\n",
        "  '''\n",
        "  def __init__(self, name, base_url, path_dissalow, crawl_delay, request_rate, n_request,last_access, header):\n",
        "        self.base_url = base_url\n",
        "        self.name = name\n",
        "        self.path_dissalow = path_dissalow\n",
        "        self.crawl_delay = crawl_delay\n",
        "        self.request_rate = request_rate\n",
        "        self.n_request = n_request\n",
        "        self.last_access = last_access\n",
        "        self.header = header"
      ],
      "metadata": {
        "id": "JS4QwNGoXol-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Useragents di prova#\n",
        "useragent_dict = {\"Googlebot\": UserAgentPolicy(\"Googlebot\",None, None, None, None, None, None,'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'),\n",
        "                  \"Bingbot\": UserAgentPolicy(\"Bingbot\",None, None, None, None, None, None,'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)'),\n",
        "                  \"Slurp\": UserAgentPolicy(\"Slurp\",None, None, None, None, None, None, 'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)'),\n",
        "                  \"DuckDuckbot\": UserAgentPolicy(\"DuckDuckbot\",None, None, None, None, None, None,'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)' ),\n",
        "                  \"Yandex\": UserAgentPolicy(\"Yandex\",None, None, None, None, None, None,'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)' ),\n",
        "                  \"*\": UserAgentPolicy(\"*\",None, None, None, None, None, None,'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)' )\n",
        "                  }\n",
        "default_agent = useragent_dict[\"*\"]\n",
        "\n",
        "# per fare test e simulare il comportamento di più useragent (non so se metterli nelle classi)\n",
        "useragent_dict_header = {'Googlebot':'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',\n",
        "                    'Bingbot':'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)',\n",
        "                    'Slurp':'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)',\n",
        "                    'DuckDuckbot':'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)',\n",
        "                    'Yandex':'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)',\n",
        "                    '*': 'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)'} #---> non so se devo mettere solo '*' anche su questo header (generato con ChatGPT)"
      ],
      "metadata": {
        "id": "2NPNZPZCMBDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_time(useragent=default_agent):\n",
        "  '''\n",
        "    controlla che lo useragent rispetti il tempo di craw-delay e il request_rate\n",
        "  '''\n",
        "\n",
        "  # Se entrambi definiti, scegli il valore più restrittivo (conversione in ns)\n",
        "  if useragent.request_rate and useragent.crawl_delay:\n",
        "     request_delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]\n",
        "     crawl_delay = useragent.crawl_delay * 1e9\n",
        "     delay = max(request_delay, crawl_delay)\n",
        "\n",
        "    elif useragent.crawl_delay:\n",
        "        delay = useragent.crawl_delay * 1e9\n",
        "\n",
        "    elif useragent.request_rate:\n",
        "        delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]\n",
        "\n",
        "    now = time.monotonic_ns()\n",
        "    wait = max(0, (useragent.last_access + delay) - now)\n",
        "\n",
        "    if wait > 0:\n",
        "        time.sleep(wait / 1e9)  # converto da ns a s limiti sleep\n",
        "\n",
        "    # aggiorno a prescindere dal succeso della connesione il last_access così da rispettare di sicuro la policy\n",
        "    useragent.last_access = time.monotonic_ns()\n",
        ""
      ],
      "metadata": {
        "id": "bY-25bn6Fkxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch(url,useragent=default_agent):\n",
        "  '''\n",
        "    contatta la pagina e gestisce le richieste da parte dello useragent\n",
        "\n",
        "  '''\n",
        "\n",
        "  # controllo che lo user non esegua troppe richieste allo stesso DNS\n",
        "  check_time(useragent)\n",
        "\n",
        "  # costuisco l'header specifico per lo useragent richiesto(usato per test, non semrpe necessario)\n",
        "  headers = {\n",
        "        'User-Agent': useragent.header\n",
        "    }\n",
        "  # contatto il sito\n",
        "  try:\n",
        "     response = requests.get(url, headers=headers, timeout=10)\n",
        "     if response.status_code == 200:\n",
        "       print(\"Pagina recuperata con successo:{}\",.format(url))\n",
        "       html = response.text\n",
        "       return html\n",
        "     else:\n",
        "      print(\"Errore nella ricezione del sito:{}\".format(url))\n",
        "      return None\n",
        "\n",
        "  # per gestire tutti gli altri errori\n",
        "  except requests.RequestException as e:\n",
        "        print(\"Eccezione durante la richiesta:\", e)\n",
        "        return None"
      ],
      "metadata": {
        "id": "JxhLLKQIC9Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parser#\n",
        "analisi contenuto della pagina, estrazione testo e link e normalizzazione link trovati"
      ],
      "metadata": {
        "id": "Sv_y5bWRARWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_url(url):\n",
        "'''\n",
        "   Normalizza l'URL secondo lo standard RFC3986 e restituisce una stringa\n",
        "'''\n",
        "\n",
        "    try:\n",
        "        uri = rfc3986.uri_reference(url).normalize()\n",
        "        return uri.unsplit()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Errore nella normalizzazione URL: {url} – {e}\")\n",
        "        return url"
      ],
      "metadata": {
        "id": "9xyD_K9RT7AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_sitemap(sitemap_list):\n",
        "    '''\n",
        "    Prende in input una lista di sitemap URLs e restituisce un DataFrame\n",
        "    con: URL, priorità e frequenza di aggiornamento.\n",
        "    '''\n",
        "    sitemap_all = pd.DataFrame(columns=['url','priority','update'])\n",
        "\n",
        "    # Standard XML Namespace ufficiale del protocollo Sitemap, serve per identificare il vocabolario e la struttura usata nel file xml\n",
        "    namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "\n",
        "    def parse_single_sitemap(sitemap_url):\n",
        "\n",
        "        response = requests.get(sitemap_url)\n",
        "        if response.status_code != 200:\n",
        "           print(\"Errore nel recupero:\", sitemap_url)\n",
        "           content_type = response.headers.get(\"Content-Type\", \"\")\n",
        "           return content_type\n",
        "\n",
        "    # Sitemap XML\n",
        "    if \"xml\" in content_type:\n",
        "\n",
        "        # Controllo che la sitemap risponda (da aggiungere la possibilità di decompressione se necessaria)\n",
        "        root = ET.fromstring(response.content)\n",
        "\n",
        "        # Se è una sitemap index (contiene altre sitemap)\n",
        "        if root.tag.endswith('index'):\n",
        "            sitemap_urls = [loc.text for loc in root.findall('ns:sitemap/ns:loc', namespace)]\n",
        "            all_entries = []\n",
        "            for sub_url in sitemap_urls:\n",
        "                all_entries.extend(parse_single_sitemap(sub_url))\n",
        "            return all_entries\n",
        "\n",
        "        # Se è una sitemap normale (contiene solo URL)\n",
        "        elif root.tag.endswith('urlset'):\n",
        "            entries = []\n",
        "            for url in root.findall('ns:url', namespace):\n",
        "                loc = url.find('ns:loc', namespace)\n",
        "                priority = url.find('ns:priority', namespace)\n",
        "                changefreq = url.find('ns:changefreq', namespace)\n",
        "\n",
        "                entries.append({\n",
        "                    'url': loc.text if loc is not None else '',\n",
        "                    'priority': priority.text if priority is not None else None,\n",
        "                    'update': changefreq.text if changefreq is not None else None\n",
        "                })\n",
        "            return entries\n",
        "\n",
        "    # Sitemap HTML\n",
        "    elif \"html\" in content_type:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        links = soup.find_all('a', href=True)\n",
        "        entries = []\n",
        "        for a in links:\n",
        "            url = normalize_url(a['href'])\n",
        "            entries.append({'url': url, 'priority': None, 'update': None})\n",
        "        return entries\n",
        "\n",
        "    else:\n",
        "        print(f\"Formato non riconosciuto per {sitemap_url}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "    # Analizza ogni sitemap passata\n",
        "    all_data = []\n",
        "\n",
        "    if sitemap_list is None:\n",
        "        print(\"Nessuna sitemap presente\")\n",
        "        return sitemap_all\n",
        "\n",
        "    for sitemap_url in sitemap_list:\n",
        "        if sitemap_url is None:\n",
        "            print(\"Nessuna sitemap presente\")\n",
        "            continue\n",
        "        all_data.extend(parse_single_sitemap(sitemap_url))\n",
        "\n",
        "    # Normalizzo gli url\n",
        "    for entry in all_data:\n",
        "        entry['url'] = normalize_url(entry['url'])\n",
        "\n",
        "    # Crea DataFrame finale\n",
        "    sitemap_all = pd.DataFrame(all_data)\n",
        "\n",
        "    # Elimino i duplicati\n",
        "    sitemap_all = sitemap_all.drop_duplicates()\n",
        "\n",
        "    return sitemap_all\n"
      ],
      "metadata": {
        "id": "dpUG-96lv6A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from xml.etree import ElementTree as ET\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def parse_sitemap(sitemap_list):\n",
        "    '''\n",
        "    Prende in input una lista di sitemap URLs e restituisce un DataFrame\n",
        "    con: URL, priorità e frequenza di aggiornamento.\n",
        "    '''\n",
        "    namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "    all_entries = []\n",
        "\n",
        "    if not sitemap_list:\n",
        "        print(\"Nessuna sitemap fornita.\")\n",
        "        return pd.DataFrame(columns=['url', 'priority', 'update'])\n",
        "\n",
        "    def parse_single_sitemap(sitemap_url):\n",
        "        try:\n",
        "            response = requests.get(sitemap_url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Errore durante il recupero di {sitemap_url}: {e}\")\n",
        "            return []\n",
        "\n",
        "        content_type = response.headers.get(\"Content-Type\", \"\").lower()\n",
        "\n",
        "        # Sitemap XML\n",
        "        if \"xml\" in content_type:\n",
        "            try:\n",
        "                root = ET.fromstring(response.content)\n",
        "            except ET.ParseError as e:\n",
        "                print(f\"Errore nel parsing XML: {e}\")\n",
        "                return []\n",
        "\n",
        "            # Sitemap index\n",
        "            if root.tag.endswith('index'):\n",
        "                sitemap_urls = [loc.text for loc in root.findall('ns:sitemap/ns:loc', namespace)]\n",
        "                entries = []\n",
        "                for sub_url in sitemap_urls:\n",
        "                    entries.extend(parse_single_sitemap(sub_url))\n",
        "                return entries\n",
        "\n",
        "            # Sitemap semplice\n",
        "            elif root.tag.endswith('urlset'):\n",
        "                return [{\n",
        "                    'url': url.findtext('ns:loc', default='', namespaces=namespace),\n",
        "                    'priority': url.findtext('ns:priority', default=None, namespaces=namespace),\n",
        "                    'update': url.findtext('ns:changefreq', default=None, namespaces=namespace)\n",
        "                } for url in root.findall('ns:url', namespace)]\n",
        "\n",
        "        # Sitemap HTML\n",
        "        elif \"html\" in content_type:\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "            return [{\n",
        "                'url': normalize_url(a['href']),\n",
        "                'priority': None,\n",
        "                'update': None\n",
        "            } for a in soup.find_all('a', href=True)]\n",
        "\n",
        "        else:\n",
        "            print(f\"Formato non riconosciuto per {sitemap_url} ({content_type})\")\n",
        "            return []\n",
        "\n",
        "    # Estrai tutte le voci da ogni sitemap\n",
        "    for sitemap_url in sitemap_list:\n",
        "        if sitemap_url:\n",
        "            entries = parse_single_sitemap(sitemap_url)\n",
        "            for entry in entries:\n",
        "                entry['url'] = normalize_url(entry['url'])  # Normalizzazione\n",
        "            all_entries.extend(entries)\n",
        "\n",
        "    df = pd.DataFrame(all_entries, columns=['url', 'priority', 'update']).drop_duplicates()\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "-b4iywtuKnz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_page_url(html,sitemaps_urls,useragent=defaullt_agent):\n",
        "  '''\n",
        "    dato una pagina html di input cerco gli urls:\n",
        "    - se sono in una directory privata li scarto\n",
        "    - se gia presenti nella sitemap, sono già inidicizzate e non salvo i link (duplicati)\n",
        "    - se non presenti salvo gli url permessi dal file robots.txt\n",
        "  '''\n",
        "\n",
        "  #parsing pagina html\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "  urls = []\n",
        "  # Elementi con attributo href\n",
        "  for tag in soup.find_all(href=True):\n",
        "      urls.append(tag['href'])\n",
        "\n",
        "  # Elementi con attributo src\n",
        "  for tag in soup.find_all(src=True):\n",
        "      urls.append(tag['src'])\n",
        "\n",
        "  # controlla se i nuovi url sono veramente accessibili per il web scraping (no in disallow)\n",
        "  if useragent.path_disallow != None:\n",
        "    for url in urls:\n",
        "      for path in useragent.path_disallow:\n",
        "        if path in url:\n",
        "         urls.remove(url)\n",
        "\n",
        "  # normalizzo gli urls\n",
        "    urls = [normalize_url(url) for url in urls]\n",
        "\n",
        "  # elimino i duplicati\n",
        "    urls = list(set(urls))\n",
        "\n",
        "\n",
        "  #controlla se ci sono duplicati nella sitemap se c'è\n",
        "  if sitemaps_urls != None:\n",
        "    def_urls = sitemaps_urls\n",
        "    for url in urls:\n",
        "      if url not in def_urls:\n",
        "         def_urls.append(url)\n",
        "  else:\n",
        "    def_urls = urls\n",
        "\n",
        "  return def_urls\n",
        "\n"
      ],
      "metadata": {
        "id": "NbmdtzHa4GKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_page_tags(html,tag_type, attrs={}):\n",
        " '''\n",
        "    accetta in input una pagina html, il tipo di taga da selezioanre ed eventuali attributi se necessari\n",
        " '''\n",
        "    # scelgo il parser\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    tags = soup.find_all(tag_type, attrs=attrs)\n",
        "\n",
        "    return [str(tag) for tag in tags]"
      ],
      "metadata": {
        "id": "rJ6l9K4-UHYT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}