{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7JUf8ZqEAeOG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Osservazioni codice#"
      ],
      "metadata": {
        "id": "RaGoZfCzKuOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **check_time**: devo usare anche il domain del sito per forza?\n",
        "*   **fetch**: devo usare un if se è la prima volta che visito l'url cosi richiamo il robots.txt? quando devo controllare la sitema etc.?\n",
        "*  **classe** crea una classe useragent così è più facile gestire tutte le funzioni!!!"
      ],
      "metadata": {
        "id": "BjpFU79iKz4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Librerie#"
      ],
      "metadata": {
        "id": "EFS9PhzpB5wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#url di prova\n",
        "# ha sitemap-index ma non priority etc.\n",
        "start_url_uno = \"https://www.dragopublisher.com/it/\"\n",
        "# non ha sitemap\n",
        "start_url_due = \"https://www.wildraccoon.it/shop/\"\n",
        "# sito giocattolo ha tutto\n",
        "start_url = \"https://nonciclopedia.org/wiki/Wikipedia\""
      ],
      "metadata": {
        "id": "wsDOXSrnc3Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re #per gestire le regex --- non credo servirà\n",
        "import pandas as pd\n",
        "import urllib.robotparser #per gestire il file robot.txt\n",
        "import xml.etree.ElementTree as ET  #per gestire i file xml\n",
        "import requests #per http\n",
        "import time #per gestire il tempo\n",
        "from bs4 import BeautifulSoup #per gestire il parsing del html (si può usare anche per xml)"
      ],
      "metadata": {
        "id": "X-hlOT7K3Wzn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chech_roobots(url,useragent=\"*\"):\n",
        "  '''\n",
        "  input: useragent (default: *, generico), url sito da analizzare\n",
        "  output: check,path_disallow,crawl_delay,request_rate,sitemap\n",
        "\n",
        "  controlla il file robot.txt associato all'url\n",
        "  '''\n",
        "\n",
        "  rfl = urllib.robotparser.RobotFileParser()\n",
        "  # aggiungo all'url selezionato l'estensione corretta per leggere il file robots.txt\n",
        "  new_url = url + \"/robots.txt\"\n",
        "\n",
        "  rfl.set_url(new_url)\n",
        "  #legge il file robots.txt e controlla se lo useragent puo effettuare il fetch dei dati\n",
        "  rfl.read()\n",
        "  check = rfl.can_fetch(useragent,new_url)\n",
        "  if check:\n",
        "    print(\"{} il sito puo effettuare il fetch dei dati\".format(check))\n",
        "  else:\n",
        "    print(\"il sito non puo effettuare il fetch dei dati\")\n",
        "\n",
        "  # salva le path non accessibili (Disallow:...)\n",
        "  path_disallow = rfl.parse(\"Disallow\")\n",
        "  print(\"path non accessibili: {}\".format(path_disallow))\n",
        "\n",
        "  # salva il craw_delay: intervallo temporale tra richieste dello stesso useragent\n",
        "  crawl_delay = rfl.crawl_delay(useragent)\n",
        "  print(\"crawl delay: {}\".format(crawl_delay))\n",
        "\n",
        "  # salva il request_rate: numero di richieste ogni tot secondi (tuple (r,s))\n",
        "  request_rate = request_rate(useragent)\n",
        "  print(\"request rate: {}\".format(request_rate))\n",
        "\n",
        "  # salva la sitemap\n",
        "  sitemap = rfl.site_maps()\n",
        "  print(\"sitemap: {}\".format(sitemap))\n",
        "\n",
        "\n",
        "\n",
        "  return check,path_disallow,crawl_delay,request_rate,sitemap"
      ],
      "metadata": {
        "id": "IAoElvcYAQsv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fetcher#\n",
        "gestione richieste e download pagine web"
      ],
      "metadata": {
        "id": "o64w-kJVAOHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UserAgentPolicy\n",
        "  '''\n",
        "    classe per identificare gli useragent e salvare di volta in volta le informazioni necessarie per gestire le richieste\n",
        "  '''\n",
        "  def __init__(self, name, base_url, path_dissalow, crawl_delay, request_rate, n_request,last_access, header):\n",
        "        self.base_url = base_url\n",
        "        self.name = name\n",
        "        self.path_dissalow = path_dissalow\n",
        "        self.crawl_delay = crawl_delay\n",
        "        self.request_rate = request_rate\n",
        "        self.n_request = n_request\n",
        "        self.last_access = last_access\n",
        "        self.header = header"
      ],
      "metadata": {
        "id": "JS4QwNGoXol-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Useragents di prova#"
      ],
      "metadata": {
        "id": "jfjuYaKlicjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "devo sistemare il codice pensando a come usare la classe user agent ovunque... spreco di memoria? ---> check necessario"
      ],
      "metadata": {
        "id": "_JK_N8JfmD2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_time(crawl_delay=None,last_access=0,request_rate=None, n_requests= 0):\n",
        "  '''\n",
        "    controlla che lo useragent rispetti il tempo di craw-delay e il request_rate\n",
        "  '''\n",
        "\n",
        "  # check che il bot on faccia più di n richieste in m secondi (= rispetti il request rate)\n",
        "  # check che il bot aspetti sempre il tempo di crawl delay tra le richieste\n",
        "  # orologio assoluto non sensibile alle variazioni del system clock, precisione ns\n",
        "\n",
        "  if request_rate == None && crawl_delay == None:\n",
        "     delay = 1.5  #valore a caso\n",
        "\n",
        "  # il crawl delay può essere inferiore all'intervallo id tempo tra le richieste del request_delay  e questo fa si che la frequenza di richieste nonvenga rispettata\n",
        "  if request_rate[0]/request_rate[1] > crawl_delay:\n",
        "    delay = request_rate[0]/request_rate[1]\n",
        "  else:\n",
        "    delay = crawl_delay\n",
        "\n",
        "  now = time.monotonic_ns()\n",
        "  wait = max(0, (last_access + delay) - now)\n",
        "  if wait > 0:\n",
        "      time.sleep(wait)\n",
        "  last_access = time.monotonic_ns()\n",
        "  return"
      ],
      "metadata": {
        "id": "bY-25bn6Fkxx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch(url,UserAgent='*',craw_delay=5, last_access=0, request_rate=(3,10)): #hp buttato valori di default a caso per ora\n",
        "  '''\n",
        "  '''\n",
        "  # per fare test e simulare il comportamento di più useragent (non so se metterli nelle classi)\n",
        "  useragent_dict = {'Googlebot':'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',\n",
        "                    'Bingbot':'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)',\n",
        "                    'Slurp':'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)',\n",
        "                    'DuckDuckbot':'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)',\n",
        "                    'Yandex':'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)',\n",
        "                    '*': 'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)'} #---> non so se devo mettere solo '*' anche su questo header (generato con ChatGPT)\n",
        "\n",
        "\n",
        "  # controllo che lo user non esegua troppe richieste allo stesso DNS\n",
        "  check_time(crawl_delay=craw_delay,last_access=last_access)\n",
        "\n",
        "  # costuisco l'header specifico per lo useragent richiesto(usato per test, non semrpe necessario)\n",
        "  headers = {\n",
        "        'User-Agent': useragent_dict[useragent]\n",
        "    }\n",
        "  # contatto il sito\n",
        "  response = requests.get(url, headers=headers, timeout=10)\n",
        "  if response.status_code != 200:\n",
        "     print(\"Errore nel recupero:\", url)\n",
        "  return"
      ],
      "metadata": {
        "id": "JxhLLKQIC9Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_allowed(url,path_disallow,useragent='*'):\n"
      ],
      "metadata": {
        "id": "lH4ereCLkv8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main fetcher esempio#"
      ],
      "metadata": {
        "id": "hX9DsWjTcSl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parser#\n",
        "analisi contenuto della pagina, estrazione testo e link e normalizzazione link trovati"
      ],
      "metadata": {
        "id": "Sv_y5bWRARWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_sitemap(sitemap_list):\n",
        "    '''\n",
        "    Prende in input una lista di sitemap URLs e restituisce un DataFrame\n",
        "    con: URL, priorità e frequenza di aggiornamento.\n",
        "    '''\n",
        "    sitemap_all = pd.DataFrame(columns=['url','priority','update'])\n",
        "\n",
        "    # Standard XML Namespace ufficiale del protocollo Sitemap, serve per identificare il vocabolario e la struttura usata nel file xml\n",
        "    namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "\n",
        "    def parse_single_sitemap(sitemap_url):\n",
        "\n",
        "        response = requests.get(sitemap_url)\n",
        "        if response.status_code != 200:\n",
        "           print(\"Errore nel recupero:\", sitemap_url)\n",
        "           return []\n",
        "\n",
        "        # Controllo che la sitemap risponda (da aggiungere la possibilità di decompressione se necessaria)\n",
        "        root = ET.fromstring(response.content)\n",
        "\n",
        "        # Se è una sitemap index (contiene altre sitemap)\n",
        "        if root.tag.endswith('index'):\n",
        "            sitemap_urls = [loc.text for loc in root.findall('ns:sitemap/ns:loc', namespace)]\n",
        "            all_entries = []\n",
        "            for sub_url in sitemap_urls:\n",
        "                all_entries.extend(parse_single_sitemap(sub_url))\n",
        "            return all_entries\n",
        "\n",
        "        # Se è una sitemap normale (contiene solo URL)\n",
        "        elif root.tag.endswith('urlset'):\n",
        "            entries = []\n",
        "            for url in root.findall('ns:url', namespace):\n",
        "                loc = url.find('ns:loc', namespace)\n",
        "                priority = url.find('ns:priority', namespace)\n",
        "                changefreq = url.find('ns:changefreq', namespace)\n",
        "\n",
        "                entries.append({\n",
        "                    'url': loc.text if loc is not None else '',\n",
        "                    'priority': priority.text if priority is not None else None,\n",
        "                    'update': changefreq.text if changefreq is not None else None\n",
        "                })\n",
        "            return entries\n",
        "        else:\n",
        "            print(f\"Tipo di sitemap non riconosciuto: {root.tag}\")\n",
        "            return []\n",
        "\n",
        "    # Analizza ogni sitemap passata\n",
        "    all_data = []\n",
        "\n",
        "    if sitemap_list is None:\n",
        "        print(\"Nessuna sitemap presente\")\n",
        "        return sitemap_all\n",
        "\n",
        "    for sitemap_url in sitemap_list:\n",
        "        if sitemap_url is None:\n",
        "            print(\"Nessuna sitemap presente\")\n",
        "            continue\n",
        "        all_data.extend(parse_single_sitemap(sitemap_url))\n",
        "\n",
        "    # Crea DataFrame finale\n",
        "    sitemap_all = pd.DataFrame(all_data)\n",
        "    return sitemap_all\n"
      ],
      "metadata": {
        "id": "dpUG-96lv6A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_page(url,sitemaps_urls,path_disallow):\n",
        "  '''\n",
        "    dato un url di input cerco le pagine non indicizzate accessibili:\n",
        "    - se sono in una directory privata li scarto\n",
        "    - se gia presenti nella sitemap, sono già inidicizzate e non salvo i link (duplicati)\n",
        "    - se non presenti salvo gli url permessi dal file robots.txt\n",
        "  '''\n",
        "  #simulo uno user-agent\n",
        "  headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0 Safari/537.36'\n",
        "    }\n",
        "\n",
        "  #contatto la pagina\n",
        "  response = requests.get(url, headers=headers, timeout=10)\n",
        "  if response.status_code != 200:\n",
        "     print(\"Errore nel recupero:\", url)\n",
        "     print(response)\n",
        "\n",
        "  #parsing pagina html\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "  urls = []\n",
        "  # Elementi con attributo href\n",
        "  for tag in soup.find_all(href=True):\n",
        "      urls.append(tag['href'])\n",
        "\n",
        "  # Elementi con attributo src\n",
        "  for tag in soup.find_all(src=True):\n",
        "      urls.append(tag['src'])\n",
        "\n",
        "  # controlla se i nuovi url sono veramente accessibili per il web scraping (no in disallow)\n",
        "  if path_disallow != None:\n",
        "    for url in urls:\n",
        "      for path in path_disallow:\n",
        "        if path in url:\n",
        "         urls.remove(url)\n",
        "\n",
        "  #controlla se ci sono duplicati nella sitemap se c'è\n",
        "  if sitemaps_urls != None:\n",
        "    def_urls = sitemaps_urls\n",
        "    for url in urls:\n",
        "      if url not in def_urls:\n",
        "         def_urls.append(url)\n",
        "  else:\n",
        "    def_urls = urls\n",
        "\n",
        "  return def_urls\n",
        "\n"
      ],
      "metadata": {
        "id": "NbmdtzHa4GKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_link():\n",
        "  return"
      ],
      "metadata": {
        "id": "IDWMtySjUEDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_text():\n",
        "  return"
      ],
      "metadata": {
        "id": "rJ6l9K4-UHYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main prove#"
      ],
      "metadata": {
        "id": "7JUf8ZqEAeOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BotPolicy:\n",
        "    def __init__(self, domain, user_agent):\n",
        "        self.domain = domain\n",
        "        self.user_agent = user_agent\n",
        "        self.parser = self._load_robots()\n",
        "        self.last_request_time = None\n",
        "        self.request_counter = 0\n",
        "        self.rate_window_start = time.monotonic()\n",
        "\n",
        "    def _load_robots(self):\n",
        "        parser = RobotFileParser()\n",
        "        parser.set_url(f\"{self.domain}/robots.txt\")\n",
        "        parser.read()\n",
        "        return parser\n",
        "\n",
        "    def can_fetch(self, path):\n",
        "        return self.parser.can_fetch(self.user_agent, self.domain + path)\n",
        "\n",
        "    def crawl_delay(self):\n",
        "        return self.parser.crawl_delay(self.user_agent)\n",
        "\n",
        "    def request_rate(self):\n",
        "        return self.parser.request_rate(self.user_agent)\n",
        "\n",
        "    def wait(self):\n",
        "        # stessa logica vista prima: crawl_delay e request_rate\n",
        "        pass\n",
        "\n",
        "class UserAgentManager:\n",
        "    def __init__(self):\n",
        "        self.policies = {}  # chiave: (domain, user_agent)\n",
        "\n",
        "    def get_policy(self, domain, user_agent):\n",
        "        key = (domain, user_agent)\n",
        "        if key not in self.policies:\n",
        "            self.policies[key] = BotPolicy(domain, user_agent)\n",
        "        return self.policies[key]\n"
      ],
      "metadata": {
        "id": "88PyfgdoZFJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "from urllib.robotparser import RobotFileParser\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "class RespectfulCrawler:\n",
        "    def __init__(self, base_url, user_agent=\"MyBot\"):\n",
        "        self.base_url = base_url\n",
        "        self.user_agent = user_agent\n",
        "        self.domain = urlparse(base_url).scheme + \"://\" + urlparse(base_url).netloc\n",
        "        self.parser = RobotFileParser()\n",
        "        self.parser.set_url(self.domain + \"/robots.txt\")\n",
        "        self.parser.read()\n",
        "        self.last_request_time = None\n",
        "        self.request_counter = 0\n",
        "        self.start_window = time.monotonic()\n",
        "\n",
        "    def is_allowed(self, path):\n",
        "        return self.parser.can_fetch(self.user_agent, self.domain + path)\n",
        "\n",
        "    def wait_respecting_robots(self):\n",
        "        # Handle crawl-delay\n",
        "        delay = self.parser.crawl_delay(self.user_agent)\n",
        "        if delay and self.last_request_time:\n",
        "            elapsed = time.monotonic() - self.last_request_time\n",
        "            if elapsed < delay:\n",
        "                time_to_wait = delay - elapsed\n",
        "                print(f\"[WAIT] Crawl-delay: sleeping for {time_to_wait:.2f} seconds\")\n",
        "                time.sleep(time_to_wait)\n",
        "\n",
        "        # Handle request-rate\n",
        "        rate = self.parser.request_rate(self.user_agent)\n",
        "        if rate:\n",
        "            now = time.monotonic()\n",
        "            if now - self.start_window > rate.seconds:\n",
        "                self.start_window = now\n",
        "                self.request_counter = 0\n",
        "            self.request_counter += 1\n",
        "            if self.request_counter > rate.requests:\n",
        "                reset_time = self.start_window + rate.seconds - now\n",
        "                print(f\"[WAIT] Request-rate: sleeping for {reset_time:.2f} seconds\")\n",
        "                time.sleep(reset_time)\n",
        "                self.start_window = time.monotonic()\n",
        "                self.request_counter = 1\n",
        "\n",
        "        self.last_request_time = time.monotonic()\n",
        "\n",
        "    def fetch(self, path):\n",
        "        if not self.is_allowed(path):\n",
        "            print(f\"[BLOCKED] Access to {path} is disallowed by robots.txt\")\n",
        "            return None\n",
        "\n",
        "        self.wait_respecting_robots()\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': self.user_agent\n",
        "        }\n",
        "\n",
        "        url = self.domain + path\n",
        "        print(f\"[FETCH] Getting {url}\")\n",
        "        response = requests.get(url, headers=headers)\n",
        "        return response\n",
        "\n",
        "# 🧪 ESEMPIO USO\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    crawler = RespectfulCrawler(\"https://example.com\", user_agent=\"MyBot\")\n",
        "\n",
        "    # Lista di pagine da visitare\n",
        "    paths = [\"/\", \"/about\", \"/contact\"]\n",
        "\n",
        "    for path in paths:\n",
        "        response = crawler.fetch(path)\n",
        "        if response and response.status_code == 200:\n",
        "            print(f\"[OK] {path} - {len(response.text)} bytes\")\n",
        "        elif response:\n",
        "            print(f\"[ERROR] {path} - Status: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "5-iXYOi9WqDT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}