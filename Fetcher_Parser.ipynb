{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7JUf8ZqEAeOG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Osservazioni codice#"
      ],
      "metadata": {
        "id": "RaGoZfCzKuOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **check_time**: devo usare anche il domain del sito per forza?\n",
        "*   **fetch**: devo usare un if se è la prima volta che visito l'url cosi richiamo il robots.txt? quando devo controllare la sitema etc.?\n"
      ],
      "metadata": {
        "id": "BjpFU79iKz4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Librerie#"
      ],
      "metadata": {
        "id": "EFS9PhzpB5wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#url di prova\n",
        "# ha sitemap-index ma non priority etc.\n",
        "start_url_uno = \"https://www.dragopublisher.com/it/\"\n",
        "# non ha sitemap\n",
        "start_url_due = \"https://www.wildraccoon.it/shop/\"\n",
        "# sito giocattolo ha tutto\n",
        "start_url = \"https://nonciclopedia.org/wiki/Wikipedia\""
      ],
      "metadata": {
        "id": "wsDOXSrnc3Ti"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rfc3986\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sRo7YHUxAKc",
        "outputId": "f5907815-3750-4237-f121-5aac8aaa6936"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rfc3986\n",
            "  Downloading rfc3986-2.0.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Downloading rfc3986-2.0.0-py2.py3-none-any.whl (31 kB)\n",
            "Installing collected packages: rfc3986\n",
            "Successfully installed rfc3986-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import urllib.robotparser #per gestire il file robot.txt\n",
        "import xml.etree.ElementTree as ET  #per gestire i file xml\n",
        "import requests #per http\n",
        "import time #per gestire il tempo\n",
        "from bs4 import BeautifulSoup #per gestire il parsing del html (si può usare anche per xml)\n",
        "import rfc3986 # per la normalizzazione degli urls\n"
      ],
      "metadata": {
        "id": "X-hlOT7K3Wzn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fetcher#\n",
        "gestione richieste e download pagine web"
      ],
      "metadata": {
        "id": "o64w-kJVAOHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chech_roobots(url,useragent=\"*\"):\n",
        "  '''\n",
        "  input: useragent (default: *, generico), url sito da analizzare\n",
        "  output: check,path_disallow,crawl_delay,request_rate,sitemap\n",
        "\n",
        "  controlla il file robot.txt associato all'url\n",
        "  '''\n",
        "\n",
        "  rfl = urllib.robotparser.RobotFileParser()\n",
        "  # aggiungo all'url selezionato l'estensione corretta per leggere il file robots.txt\n",
        "  new_url = url + \"/robots.txt\"\n",
        "\n",
        "  rfl.set_url(new_url)\n",
        "  #legge il file robots.txt e controlla se lo useragent puo effettuare il fetch dei dati\n",
        "  rfl.read()\n",
        "  check = rfl.can_fetch(useragent,new_url)\n",
        "  if check:\n",
        "    print(\"{} il sito puo effettuare il fetch dei dati\".format(check))\n",
        "  else:\n",
        "    print(\"il sito non puo effettuare il fetch dei dati\")\n",
        "\n",
        "  # salva le path non accessibili (Disallow:...)\n",
        "  path_disallow = rfl.parse(\"Disallow\")\n",
        "  print(\"path non accessibili: {}\".format(path_disallow))\n",
        "\n",
        "  # salva il craw_delay: intervallo temporale tra richieste dello stesso useragent\n",
        "  crawl_delay = rfl.crawl_delay(useragent)\n",
        "  print(\"crawl delay: {}\".format(crawl_delay))\n",
        "\n",
        "  # salva il request_rate: numero di richieste ogni tot secondi (tuple (r,s))\n",
        "  request_rate = request_rate(useragent)\n",
        "  print(\"request rate: {}\".format(request_rate))\n",
        "\n",
        "  # salva la sitemap\n",
        "  sitemap = rfl.site_maps()\n",
        "  print(\"sitemap: {}\".format(sitemap))\n",
        "\n",
        "\n",
        "\n",
        "  return check,path_disallow,crawl_delay,request_rate,sitemap"
      ],
      "metadata": {
        "id": "IAoElvcYAQsv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UserAgentPolicy:\n",
        "  '''\n",
        "    classe per identificare gli useragent e salvare di volta in volta le informazioni necessarie per gestire le richieste\n",
        "  '''\n",
        "  def __init__(self, name, base_url, path_dissalow, crawl_delay, request_rate, n_request,last_access, header):\n",
        "        self.base_url = base_url\n",
        "        self.name = name\n",
        "        self.path_dissalow = path_dissalow\n",
        "        self.crawl_delay = crawl_delay\n",
        "        self.request_rate = request_rate\n",
        "        self.n_request = n_request\n",
        "        self.last_access = last_access\n",
        "        self.header = header"
      ],
      "metadata": {
        "id": "JS4QwNGoXol-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Useragents di prova#\n",
        "useragent_dict = {\"Googlebot\": UserAgentPolicy(\"Googlebot\",None, None, None, None, None, 1e9,'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'),\n",
        "                  \"Bingbot\": UserAgentPolicy(\"Bingbot\",None, None, None, None, None, 1e9,'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)'),\n",
        "                  \"Slurp\": UserAgentPolicy(\"Slurp\",None, None, None, None, None, 1e9, 'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)'),\n",
        "                  \"DuckDuckbot\": UserAgentPolicy(\"DuckDuckbot\",None, None, None, None, None, 1e9,'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)' ),\n",
        "                  \"Yandex\": UserAgentPolicy(\"Yandex\",None, None, None, None, None, 1e9,'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)' ),\n",
        "                  \"*\": UserAgentPolicy(\"*\",None, None, None, None, None, 1e9,'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)' )\n",
        "                  }\n",
        "default_agent = useragent_dict[\"Googlebot\"]\n",
        "\n",
        "# per fare test e simulare il comportamento di più useragent (non so se metterli nelle classi)\n",
        "useragent_dict_header = {'Googlebot':'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',\n",
        "                    'Bingbot':'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)',\n",
        "                    'Slurp':'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)',\n",
        "                    'DuckDuckbot':'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)',\n",
        "                    'Yandex':'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)',\n",
        "                    '*': 'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)'} #---> non so se devo mettere solo '*' anche su questo header (generato con ChatGPT)"
      ],
      "metadata": {
        "id": "2NPNZPZCMBDm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_time(useragent=default_agent):\n",
        "  '''\n",
        "    controlla che lo useragent rispetti il tempo di craw-delay e il request_rate\n",
        "  '''\n",
        "\n",
        "  # Se entrambi definiti, scegli il valore più restrittivo (conversione in ns)\n",
        "  if useragent.request_rate and useragent.crawl_delay:\n",
        "     request_delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]\n",
        "     crawl_delay = useragent.crawl_delay * 1e9\n",
        "     delay = max(request_delay, crawl_delay)\n",
        "\n",
        "  elif useragent.crawl_delay:\n",
        "     delay = useragent.crawl_delay * 1e9\n",
        "\n",
        "  elif useragent.request_rate:\n",
        "     delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]\n",
        "\n",
        "  else:\n",
        "    delay = 1.5 * 1e9  # 1.5 secondi in nanosecondi\n",
        "\n",
        "  now = time.monotonic_ns()\n",
        "  wait = max(0, (useragent.last_access + delay) - now)\n",
        "\n",
        "  if wait > 0:\n",
        "     time.sleep(wait / 1e9)  # converto da ns a s limiti sleep\n",
        "\n",
        "  # aggiorno a prescindere dal successo della connesione il last_access così da rispettare di sicuro la policy\n",
        "  useragent.last_access = time.monotonic_ns()\n"
      ],
      "metadata": {
        "id": "bY-25bn6Fkxx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch(url,useragent=default_agent):\n",
        "  '''\n",
        "    contatta la pagina e gestisce le richieste da parte dello useragent\n",
        "\n",
        "  '''\n",
        "\n",
        "  # controllo che lo user non esegua troppe richieste allo stesso DNS\n",
        "  check_time(useragent)\n",
        "\n",
        "  # costuisco l'header specifico per lo useragent richiesto(usato per test, non semrpe necessario)\n",
        "  headers = {\n",
        "        'User-Agent': useragent.header\n",
        "    }\n",
        "  # contatto il sito\n",
        "  try:\n",
        "     response = requests.get(url, headers=headers, timeout=10)\n",
        "     if response.status_code == 200:\n",
        "       print(\"Pagina recuperata con successo:{}\".format(url))\n",
        "       html = response.text\n",
        "       return html\n",
        "     else:\n",
        "      print(\"Errore nella ricezione del sito:{}\".format(url))\n",
        "      return None\n",
        "\n",
        "  # per gestire tutti gli altri errori\n",
        "  except requests.RequestException as e:\n",
        "        print(\"Eccezione durante la richiesta:\", e)\n",
        "        return None"
      ],
      "metadata": {
        "id": "JxhLLKQIC9Nd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parser#\n",
        "analisi contenuto della pagina, estrazione testo e link e normalizzazione link trovati"
      ],
      "metadata": {
        "id": "Sv_y5bWRARWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_url(url):\n",
        "  '''\n",
        "     Normalizza l'URL secondo lo standard RFC3986 e restituisce una stringa\n",
        "  '''\n",
        "  try:\n",
        "      uri = rfc3986.uri_reference(url).normalize()\n",
        "      return uri.unsplit()\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"Errore nella normalizzazione URL: {url} – {e}\")\n",
        "      return url"
      ],
      "metadata": {
        "id": "9xyD_K9RT7AI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_sitemap(sitemap_list):\n",
        "    '''\n",
        "    Prende in input una lista di sitemap URLs e restituisce un DataFrame\n",
        "    con: URL, priorità e frequenza di aggiornamento.\n",
        "    '''\n",
        "    namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "    all_entries = []\n",
        "\n",
        "    if not sitemap_list:\n",
        "        print(\"Nessuna sitemap fornita.\")\n",
        "        return pd.DataFrame(columns=['url', 'priority', 'update'])\n",
        "\n",
        "    def parse_single_sitemap(sitemap_url):\n",
        "        try:\n",
        "            response = requests.get(sitemap_url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Errore durante il recupero di {sitemap_url}: {e}\")\n",
        "            return []\n",
        "\n",
        "        content_type = response.headers.get(\"Content-Type\", \"\").lower()\n",
        "\n",
        "        # Sitemap XML\n",
        "        if \"xml\" in content_type:\n",
        "            try:\n",
        "                root = ET.fromstring(response.content)\n",
        "            except ET.ParseError as e:\n",
        "                print(f\"Errore nel parsing XML: {e}\")\n",
        "                return []\n",
        "\n",
        "            # Sitemap index\n",
        "            if root.tag.endswith('index'):\n",
        "                sitemap_urls = [loc.text for loc in root.findall('ns:sitemap/ns:loc', namespace)]\n",
        "                entries = []\n",
        "                for sub_url in sitemap_urls:\n",
        "                    entries.extend(parse_single_sitemap(sub_url))\n",
        "                return entries\n",
        "\n",
        "            # Sitemap semplice\n",
        "            elif root.tag.endswith('urlset'):\n",
        "                return [{\n",
        "                    'url': url.findtext('ns:loc', default='', namespaces=namespace),\n",
        "                    'priority': url.findtext('ns:priority', default=None, namespaces=namespace),\n",
        "                    'update': url.findtext('ns:changefreq', default=None, namespaces=namespace)\n",
        "                } for url in root.findall('ns:url', namespace)]\n",
        "\n",
        "        # Sitemap HTML\n",
        "        elif \"html\" in content_type:\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "            return [{\n",
        "                'url': normalize_url(a['href']),\n",
        "                'priority': None,\n",
        "                'update': None\n",
        "            } for a in soup.find_all('a', href=True)]\n",
        "\n",
        "        else:\n",
        "            print(f\"Formato non riconosciuto per {sitemap_url} ({content_type})\")\n",
        "            return []\n",
        "\n",
        "    # Estrai tutte le voci da ogni sitemap\n",
        "    for sitemap_url in sitemap_list:\n",
        "        if sitemap_url:\n",
        "            entries = parse_single_sitemap(sitemap_url)\n",
        "            for entry in entries:\n",
        "                entry['url'] = normalize_url(entry['url'])  # Normalizzazione\n",
        "            all_entries.extend(entries)\n",
        "\n",
        "    df = pd.DataFrame(all_entries, columns=['url', 'priority', 'update']).drop_duplicates()\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "-b4iywtuKnz5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_page_url(html,sitemaps_urls,useragent=default_agent):\n",
        "  '''\n",
        "    dato una pagina html di input cerco gli urls:\n",
        "    - se sono in una directory privata li scarto\n",
        "    - se gia presenti nella sitemap, sono già inidicizzate e non salvo i link (duplicati)\n",
        "    - se non presenti salvo gli url permessi dal file robots.txt\n",
        "  '''\n",
        "\n",
        "  #parsing pagina html\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "  urls = []\n",
        "  # Elementi con attributo href\n",
        "  for tag in soup.find_all(href=True):\n",
        "      urls.append(tag['href'])\n",
        "\n",
        "  # Elementi con attributo src\n",
        "  for tag in soup.find_all(src=True):\n",
        "      urls.append(tag['src'])\n",
        "\n",
        "  # controlla se i nuovi url sono veramente accessibili per il web scraping (no in disallow)\n",
        "  if useragent.path_disallow != None:\n",
        "    for url in urls:\n",
        "      for path in useragent.path_disallow:\n",
        "        if path in url:\n",
        "         urls.remove(url)\n",
        "\n",
        "  # normalizzo gli urls\n",
        "    urls = [normalize_url(url) for url in urls]\n",
        "\n",
        "  # elimino i duplicati\n",
        "    urls = list(set(urls))\n",
        "\n",
        "\n",
        "  #controlla se ci sono duplicati nella sitemap se c'è\n",
        "  if sitemaps_urls != None:\n",
        "    def_urls = sitemaps_urls\n",
        "    for url in urls:\n",
        "      if url not in def_urls:\n",
        "         def_urls.append(url)\n",
        "  else:\n",
        "    def_urls = urls\n",
        "\n",
        "  return def_urls\n",
        "\n"
      ],
      "metadata": {
        "id": "NbmdtzHa4GKp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_page_tags(html,tag_type, attrs={}):\n",
        "  '''\n",
        "    accetta in input una pagina html, il tipo di tag da selezionare ed eventuali attributi se necessari\n",
        "  '''\n",
        "  # scelgo il parser\n",
        "  soup = BeautifulSoup(html, \"html.parser\")\n",
        "  tags = soup.find_all(tag_type, attrs=attrs)\n",
        "\n",
        "  return [str(tag) for tag in tags]"
      ],
      "metadata": {
        "id": "rJ6l9K4-UHYT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_page_tags_all(html,tags_type = ['p','article','section','li','h1','h2','h3']):\n",
        "  '''\n",
        "    accetta in input una pagina html, il tipo di tag da selezionare ed eventuali attributi se necessari\n",
        "  '''\n",
        "  # scelgo il parser\n",
        "  soup = BeautifulSoup(html, \"html.parser\")\n",
        "  tags = soup.find_all(tags_type) #dovrebbe rispettare l'ordine del DOM\n",
        "  texts = [tag.get_text(separator=' ', strip=True) for tag in tags]\n",
        "\n",
        "  return texts\n",
        "\n"
      ],
      "metadata": {
        "id": "AsZuPyQ_s5No"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html = fetch(start_url_due, default_agent)\n",
        "words = parse_page_tags_all(html)\n",
        "print(words)\n",
        "print(default_agent.last_access)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbdkGLyhwrre",
        "outputId": "b0a70050-ceff-4444-c0b0-b10bca7b501a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pagina recuperata con successo:https://www.wildraccoon.it/shop/\n",
            "['Home', 'Le birre', 'Shop Lattine Merchandising', 'Lattine', 'Merchandising', 'Il Metodo', 'Chi siamo', 'Bolli spina', 'Contatti', 'Home', 'Le birre', 'Shop Lattine Merchandising', 'Lattine', 'Merchandising', 'Il Metodo', 'Chi siamo', 'Bolli spina', 'Contatti', 'SHOP', 'Shop', 'Straight to harvest 22,00 € – 62,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Straight to harvest', 'Each others paranoia 20,00 € – 58,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Each others paranoia', 'Vfx 19,00 € – 54,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Vfx', 'Dolci on fire – Mango, banana & lassi 25,00 € – 68,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Dolci on fire – Mango, banana & lassi', 'End of the road 22,00 € – 62,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'End of the road', 'Niceness doesn’t last 19,00 € – 52,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Niceness doesn’t last', 'Don’t be gloomy 20,00 € – 58,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Don’t be gloomy', 'All the layers peanut 32,00 € – 96,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'All the layers peanut', 'All the layers tonka coffee 32,00 € – 96,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'All the layers tonka coffee', 'Good bye winter 18,00 € – 50,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Good bye winter', 'A little money 22,00 € – 62,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'A little money', 'Twenty shells 22,00 € – 62,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Twenty shells', 'Dolci on fire – PB&J 25,00 € – 68,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Dolci on fire – PB&J', 'Time and Space 24,00 € – 66,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Time and Space', 'Tears in the rain 20,00 € – 58,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Tears in the rain', 'APA – Blockbuster 18,00 € – 50,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'APA – Blockbuster', 'Ragioniere, batti? 17,00 € – 48,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Ragioniere, batti?', 'Cross your heart 22,00 € – 62,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Cross your heart', 'Arctic Terror 20,00 € – 58,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Arctic Terror', 'Gentle Crime 17,00 € – 48,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Gentle Crime', 'Maglietta beige Wild Raccoon 20,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Maglietta beige Wild Raccoon', 'Maglietta lilla Wild Raccoon 20,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Maglietta lilla Wild Raccoon', 'Bicchiere Wild Raccoon Oslo 400ml 5,00 € Aggiungi al carrello', 'Bicchiere Wild Raccoon Oslo 400ml', 'Crush on you 20,00 € – 58,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'Crush on you', 'One of a kind 17,00 € – 48,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', 'One of a kind', '5minute window 20,00 € – 58,00 € Scegli Questo prodotto ha più varianti. Le opzioni possono essere scelte nella pagina del prodotto', '5minute window', 'Cartone degustazione 60,00 € Aggiungi al carrello', 'Cartone degustazione', 'Wild Raccoon SRL Sede legale Via Giosue Carducci 22 Tavagnacco 33010 (UD) Sede operativa Via Decani Di Cussignacco Udine 33100 (UD) P.IVA 03067900302 email: amministrazione@wildraccoon.it', 'PRIVACY POLICY', 'COOKIE POLICY', 'FOLLOW US']\n",
            "196773083210\n"
          ]
        }
      ]
    }
  ]
}