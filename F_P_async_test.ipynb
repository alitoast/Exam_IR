{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rfc3986\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "import urllib.robotparser #per gestire il file robot.txt\n",
        "import xml.etree.ElementTree as ET  #per gestire i file xml\n",
        "import requests #per http\n",
        "import time #per gestire il tempo\n",
        "from bs4 import BeautifulSoup #per gestire il parsing del html (si pu√≤ usare anche per xml)\n",
        "import rfc3986 # per la normalizzazione degli urls\n",
        "import logging\n",
        "import asyncio\n",
        "import aiohttp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAMAnPbtaaWc",
        "outputId": "13028fc0-ebc3-47fd-e711-ea189a58da18"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rfc3986\n",
            "  Downloading rfc3986-2.0.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Downloading rfc3986-2.0.0-py2.py3-none-any.whl (31 kB)\n",
            "Installing collected packages: rfc3986\n",
            "Successfully installed rfc3986-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "BwrTvIA0Zivb"
      },
      "outputs": [],
      "source": [
        "\n",
        "#url di prova\n",
        "# ha sitemap-index ma non priority etc.\n",
        "start_url_uno = \"https://www.dragopublisher.com/it/\"\n",
        "# non ha sitemap\n",
        "start_url_due = \"https://www.wildraccoon.it/shop/\"\n",
        "# sito con sitemap\n",
        "start_url = \"https://www.bbc.com/\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# header generated by ChatGPT\n",
        "useragent_dict_header = {'Googlebot':'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',\n",
        "                    'Bingbot':'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)',\n",
        "                    'Slurp':'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)',\n",
        "                    'DuckDuckbot':'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)',\n",
        "                    'Yandex':'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)',\n",
        "                    '*': 'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)'}\n",
        "\n",
        "\n",
        "class UserAgentPolicy:\n",
        "  '''\n",
        "     Class to identify user agents and store relevant information to manage their requests appropriately over time\n",
        "  '''\n",
        "  def __init__(self, name, base_url, path_disallow, crawl_delay, request_rate,last_access, header, visited):\n",
        "        self.lock = asyncio.Lock()\n",
        "        self.base_url = base_url\n",
        "        self.name = name\n",
        "        self.path_disallow = path_disallow\n",
        "        self.crawl_delay = crawl_delay\n",
        "        self.request_rate = request_rate\n",
        "        self.last_access = last_access\n",
        "        self.header = header\n",
        "        self.visited = visited\n",
        "\n",
        "\n",
        "# Setup logging configuration\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
        "    datefmt='%H:%M:%S'\n",
        ")\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "useragent_dict = {\n",
        "    \"Googlebot\": UserAgentPolicy(\"Googlebot\", None, None, None, None, 1e9,\n",
        "                                 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)', set()),\n",
        "\n",
        "    \"Bingbot\": UserAgentPolicy(\"Bingbot\", None, None, None, None, 1e9,\n",
        "                               'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)', set()),\n",
        "\n",
        "    \"Slurp\": UserAgentPolicy(\"Slurp\", None, None, None, None, 1e9,\n",
        "                             'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)', set()),\n",
        "\n",
        "    \"DuckDuckbot\": UserAgentPolicy(\"DuckDuckbot\", None, None, None, None, 1e9,\n",
        "                                   'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)', set()),\n",
        "\n",
        "    \"Yandex\": UserAgentPolicy(\"Yandex\", None, None, None, None, 1e9,\n",
        "                              'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)', set()),\n",
        "\n",
        "    \"*\": UserAgentPolicy(\"*\", None, None, None, None, 1e9,\n",
        "                         'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)', set())\n",
        "}\n",
        "\n",
        "default_agent=useragent_dict[\"Googlebot\"]"
      ],
      "metadata": {
        "id": "Hjaw71k1ca14"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fetcher#"
      ],
      "metadata": {
        "id": "Q2RPwTOde_1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from types import new_class\n",
        "async def check_robots(url,useragent=default_agent):\n",
        "\n",
        "  '''\n",
        "    Inputs:\n",
        "        url (str): The base URL of the website to analyze.\n",
        "        useragent (class): The user agent to use when checking access permissions. Default is '*'.\n",
        "\n",
        "    Output:\n",
        "        sitemap (list or None): List of sitemap URLs found in robots.txt, if any.\n",
        "\n",
        "    Description:\n",
        "        This function checks the robots.txt file of a given website to determine:\n",
        "        - Whether the given user agent is allowed to fetch content.\n",
        "        - Which paths are disallowed for the user agent.\n",
        "        - The crawl delay and request rate for the user agent.\n",
        "        - Any sitemap URLs provided in the robots.txt.\n",
        "\n",
        "        It updates the given useragent object with these details.\n",
        "    '''\n",
        "\n",
        "  useragent.base_url = url\n",
        "\n",
        "  # Initialize the robot parser\n",
        "  rfl = urllib.robotparser.RobotFileParser()\n",
        "  # Construct the full URL to the robots.txt file\n",
        "  new_url = url + \"/robots.txt\"\n",
        "\n",
        "  try:\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            async with session.get(new_url, timeout=10) as response:\n",
        "                if response.status == 200:\n",
        "                    robots_txt = await response.text()\n",
        "                    # Setup and parse with robotparser\n",
        "                    rfl.set_url(new_url)\n",
        "                    rfl.parse(robots_txt.splitlines())\n",
        "                else:\n",
        "                    logging.warning(f\"Failed to fetch robots.txt: HTTP {response.status}\")\n",
        "                    return None\n",
        "  except aiohttp.ClientError as e:\n",
        "        logging.error(f\"Error fetching robots.txt from {new_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "  # Check if the user agent is allowed to fetch its own robots.txt\n",
        "  if rfl.can_fetch(useragent.name, new_url):\n",
        "        logging.info(f\"{useragent.name} is allowed to fetch {new_url}\")\n",
        "  else:\n",
        "        logging.warning(f\"{useragent.name} is NOT allowed to fetch {new_url}\")\n",
        "\n",
        "  # Check if the useragent can fetch the informations\n",
        "  check = rfl.can_fetch(useragent.name,new_url)\n",
        "\n",
        "  # Retrieve the list of the path not to follow\n",
        "  path_disallow = rfl.parse(\"Disallow\")\n",
        "  useragent.path_disallow = path_disallow\n",
        "\n",
        "  # Retrieve the crawl delay (minimum interval between requests)\n",
        "  crawl_delay = rfl.crawl_delay(useragent.name)\n",
        "  useragent.crawl_delay = crawl_delay\n",
        "\n",
        "  # Retrieve the request rate (tuple of requests per time interval)\n",
        "  request_rate = rfl.request_rate(useragent.name)\n",
        "  useragent.request_rate = request_rate\n",
        "\n",
        "  # Retrieve any sitemap URLs declared in the robots.txt\n",
        "  sitemap = rfl.site_maps()\n",
        "\n",
        "  return sitemap\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VHqjvHeGaque"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "async def check_time(useragent=default_agent):\n",
        "\n",
        "  '''\n",
        "    Inputs:\n",
        "        useragent (UserAgentPolicy): The user agent object containing rate-limiting information.\n",
        "\n",
        "    Output:\n",
        "      None\n",
        "\n",
        "    Description:\n",
        "        Ensures that the user agent respects the specified crawl delay and request rate policies:\n",
        "        - Calculates the appropriate delay based on the more restrictive value between `crawl_delay` and `request_rate`.\n",
        "        - If neither is defined, defaults to a delay of 1.5 seconds.\n",
        "        - Ensures the delay is respected by pausing execution as needed.\n",
        "        - Updates `last_access` to the current time after waiting.\n",
        "  '''\n",
        "\n",
        "\n",
        "  # If both crawl_delay and request_rate are defined, use the more restrictive one (converted to nanoseconds)\n",
        "\n",
        "  if useragent.request_rate and useragent.crawl_delay:\n",
        "     # Calculate delay from request rate: total interval (in ns) divided by number of requests\n",
        "     request_delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]\n",
        "     # Convert crawl delay from seconds to nanoseconds\n",
        "     crawl_delay = useragent.crawl_delay * 1e9\n",
        "     # Use the larger (i.e., more restrictive) delay\n",
        "     delay = max(request_delay, crawl_delay)\n",
        "\n",
        "  elif useragent.crawl_delay:\n",
        "     delay = useragent.crawl_delay * 1e9\n",
        "\n",
        "  elif useragent.request_rate:\n",
        "     delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]\n",
        "\n",
        "  # If neither crawl_delay and request rate are defined, use a default delay of 1.5 seconds\n",
        "  else:\n",
        "    delay = 1.5 * 1e9  # 1.5 secondi in nanosecondi\n",
        "\n",
        "  # Get the current time in nanoseconds\n",
        "  now = time.monotonic_ns()\n",
        "  wait = max(0, (useragent.last_access + delay) - now)\n",
        "\n",
        "  if wait > 0:\n",
        "     await asyncio.sleep(wait / 1e9)\n",
        "\n",
        "  # Update last_access to the current time to enforce timing on next request\n",
        "  useragent.last_access = time.monotonic_ns()\n"
      ],
      "metadata": {
        "id": "VgozsixYc0KM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def fetch(url, useragent=default_agent):\n",
        "    '''\n",
        "    Inputs:\n",
        "        url (str): The target URL to fetch.\n",
        "        useragent (UserAgentPolicy): The user agent object containing headers and rate-limiting rules.\n",
        "\n",
        "    Outputs:\n",
        "        str or None: The HTML content of the page if the request is successful (HTTP 200), otherwise None.\n",
        "\n",
        "    Description:\n",
        "        Sends an HTTP GET request to the specified URL, respecting the request policies defined for the given user agent.\n",
        "    '''\n",
        "    # To avoid simultaneous tasks between same useragents\n",
        "    async with useragent.lock:\n",
        "\n",
        "      # Check if the page has been visited already\n",
        "      if url in useragent.visited:\n",
        "          logging.warning(\"Page visited already\")\n",
        "          return None\n",
        "\n",
        "      # Enforce crawl delay and request rate restrictions for the user agent\n",
        "      await check_time(useragent)\n",
        "\n",
        "      headers = {\n",
        "          'User-Agent': useragent.header\n",
        "      }\n",
        "\n",
        "      try:\n",
        "          async with aiohttp.ClientSession() as session:\n",
        "              async with session.get(url, headers=headers, timeout=10) as response:\n",
        "                  if response.status == 200:\n",
        "                      html = await response.text()\n",
        "                      logging.info(\"Page successfully recovered\")\n",
        "                      useragent.visited.add(url)\n",
        "                      return html\n",
        "                  else:\n",
        "                      logging.warning(f\"Page not available: {url} (Status {response.status})\")\n",
        "                      return None\n",
        "      except aiohttp.ClientError as e:\n",
        "          logging.error(f\"Error, impossible to fetch {url}: {e}\")\n",
        "          return None\n"
      ],
      "metadata": {
        "id": "eIZYVQNpcoo9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parser#"
      ],
      "metadata": {
        "id": "wK3KSW3Qe2Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_url(url):\n",
        "\n",
        "  '''\n",
        "    Input:\n",
        "        url (str): The URL to be normalized.\n",
        "\n",
        "    Output:\n",
        "        str: The normalized URL as a string. If normalization fails, the original URL is returned.\n",
        "\n",
        "    Description:\n",
        "        Normalizes the input URL according to the RFC 3986 standard and returns a string representation.\n",
        "        This function uses the `rfc3986` library to parse and normalize the URL according to the\n",
        "        URI standard defined in RFC 3986. This includes handling issues such as case normalization,\n",
        "        removing default ports, sorting query parameters (if applicable), and more.\n",
        "\n",
        "        If an error occurs during normalization (e.g., invalid input), it catches the exception\n",
        "        and returns the original URL as a fallback.\n",
        "  '''\n",
        "\n",
        "  try:\n",
        "      uri = rfc3986.uri_reference(url).normalize()\n",
        "      return uri.unsplit()\n",
        "\n",
        "  except Exception as e:\n",
        "      logger.error(\"Error, impossble to normalize %s: %s\", url, e)\n",
        "      return url"
      ],
      "metadata": {
        "id": "wJjmOeqqkM89"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_spider_traps(url):\n",
        "\n",
        "  '''\n",
        "\n",
        "    Input:\n",
        "     url(str):The URL to be analyzed.\n",
        "\n",
        "    Output:\n",
        "      - Returns False if the URL is suspicious or considered a \"spider trap.\"\n",
        "      - Returns True if the URL seems safe to crawl.\n",
        "\n",
        "    Description:\n",
        "      Checks whether a given URL is potentially harmful or could trap a web crawler\n",
        "      in infinite loops or unnecessary crawling paths.\n",
        "\n",
        "  '''\n",
        "\n",
        "  MAX_URL_LENGTH = 200   # Arbitrary maximum allowed URL length\n",
        "  MAX_PATH_DEPTH = 6     # Maximum allowed number of slashes in path\n",
        "  trap_pattern = re.compile(r\"(calendar|sessionid|track|ref|sort|date=|page=\\d{3,})\", re.IGNORECASE)   # Pattern matching common signs of spider traps:\n",
        "                                                                                                       # calendars, session IDs, tracking params, endless pagination, etc.\n",
        "\n",
        "  link = urlparse(url)\n",
        "\n",
        "  # Check URL length\n",
        "  if len(link) > MAX_URL_LENGTH:\n",
        "     return False\n",
        "\n",
        "  # Check path depth (number of '/' in path)\n",
        "  if link.path.count('/') > MAX_PATH_DEPTH:\n",
        "     return False\n",
        "\n",
        "  # Check for suspicious patterns in the URL\n",
        "  if trap_pattern.search(url):\n",
        "     return False\n",
        "\n",
        "  return True\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lbf4bzUokMn5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "async def fetch_sitemap(session, sitemap_url):\n",
        "    '''\n",
        "    Input:\n",
        "    session : aiohttp.ClientSession\n",
        "        An existing aiohttp session used to make the HTTP request.\n",
        "\n",
        "    sitemap_url : str\n",
        "        The URL pointing to a sitemap (typically found in robots.txt or known ahead of time).\n",
        "\n",
        "    Output:\n",
        "    list of dict\n",
        "        A list of dictionaries containing:\n",
        "            - 'url': the page URL\n",
        "            - 'priority': optional priority value (from XML)\n",
        "            - 'update': optional change frequency (from XML)\n",
        "        If the sitemap is invalid or inaccessible, returns an empty list.\n",
        "\n",
        "    Description:\n",
        "    Asynchronously fetches and parses a single sitemap (either XML or HTML) and returns a list of URLs of sitemaps\n",
        "    This function supports:\n",
        "        - XML sitemaps (including recursive sitemap index support)\n",
        "        - HTML sitemaps (basic <a> link extraction)\n",
        "    It validates HTTP status, parses content based on MIME type, and returns structured data\n",
        "    ready for further processing or filtering.\n",
        "    '''\n",
        "\n",
        "    try:\n",
        "        # Perform an asynchronous HTTP GET request for the sitemap URL\n",
        "        async with session.get(sitemap_url, timeout=10) as response:\n",
        "\n",
        "            # Handle HTTP failure (non-200 status)\n",
        "            if response.status != 200:\n",
        "                logger.warning(f\"Failed to fetch {sitemap_url} (Status {response.status})\")\n",
        "                return []\n",
        "\n",
        "            # Extract MIME type and content\n",
        "            content_type = response.headers.get(\"Content-Type\", \"\").lower()\n",
        "            content = await response.text()\n",
        "\n",
        "            # XML Sitemap\n",
        "            if \"xml\" in content_type:\n",
        "                try:\n",
        "                    # Parse the XML content into an ElementTree structure\n",
        "                    root = ET.fromstring(content.encode())\n",
        "                except ET.ParseError as e:\n",
        "                    logger.error(f\"XML parse error in {sitemap_url}: {e}\")\n",
        "                    return []\n",
        "\n",
        "                # Define the XML namespace used by standard sitemaps\n",
        "                namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "\n",
        "                # Handle <sitemapindex> files (recursive fetching of sub-sitemaps)\n",
        "                if root.tag.endswith('index'):\n",
        "                    sub_sitemaps = [loc.text for loc in root.findall('ns:sitemap/ns:loc', namespace)]\n",
        "                    results = []\n",
        "                    for sub_url in sub_sitemaps:\n",
        "                        # Recursively fetch and parse each child sitemap\n",
        "                        results.extend(await fetch_sitemap(session, sub_url))\n",
        "                    return results\n",
        "\n",
        "                # Handle regular <urlset> sitemap with actual page URLs\n",
        "                elif root.tag.endswith('urlset'):\n",
        "                    return [{\n",
        "                        'url': url.findtext('ns:loc', default='', namespaces=namespace),\n",
        "                        'priority': url.findtext('ns:priority', default=None, namespaces=namespace),\n",
        "                        'update': url.findtext('ns:changefreq', default=None, namespaces=namespace)\n",
        "                    } for url in root.findall('ns:url', namespace)]\n",
        "\n",
        "            #HTML Sitemap\n",
        "\n",
        "            elif \"html\" in content_type:\n",
        "                soup = BeautifulSoup(content, \"html.parser\")\n",
        "                return [{\n",
        "                    'url': normalize_url(a['href']),  # Normalize the extracted link\n",
        "                    'priority': None,                 # HTML sitemaps usually don't include priority\n",
        "                    'update': None\n",
        "                } for a in soup.find_all('a', href=True)]\n",
        "\n",
        "            # Unsupported format\n",
        "            else:\n",
        "                logger.warning(f\"Unsupported format: {sitemap_url} ({content_type})\")\n",
        "                return []\n",
        "\n",
        "    # Handle network or HTTP client exceptions\n",
        "    except aiohttp.ClientError as e:\n",
        "        logger.error(f\"Error fetching {sitemap_url}: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "async def parse_sitemap(sitemap_list):\n",
        "    '''\n",
        "    Input:\n",
        "        sitemap_list (list): A list of sitemap URLs to parse.\n",
        "\n",
        "    Output:\n",
        "          Returns a DataFrame containing:\n",
        "         - URL\n",
        "         - Priority\n",
        "         - Update frequency\n",
        "\n",
        "\n",
        "    Description:\n",
        "         This function uses aiohttp to fetch sitemaps asynchronously. It supports:\n",
        "            - XML sitemaps (standard or sitemap index)\n",
        "            - HTML-based sitemaps as fallback\n",
        "        It normalizes the URLs, filters out potential spider traps,\n",
        "        and merges the results into a clean DataFrame for crawling or analysis.\n",
        "    '''\n",
        "\n",
        "    # If the list is empty, return an empty DataFrame\n",
        "    if not sitemap_list:\n",
        "        logger.warning(\"No sitemap available.\")\n",
        "        return pd.DataFrame(columns=['url', 'priority', 'update'])\n",
        "\n",
        "    all_entries = []  # Accumulate all parsed and cleaned URL entries here\n",
        "\n",
        "    # Create an asynchronous HTTP session for efficient reuse of connections\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "\n",
        "        # Create a list of async tasks for all sitemap URLs to fetch and parse them concurrently\n",
        "        tasks = [fetch_sitemap(session, sitemap_url) for sitemap_url in sitemap_list if sitemap_url]\n",
        "\n",
        "        # Run all fetch tasks concurrently and wait for them to complete\n",
        "        results = await asyncio.gather(*tasks)\n",
        "\n",
        "        # Process each list of entries returned from individual sitemap URLs\n",
        "        for entries in results:\n",
        "            for entry in entries:\n",
        "                url = normalize_url(entry['url'])  # Normalize the URL to a standard format\n",
        "                if check_spider_traps(url):        # Filter out known spider traps\n",
        "                    entry['url'] = url\n",
        "                    all_entries.append(entry)       # Keep the cleaned and validated entry\n",
        "\n",
        "    # Convert list of entries into a DataFrame and remove duplicate URLs\n",
        "    df = pd.DataFrame(all_entries, columns=['url', 'priority', 'update']).drop_duplicates()\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def parse_page_url(html,sitemaps_urls,useragent=default_agent):\n",
        "\n",
        "  '''\n",
        "     Inputs:\n",
        "     - html(str): The HTML content of the page.\n",
        "     - sitemaps_urls(str): A list of URLs already known from sitemaps.\n",
        "     - useragent(class): A user agent object containing disallowed paths (robots.txt rules).\n",
        "\n",
        "     Output:\n",
        "     - A list of new, allowed URLs extracted from the page.\n",
        "\n",
        "     Description:\n",
        "       Given an HTML page, this function extracts all URLs and filters them based on:\n",
        "       - Exclusion of private or disallowed directories (from robots.txt rules)\n",
        "       - Deduplication with URLs already found in the sitemap\n",
        "       - Optional normalization and trap filtering\n",
        "\n",
        "  '''\n",
        "\n",
        "  # Parse HTML content\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "  urls = []\n",
        "  # Extract URLs from href attributes (e.g., <a>, <link>, etc.)\n",
        "  for tag in soup.find_all(href=True):\n",
        "      urls.append(tag['href'])\n",
        "\n",
        "  # Extract URLs from src attributes (e.g., <img>, <script>, etc.)\n",
        "  for tag in soup.find_all(src=True):\n",
        "      urls.append(tag['src'])\n",
        "\n",
        "  # Filter out disallowed URLs based on robots.txt rules\n",
        "  if useragent.path_disallow != None:\n",
        "    for url in urls:\n",
        "      for path in useragent.path_disallow:\n",
        "        if path in url:\n",
        "         urls.remove(url)\n",
        "\n",
        "  # Normalize URLs (e.g., remove fragments, resolve relative paths, etc.)\n",
        "    urls = [normalize_url(url) for url in urls]\n",
        "\n",
        "  # Remove duplicate URLs\n",
        "    urls = list(set(urls))\n",
        "\n",
        "  # Filter out potential spider traps\n",
        "    urls = [url for url in urls if check_spider_traps(url)]\n",
        "\n",
        "  # Filter out URLs that are already in the sitemap\n",
        "  if sitemaps_urls != None:\n",
        "    def_urls = sitemaps_urls\n",
        "    for url in urls:\n",
        "      if url not in def_urls:\n",
        "         def_urls.append(url)\n",
        "  else:\n",
        "    def_urls = urls\n",
        "\n",
        "  return def_urls\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ux4MqwcFdQBf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_page_tags_all(html,tags_type = None):\n",
        "\n",
        "  '''\n",
        "    Inputs:\n",
        "    - html: The HTML content as a string.\n",
        "    - tags_type: A list of tag names to search for (default includes common content tags).\n",
        "\n",
        "    Output:\n",
        "    - A list of text strings extracted from the specified tags, preserving DOM order\n",
        "\n",
        "    Description:\n",
        "      Parses the given HTML content and extracts text from specified HTML tags.\n",
        "\n",
        "  '''\n",
        "  if tags_type == None:\n",
        "     tags_type = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span', 'a']\n",
        "\n",
        "  # Initialize the HTML parser\n",
        "  soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "  # Find tag to remove\n",
        "  for tag in soup(['script','style','footer','nav','noscript','header','form','aside']):\n",
        "      tag.decompose()\n",
        "\n",
        "  # Find all comments and removes them\n",
        "  for comment in soup.find_all(string=lambda text: isinstance(text,comment)):\n",
        "      comment.extract()\n",
        "\n",
        "  # Find all tags of the specified types (respects DOM order)\n",
        "  tags = soup.find_all(tags_type)\n",
        "\n",
        "  # Extract clean text from each tag (removing whitespace and combining with spaces)\n",
        "  texts = [tag.get_text(separator=' ', strip=True) for tag in tags]\n",
        "\n",
        "  return texts\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dCTZxBJLj_Kj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test#"
      ],
      "metadata": {
        "id": "n_fp4O6tfDPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "async def main_page(start_url: str):\n",
        "    # STEP 1: Recupera robots.txt e ottieni le sitemap (se presenti)\n",
        "    sitemaps = await check_robots(start_url, useragent=default_agent)\n",
        "\n",
        "    if not sitemaps:\n",
        "        print(f\"[!] Nessuna sitemap trovata per {start_url}\")\n",
        "        return\n",
        "\n",
        "    print(f\"[‚úì] Sitemap trovate: {sitemaps}\")\n",
        "\n",
        "    # STEP 2: Parsing asincrono delle sitemap\n",
        "    df = await parse_sitemap(sitemaps)\n",
        "    print(f\"[‚úì] URLs raccolti dalle sitemap: {len(df)}\")\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"[-] Nessun URL valido trovato nella sitemap.\")\n",
        "        return\n",
        "\n",
        "    # STEP 3: Scegli un URL da testare (il primo della sitemap)\n",
        "    test_url = df.iloc[0][\"url\"]\n",
        "    print(f\"[‚Üí] Provo a fare fetch della pagina: {test_url}\")\n",
        "\n",
        "    # STEP 4: Recupera HTML\n",
        "    html = await fetch(test_url, useragent=default_agent)\n",
        "    if html is None:\n",
        "        print(\"[-] Impossibile scaricare la pagina.\")\n",
        "        return\n",
        "\n",
        "    # STEP 5: Estrai contenuti testuali dalla pagina\n",
        "    texts = parse_page_tags_all(html)\n",
        "    print(f\"[‚úì] Estratti {len(texts)} blocchi di testo.\")\n",
        "\n",
        "    # STEP 6: Estrai e filtra nuovi link dalla pagina\n",
        "    new_links = parse_page_url(html, sitemaps_urls=list(df['url']), useragent=default_agent)\n",
        "    print(f\"[‚úì] Nuovi link scoperti e filtrati: {len(new_links)}\")\n",
        "\n",
        "    # Output di esempio\n",
        "    print(\"\\n[+] Esempio di testo trovato:\")\n",
        "    for t in texts[:3]:\n",
        "        print(f\"- {t}\")\n",
        "\n",
        "    print(\"\\n[+] Esempio di link nuovi:\")\n",
        "    for link in new_links[:5]:\n",
        "        print(f\"- {link}\")\n"
      ],
      "metadata": {
        "id": "EEEjdzCvlzw3"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await main_page(start_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZFqt01knaAf",
        "outputId": "095f75ad-4b13-450e-81a4-eabf8086c845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[‚úì] Sitemap trovate: ['https://www.bbc.com/sitemaps/https-index-com-archive.xml', 'https://www.bbc.com/sitemaps/https-index-com-news.xml', 'https://www.bbc.com/sitemaps/https-index-com-archive_video.xml', 'https://www.bbc.com/sitemaps/https-index-com-video.xml', 'https://www.bbc.com/sitemaps/sitemap-com-ws-topics.xml', 'https://www.bbc.com/sport/sitemap.xml', 'https://www.bbc.com/sitemaps/sitemap-com-ws-topics.xml', 'https://www.bbc.com/afrique/sitemap.xml', 'https://www.bbc.com/arabic/sitemap.xml', 'https://www.bbc.com/bengali/sitemap.xml', 'https://www.bbc.com/burmese/sitemap.xml', 'https://www.bbc.com/gahuza/sitemap.xml', 'https://www.bbc.com/hausa/sitemap.xml', 'https://www.bbc.com/hindi/sitemap.xml', 'https://www.bbc.com/indonesia/sitemap.xml', 'https://www.bbc.com/mundo/sitemap.xml', 'https://www.bbc.com/pashto/sitemap.xml', 'https://www.bbc.com/persian/sitemap.xml', 'https://www.bbc.com/portuguese/sitemap.xml', 'https://www.bbc.com/russian/sitemap.xml', 'https://www.bbc.com/swahili/sitemap.xml', 'https://www.bbc.com/tajik/sitemap.xml', 'https://www.bbc.com/turkce/sitemap.xml', 'https://www.bbc.com/ukchina/simp/sitemap.xml', 'https://www.bbc.com/ukrainian/sitemap.xml', 'https://www.bbc.com/urdu/sitemap.xml', 'https://www.bbc.com/uzbek/sitemap.xml', 'https://www.bbc.com/vietnamese/sitemap.xml', 'https://www.bbc.com/zhongwen/simp/sitemap.xml', 'https://www.bbc.com/zhongwen/trad/sitemap.xml', 'https://www.bbc.com/bbcx/index_sitemap.xml', 'https://www.bbc.com/bbcx/audio_archive_sitemap.xml']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Failed to fetch https://www.bbc.com/audio_archive_sitemap/page/0 (Status 404)\n",
            "WARNING:__main__:Failed to fetch https://www.bbc.com/tajik/sitemap.xml (Status 404)\n",
            "WARNING:__main__:Failed to fetch https://www.bbc.com/audio_archive_sitemap/page/1 (Status 404)\n",
            "WARNING:__main__:Failed to fetch https://www.bbc.com/audio_archive_sitemap/page/2 (Status 404)\n",
            "WARNING:__main__:Failed to fetch https://www.bbc.com/audio_archive_sitemap/page/3 (Status 404)\n",
            "WARNING:__main__:Failed to fetch https://www.bbc.com/audio_archive_sitemap/page/4 (Status 404)\n",
            "WARNING:__main__:Failed to fetch https://www.bbc.com/audio_archive_sitemap/page/5 (Status 404)\n",
            "WARNING:__main__:Failed to fetch https://www.bbc.com/audio_archive_sitemap/page/6 (Status 404)\n",
            "WARNING:__main__:Failed to fetch https://www.bbc.com/audio_archive_sitemap/page/7 (Status 404)\n",
            "WARNING:__main__:Failed to fetch https://www.bbc.com/audio_archive_sitemap/page/8 (Status 404)\n",
            "WARNING:__main__:Failed to fetch https://www.bbc.com/audio_archive_sitemap/page/9 (Status 404)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test pi√π richieste stesso useragent"
      ],
      "metadata": {
        "id": "-eUl9GZlud-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import asyncio\n",
        "\n",
        "async def fetch_simulated(url, useragent):\n",
        "    print(useragent.last_access,useragent.crawl_delay,useragent.request_rate)\n",
        "    await check_time(useragent)\n",
        "    now = datetime.now().strftime('%H:%M:%S.%f')[:-3]\n",
        "    print(f\"[{useragent.name}] FETCH: {url} at {now}\")\n",
        "    await asyncio.sleep(0.1)  # simula tempo di elaborazione\n",
        "    return f\"Simulated HTML content from {url}\"\n",
        "\n",
        "async def main():\n",
        "    url = start_url\n",
        "    tasks = [fetch_simulated(url, default_agent) for _ in range(5)]\n",
        "    await asyncio.gather(*tasks)\n"
      ],
      "metadata": {
        "id": "o-0V7cE8rVQX"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUD6YMCHrW4G",
        "outputId": "ac47af73-821e-4c44-ac96-f34a0eed7ba3"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5544373121139 None None\n",
            "[Googlebot] FETCH: https://www.bbc.com/ at 15:49:59.728\n",
            "5567620365597 None None\n",
            "5567620365597 None None\n",
            "5567620365597 None None\n",
            "5567620365597 None None\n",
            "[Googlebot] FETCH: https://www.bbc.com/ at 15:50:01.232\n",
            "[Googlebot] FETCH: https://www.bbc.com/ at 15:50:01.232\n",
            "[Googlebot] FETCH: https://www.bbc.com/ at 15:50:01.232\n",
            "[Googlebot] FETCH: https://www.bbc.com/ at 15:50:01.232\n"
          ]
        }
      ]
    }
  ]
}