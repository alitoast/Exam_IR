
"""
Web Fetcher Module
==================

Description:
------------
This module provides tools to handle web page requests in a respectful and controlled manner,
complying with website-specific crawling policies as defined in `robots.txt`.

It includes:
- Support for multiple user agents (e.g., Googlebot, Bingbot, etc.)
- Retrieval and parsing of `robots.txt` files to determine crawling rules
- Rate-limiting mechanisms based on `crawl-delay` and `request-rate`
- Controlled HTTP GET requests using appropriate headers
- Detection of sitemap URLs from `robots.txt`

Key Features:
-------------
1. **UserAgentPolicy Class**:
   Stores metadata and crawling policies (like headers, delay, etc.) for different user agents.

2. **check_robots(url, useragent)**:
   - Parses the site's `robots.txt`
   - Updates user agent rules (disallowed paths, delays, rate limits)
   - Extracts sitemap URLs if available

3. **check_time(useragent)**:
   - Enforces timing restrictions to avoid overloading the server
   - Computes and waits for appropriate delay based on the most restrictive policy

4. **fetch(url, useragent)**:
   - Sends a GET request to a given URL
   - Applies crawl timing policies
   - Returns the page content if successful, or `None` on failure

Dependencies:
-------------
- `requests`          : HTTP requests
- `urllib.robotparser`: Robots.txt parsing
- `time`              : Timing and delays
- `bs4` (BeautifulSoup): Optional HTML/XML parsing
- `rfc3986`           : URL normalization
- `re`, `pandas`, `xml.etree.ElementTree` for extended functionality

Use Cases:
----------
- Building polite web scrapers and crawlers
- Testing different user agent behaviors
- Auditing or discovering site sitemap structures

"""



pip install rfc3986

import pandas as pd
import re
from urllib.parse import urlparse
import urllib.robotparser #per gestire il file robot.txt
import xml.etree.ElementTree as ET  #per gestire i file xml
import requests #per http
import time #per gestire il tempo
from bs4 import BeautifulSoup #per gestire il parsing del html (si può usare anche per xml)
import rfc3986 # per la normalizzazione degli urls


#url di prova
# ha sitemap-index ma non priority etc.
start_url_uno = "https://www.dragopublisher.com/it/"
# non ha sitemap
start_url_due = "https://www.wildraccoon.it/shop/"
# sito giocattolo ha tutto
start_url = "https://nonciclopedia.org/wiki/Wikipedia"


# Useragents
useragent_dict = {"Googlebot": UserAgentPolicy("Googlebot",None, None, None, None, 1e9,'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'),
                  "Bingbot": UserAgentPolicy("Bingbot",None, None, None, None, 1e9,'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)'),
                  "Slurp": UserAgentPolicy("Slurp",None, None, None, None, 1e9, 'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)'),
                  "DuckDuckbot": UserAgentPolicy("DuckDuckbot",None, None, None, None, 1e9,'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)' ),
                  "Yandex": UserAgentPolicy("Yandex",None, None, None, None, 1e9,'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)' ),
                  "*": UserAgentPolicy("*",None, None, None, None, 1e9,'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)' )
                  }
default_agent = useragent_dict["Googlebot"]

# header generated by ChatGPT
useragent_dict_header = {'Googlebot':'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',
                    'Bingbot':'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)',
                    'Slurp':'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)',
                    'DuckDuckbot':'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)',
                    'Yandex':'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)',
                    '*': 'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)'}


class UserAgentPolicy:
  '''
     Class to identify user agents and store relevant information to manage their requests appropriately over time
  '''
  def __init__(self, name, base_url, path_disallow, crawl_delay, request_rate,last_access, header):
        self.base_url = base_url
        self.name = name
        self.path_disallow = path_disallow
        self.crawl_delay = crawl_delay
        self.request_rate = request_rate
        self.last_access = last_access
        self.header = header






def check_robots(url,useragent=default_agent):

  '''
    Inputs:
        url (str): The base URL of the website to analyze.
        useragent (class): The user agent to use when checking access permissions. Default is '*'.

    Output:
        sitemap (list or None): List of sitemap URLs found in robots.txt, if any.

    Description:
        This function checks the robots.txt file of a given website to determine:
        - Whether the given user agent is allowed to fetch content.
        - Which paths are disallowed for the user agent.
        - The crawl delay and request rate for the user agent.
        - Any sitemap URLs provided in the robots.txt.

        It updates the given useragent object with these details.
    '''

  useragent.base_url = url

  # Initialize the robot parser
  rfl = urllib.robotparser.RobotFileParser()
  # Construct the full URL to the robots.txt file
  new_url = url + "/robots.txt"

  rfl.set_url(new_url)
  # Read and parse the robots.txt file
  rfl.read()
  check = rfl.can_fetch(useragent,new_url)
  if check:
    print("{} il sito puo effettuare il fetch dei dati".format(check))
  else:
    print("il sito non puo effettuare il fetch dei dati")

  # Retrieve the list of the path not to follow
  path_disallow = rfl.parse("Disallow")
  useragent.path_disallow = path_disallow

  # Retrieve the crawl delay (minimum interval between requests)
  crawl_delay = rfl.crawl_delay(useragent)
  useragent.crawl_delay = crawl_delay

  # Retrieve the request rate (tuple of requests per time interval)
  request_rate = rfl.request_rate(useragent)
  useragent.request_rate = request_rate

  # Retrieve any sitemap URLs declared in the robots.txt
  sitemap = rfl.site_maps()
  print("sitemap: {}".format(sitemap))

  return sitemap





def check_time(useragent=default_agent):

  '''
    Inputs:
        useragent (UserAgentPolicy): The user agent object containing rate-limiting information.

    Output:
      None

    Description:
        Ensures that the user agent respects the specified crawl delay and request rate policies:
        - Calculates the appropriate delay based on the more restrictive value between `crawl_delay` and `request_rate`.
        - If neither is defined, defaults to a delay of 1.5 seconds.
        - Ensures the delay is respected by pausing execution as needed.
        - Updates `last_access` to the current time after waiting.
  '''


  # If both crawl_delay and request_rate are defined, use the more restrictive one (converted to nanoseconds)

  if useragent.request_rate and useragent.crawl_delay:
     # Calculate delay from request rate: total interval (in ns) divided by number of requests
     request_delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]
     # Convert crawl delay from seconds to nanoseconds
     crawl_delay = useragent.crawl_delay * 1e9
     # Use the larger (i.e., more restrictive) delay
     delay = max(request_delay, crawl_delay)

  elif useragent.crawl_delay:
     delay = useragent.crawl_delay * 1e9

  elif useragent.request_rate:
     delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]

  # If neither crawl_delay and request rate are defined, use a default delay of 1.5 seconds
  else:
    delay = 1.5 * 1e9  # 1.5 secondi in nanosecondi

  # Get the current time in nanoseconds
  now = time.monotonic_ns()
  wait = max(0, (useragent.last_access + delay) - now)

  if wait > 0:
     time.sleep(wait / 1e9)

  # Update last_access to the current time to enforce timing on next request
  useragent.last_access = time.monotonic_ns()


visulized_url = set()


def fetch(url,useragent=default_agent):
  '''
    Inputs:
        url (str): The target URL to fetch.
        useragent (UserAgentPolicy): The user agent object containing headers and rate-limiting rules.

    Outputs:
        str or None: The HTML content of the page if the request is successful (HTTP 200), otherwise None.

    Description:
        Sends an HTTP GET request to the specified URL, respecting the request policies defined for the given user agent.

    '''

  # Check if the page is been visited alredy
  if url not in visulized_url:

    # Enforce crawl delay and request rate restrictions for the user agent
    check_time(useragent)

    # Build the request headers using the specified user agent (optional, useful for testing)
    headers = {
          'User-Agent': useragent.header
     }

    # Send the HTTP GET request
    try:
       response = requests.get(url, headers=headers, timeout=10)
       if response.status_code == 200:
         print("Pagina recuperata con successo:{}".format(url))
         html = response.text
         visulized_url.add(url)
         return html
       else:
        print("Errore nella ricezione del sito:{}".format(url))
        return None

    # Handle any request-related exceptions (connection errors, timeouts, etc.)
    except requests.RequestException as e:
          print("Eccezione durante la richiesta:", e)
          return None
  else:
    print("Pagina già visulizzata:{}".format(url))
    return None


# --------- MAIN FOR TESTING --------- #
if __name__ == "__main__":
    test_url = "https://nonciclopedia.org/wiki/Wikipedia"
    sitemap = check_robots(test_url, useragent=default_agent)

    if sitemap:
        for sm_url in sitemap:
            html = fetch(sm_url, useragent=default_agent)
            if html:
               print("Sitemap successul retrieved")
    else:
        html = fetch(test_url, useragent=default_agent)
        if html:
            print("Fetched content:{}".format(html))