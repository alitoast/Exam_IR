"""
fetcher-EVA then modified
"""

import pandas as pd
import re
from urllib.parse import urlparse
import urllib.robotparser # to analyse the robot.txt
import xml.etree.ElementTree as ET  # to modify xml file 
import time # time management
from bs4 import BeautifulSoup # parsing method
import rfc3986 # to normalize urls
import logging 

import aiohttp # switch to async
import asyncio

# Setup logging configuration
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S',
    filename="fetcher.log",  # log file
    filemode="a"  # append to the log file ('w' to overwrite)
)

logger = logging.getLogger(__name__)


# Useragents
useragent_dict = {"Googlebot": UserAgentPolicy("Googlebot",None, None, None, None, 1e9,'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',set()),
                  "Bingbot": UserAgentPolicy("Bingbot",None, None, None, None, 1e9,'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)'),set(),
                  "Slurp": UserAgentPolicy("Slurp",None, None, None, None, 1e9, 'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)',set()),
                  "DuckDuckbot": UserAgentPolicy("DuckDuckbot",None, None, None, None, 1e9,'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)',set()),
                  "Yandex": UserAgentPolicy("Yandex",None, None, None, None, 1e9,'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)',set()),
                  "*": UserAgentPolicy("*",None, None, None, None, 1e9,'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)',set())
                  }
default_agent = useragent_dict["Googlebot"]

# header generated by ChatGPT
useragent_dict_header = {'Googlebot':'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',
                    'Bingbot':'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)',
                    'Slurp':'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)',
                    'DuckDuckbot':'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)',
                    'Yandex':'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)',
                    '*': 'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)'}



class UserAgentPolicy:
  '''
     Class to identify user agents and store relevant information to manage their requests appropriately over time
  '''
  def __init__(self, name, base_url, path_disallow, crawl_delay, request_rate,last_access, header, visited):
    self.base_url = base_url
    self.name = name
    self.path_disallow = path_disallow
    self.crawl_delay = crawl_delay
    self.request_rate = request_rate
    self.last_access = last_access
    self.header = header
    self.visited = visited

def check_robots(url,useragent=default_agent):
    '''
    Inputs:
        url (str): The base URL of the website to analyze.
        useragent (class): The user agent to use when checking access permissions. Default is '*'.

    Output:
        sitemap (list or None): List of sitemap URLs found in robots.txt, if any.

    Description:
        This function checks the robots.txt file of a given website to determine:
        - Whether the given user agent is allowed to fetch content.
        - Which paths are disallowed for the user agent.
        - The crawl delay and request rate for the user agent.
        - Any sitemap URLs provided in the robots.txt.

        It updates the given useragent object with these details.
    '''

    useragent.base_url = url

    # Initialize the robot parser
    rfl = urllib.robotparser.RobotFileParser()
    # Construct the full URL to the robots.txt file
    new_url = url + "/robots.txt"

    rfl.set_url(new_url)
    # Read and parse the robots.txt file
    rfl.read()
    check = rfl.can_fetch(useragent,new_url)
    if check:
        logger.info("The useragent can fetch the link.")
    else:
        logger.warning("Impossible to fetch %s", new_url)

    # Retrieve the list of the path not to follow
    path_disallow = rfl.parse("Disallow")
    useragent.path_disallow = path_disallow

    # Retrieve the crawl delay (minimum interval between requests)
    crawl_delay = rfl.crawl_delay(useragent)
    useragent.crawl_delay = crawl_delay

    # Retrieve the request rate (tuple of requests per time interval)
    request_rate = rfl.request_rate(useragent)
    useragent.request_rate = request_rate

    # Retrieve any sitemap URLs declared in the robots.txt
    sitemap = rfl.site_maps()
    
    return sitemap

async def check_time(useragent=default_agent):
    '''
    Inputs:
        useragent (UserAgentPolicy): The user agent object containing rate-limiting information.

    Output:
      None

    Description:
        Ensures that the user agent respects the specified crawl delay and request rate policies:
        - Calculates the appropriate delay based on the more restrictive value between crawl_delay and request_rate.
        - If neither is defined, defaults to a delay of 1.5 seconds.
        - Ensures the delay is respected by pausing execution as needed.
        - Updates last_access to the current time after waiting.
    '''


    # If both crawl_delay and request_rate are defined, use the more restrictive one (converted to nanoseconds)

    if useragent.request_rate and useragent.crawl_delay:
        # Calculate delay from request rate: total interval (in ns) divided by number of requests
        request_delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]
        # Convert crawl delay from seconds to nanoseconds
        crawl_delay = useragent.crawl_delay * 1e9
        # Use the larger (i.e., more restrictive) delay
        delay = max(request_delay, crawl_delay)

    elif useragent.crawl_delay:
        delay = useragent.crawl_delay * 1e9

    elif useragent.request_rate:
        delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]

    # If neither crawl_delay and request rate are defined, use a default delay of 1.5 seconds
    else:
        delay = 1.5 * 1e9  # 1.5 secondi in nanosecondi

    # Get the current time in nanoseconds
    now = time.monotonic_ns()
    wait = max(0, (useragent.last_access + delay) - now)

    if wait > 0:
            await asyncio.sleep(wait / 1e9)

    # Update last_access to the current time to enforce timing on next request
    useragent.last_access = time.monotonic_ns()


async def fetch(url,useragent=default_agent, session=None):
    '''
    Inputs:
        url (str): The target URL to fetch.
        useragent (UserAgentPolicy): The user agent object containing headers and rate-limiting rules.

    Outputs:
        str or None: The HTML content of the page if the request is successful (HTTP 200), otherwise None.

    Description:
        Sends an HTTP GET request to the specified URL, respecting the request policies defined for the given user agent.

    '''

    # Check if the page is been visited alredy
    if url in useragent.visited:
        logging.warning("Page already visited: %s", url)
        return None
    
    await check_time(useragent)
    headers = {'User-Agent': useragent.header}

    try:
        async with session.get(url, headers=headers, timeout=10) as response:
            if response.status == 200:
                html = await response.text()
                logging.info("Page successfully recovered: %s", url)
                useragent.visited.add(url)
                return html
            else:
                logging.warning("Page not available: %s (status: %d)", url, response.status)
                return None
    except aiohttp.ClientError as e:
        logging.error("Error fetching %s: %s", url, e)
        return None