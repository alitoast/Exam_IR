"""
Web Fetcher Module (Asynchronous)
=================================

Description:
------------
This module provides tools to handle web page requests in a respectful, non-blocking, and controlled manner,
complying with website-specific crawling policies defined in `robots.txt`.

It is designed to support **high-concurrency scenarios** by using asynchronous functions (`async`/`await`) and non-blocking HTTP requests via `aiohttp`.

Features:
---------
- Support for multiple user agents (e.g., Googlebot, Bingbot, etc.)
- Asynchronous retrieval and parsing of `robots.txt` files
- Rate-limiting based on `crawl-delay` and `request-rate` rules per user agent
- Asynchronous HTTP GET requests with proper headers
- Detection of sitemap URLs declared in `robots.txt`
- Efficient timing control via `asyncio.sleep` to avoid overloading servers

Key Components:
---------------
1. **UserAgentPolicy Class**:
   - Stores metadata and crawling policies for each user agent, such as header, delay settings, and visited URLs.

2. **async check_robots(url, useragent)**:
   - Fetches and parses the site's `robots.txt` file
   - Determines fetch permissions for the user agent
   - Updates rules like `crawl-delay`, `request-rate`, disallowed paths, and sitemaps

3. **async check_time(useragent)**:
   - Computes the appropriate delay between requests (based on crawl rules)
   - Awaits asynchronously to enforce timing limits without blocking the event loop

4. **async fetch(url, useragent)**:
   - Sends asynchronous HTTP GET requests
   - Ensures timing rules are respected
   - Returns the page's HTML content if successfully fetched

Dependencies:
-------------
- `aiohttp`            : For asynchronous HTTP requests
- `asyncio`            : For managing async flow and delays
- `requests`           : Used only in earlier synchronous versions (can be removed)
- `urllib.robotparser` : For parsing `robots.txt` files
- `time`               : For high-resolution timing (`monotonic_ns`)
- `bs4` (BeautifulSoup): For parsing HTML/XML (optional, for downstream processing)
- `rfc3986`            : For URL validation and normalization
- `xml.etree.ElementTree`: XML parsing (e.g., for sitemap processing)
- `logging`            : For tracking behavior and debugging

Use Cases:
----------
- Building scalable and respectful web scrapers or crawlers
- Testing behavior of different user agents under robots.txt rules
- Discovering sitemap structures of websites
- Managing high traffic scraping tasks efficiently using async

Installation:
-------------
To use this module, install the required packages:

    pip install aiohttp rfc3986 beautifulsoup4 pandas

"""

from urllib.parse import urlparse
import urllib.robotparser 
import xml.etree.ElementTree as ET 
import time 
from bs4 import BeautifulSoup
import rfc3986
import logging
import asyncio
import aiohttp

logger = logging.getLogger(__name__)


class UserAgentPolicy:
    def __init__(self, name, header):
        self.name = name
        self.header = header
        self.base_url = None
        self.path_disallow = None
        self.crawl_delay = None
        self.request_rate = None
        self.last_access = 1e9
        self.visited = set()



# Useragents
useragent_dict = {
    "Googlebot": UserAgentPolicy("Googlebot",'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'),

    "Bingbot": UserAgentPolicy("Bingbot",'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)'),

    "Slurp": UserAgentPolicy("Slurp",'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)'),

    "DuckDuckbot": UserAgentPolicy("DuckDuckbot",'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)'),

    "Yandex": UserAgentPolicy("Yandex", 'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)'),

    "*": UserAgentPolicy("*",'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)')
}

# header generated by ChatGPT
useragent_dict_header = {'Googlebot':'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',
                    'Bingbot':'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)',
                    'Slurp':'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)',
                    'DuckDuckbot':'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)',
                    'Yandex':'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)',
                    '*': 'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)'}


class Fetcher:
    def __init__(self, session, useragent_name="Googlebot"):
        self.session = session
        self.useragents = {
            "Googlebot": UserAgentPolicy("Googlebot", 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'),
            "Bingbot": UserAgentPolicy("Bingbot", 'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)'),
            "*": UserAgentPolicy("*", 'Mozilla/5.0 (compatible; MyBot/1.0; +http://example.com/bot)'),
        }
        self.default_agent = self.useragents.get(useragent_name, self.useragents["Googlebot"])

    def get_user_agent(self):
        return self.default_agent
        
    async def check_robots(self, url, useragent=None):
        """
        Asynchronously fetches and parses the robots.txt file for a given URL.
        Extracts crawling policies (disallowed paths, crawl-delay, request-rate, sitemaps)
        and stores them in the specified UserAgentPolicy object.

        Args:
            url (str): The base URL of the target website.
            useragent (UserAgentPolicy, optional): A user agent policy object. If not provided,
                                               the default user agent is used.

        Returns:
            list or None: A list of sitemap URLs if declared in robots.txt, otherwise None.
        """
        # Use the default user agent if none is provided
        if not useragent:
            useragent = self.default_agent
        # Set base URL for tracking and generate the robots.txt URL    
        useragent.base_url = url
        new_url = url.rstrip("/") + "/robots.txt"

        # Create a new RobotFileParser object for parsing robots.txt content
        rfl = urllib.robotparser.RobotFileParser()
        try:
            # Send asynchronous GET request to fetch robots.txt
            async with self.session.get(new_url, timeout=10) as response:
                if response.status == 200:
                    # Read and parse the robots.txt content
                    robots_txt = await response.text()
                    rfl.parse(robots_txt.splitlines())
                    logging.info(f"Fetched robots.txt: {new_url}")
                else:
                    # Log if the file was not retrieved successfully
                    logging.warning(f"Failed to fetch robots.txt: HTTP {response.status}")
                    return None
        # Handle connection errors (e.g., DNS issues, timeouts)            
        except aiohttp.ClientError as e:
            logging.error(f"Error fetching robots.txt: {e}")
            return None
        # Extract rules and store them in the user agent policy
        useragent.path_disallow = rfl.parse("Disallow")
        useragent.crawl_delay = rfl.crawl_delay(useragent.name)
        useragent.request_rate = rfl.request_rate(useragent.name)

        return rfl.site_maps()

    async def check_time(self, useragent):
            """
             Enforces a delay between consecutive requests made by a given user agent,
             in accordance with robots.txt rules (crawl-delay or request-rate).
             Prevents overwhelming the target server by spacing out requests properly.

             Args:
                useragent (UserAgentPolicy): The user agent making the request.
            """
            # If both crawl-delay and request-rate are defined, use the larger delay
            if useragent.request_rate and useragent.crawl_delay:
                request_delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]
                crawl_delay = useragent.crawl_delay * 1e9
                delay = max(request_delay, crawl_delay)
            # Only crawl-delay is defined    
            elif useragent.crawl_delay:
                delay = useragent.crawl_delay * 1e9
            # Only request-rate is defined
            elif useragent.request_rate:
                delay = (useragent.request_rate[1] * 1e9) / useragent.request_rate[0]
            # No delay info found â€” use a reasonable default (1.5s)    
            else:
                delay = 1.5 * 1e9  # default delay
            logging.info(f"Delay: {delay / 1e9} seconds")
            # Calculate how much time to wait based on the last access time
            now = time.monotonic_ns()
            wait = max(0, (useragent.last_access + delay) - now)
            # If a wait is needed, sleep asynchronously
            if wait > 0:
                await asyncio.sleep(wait / 1e9)
            useragent.last_access = time.monotonic_ns()
            logging.info(f"{useragent.name} Last access: {useragent.last_access / 1e9} seconds")

    async def fetch(self, url, useragent=None):
      """
        Asynchronously fetches the content of a given URL using the specified user agent.

        The function respects crawling policies such as crawl-delay and request-rate,
        prevents duplicate requests, and logs timing and error information.

        Args:
            url (str): The target URL to fetch.
            useragent (UserAgentPolicy, optional): The user agent policy to use for this request.
                                               If not provided, the default agent is used.

        Returns:
            Tuple[str, str, int] or None:
                - HTML content of the fetched page (str)
                - Final URL after redirections (str)
                - HTTP status code (int)
                Returns None if the page is already visited or if an error occurs.
      """
      # Use default user agent if none is provided
      if not useragent:
          useragent = self.default_agent

      # Avoid re-fetching URLs already visited by this user agent
      if url in useragent.visited:
          logging.warning(f"[{useragent.name}] Page already visited: {url}")
          return None
      
      # Respect timing policies (e.g., crawl-delay or request-rate)
      await self.check_time(useragent)

      headers = {'User-Agent': useragent.header}

      start_time = time.perf_counter()
      try:
          # Perform asynchronous HTTP GET request with timeout
          async with self.session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=20)) as response:
              duration = time.perf_counter() - start_time

              if response.status == 200:
                      html = await response.text()
                      logging.info(f"[{useragent.name}] Fetched HTML page: {url} in {duration:.2f}s")
                      print(f"[{useragent.name}] Fetched HTML page: {url} in {duration:.2f}s")
                      # Mark URL as visited
                      useragent.visited.add(url)
                      return (html, str(response.url), response.status)
              else:
                  # Log and report non-200 status responses
                  logging.warning(f"[{useragent.name}] Failed to fetch {url}: HTTP {response.status} in {duration:.2f}s")
                  print(f"[{useragent.name}] Failed to fetch {url}: HTTP {response.status} in {duration:.2f}s")
                  return None
      
      # Handle timeout errors
      except asyncio.TimeoutError:
          duration = time.perf_counter() - start_time
          logging.error(f"[{useragent.name}] Timeout after {duration:.2f}s fetching {url}")
          return None

      # Handle network-related client errors    
      except aiohttp.ClientError as e:
          duration = time.perf_counter() - start_time
          logging.error(f"[{useragent.name}] Client error fetching {url} after {duration:.2f}s: {e}")
          return None

      # Handle any other unexpected errors    
      except Exception as e:
          duration = time.perf_counter() - start_time
          logging.error(f"[{useragent.name}] Unexpected error fetching {url} after {duration:.2f}s: {e}")
          return None

